{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sci\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import io\n",
    "import requests\n",
    "import tensorflow as tf\n",
    "from scipy import sparse\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sklearn.feature_extraction.text as sk_text\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn import metrics\n",
    "import collections\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "import shutil\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Open business.json file, create tsv file with business_id, business name, categories, and review count to be used as features \n",
    "#and stars as label\n",
    "\n",
    "outfile = open(\"business.tsv\", 'w')\n",
    "sfile = csv.writer(outfile, delimiter =\"\\t\", quoting=csv.QUOTE_MINIMAL)\n",
    "sfile.writerow(['business_id','categories', 'stars', 'review_count', 'postal code'])\n",
    "with open('../yelp_dataset/yelp_academic_dataset_business.json', encoding=\"utf8\") as f:\n",
    "    for line in f:\n",
    "        row = json.loads(line)\n",
    "        sfile.writerow([row['business_id'], row['categories'], row['stars'], row['review_count'], row['postal_code']])\n",
    "\n",
    "outfile.close()\n",
    "\n",
    "business_df= pd.read_csv('business.tsv', delimiter =\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Open review.json file, create tsv file with business_id,text to be used as features \n",
    "#and stars as label\n",
    "\n",
    "outfile = open(\"review_stars.tsv\", 'w')\n",
    "sfile = csv.writer(outfile, delimiter =\"\\t\", quoting=csv.QUOTE_MINIMAL)\n",
    "sfile.writerow(['business_id','stars', 'text'])\n",
    "with open('../yelp_dataset/yelp_academic_dataset_review.json', encoding=\"utf8\") as f:\n",
    "    for line in f:\n",
    "        row = json.loads(line)\n",
    "        # some special char must be encoded in 'utf-8'\n",
    "        sfile.writerow([row['business_id'], row['stars'], (row['text']).encode('utf-8')])\n",
    "\n",
    "outfile.close()\n",
    "\n",
    "review_df= pd.read_csv('review_stars.tsv', delimiter =\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group all reviews by business_id\n",
    "review_agg_df = review_df.groupby('business_id')['text'].sum()\n",
    "review_df_ready_for_sklearn = pd.DataFrame({'business_id': review_agg_df.index, 'all_reviews': review_agg_df.values})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merge the resulting review aggregate dataframe with business dataframe\n",
    "merge_df = pd.merge(business_df, review_df_ready_for_sklearn, on='business_id')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalization of review count field so it becomes comparable and remove bias\n",
    "\n",
    "from scipy.stats import zscore\n",
    "\n",
    "merge_df.insert(3,'normalized_count',((merge_df['review_count'] - merge_df['review_count'].min()) / (merge_df['review_count'].max() - merge_df['review_count'].min())).astype(float))\n",
    "\n",
    "merge_df['review_count'] = zscore(merge_df['review_count'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing NaN categories\n",
    "\n",
    "merge_df = merge_df[merge_df['categories'].notnull()]\n",
    "\n",
    "merge_df = merge_df[0:10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting categories\n",
    "\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "encoded_categories = MultiLabelBinarizer()\n",
    "category_matrix = encoded_categories.fit_transform(merge_df['categories'].str.split(','))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TF-IDF calculation\n",
    "\n",
    "tfidf = sk_text.TfidfVectorizer(max_features=1000, lowercase=True, analyzer='word',\n",
    " stop_words= 'english',ngram_range=(1,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the reviews column with TFIDFvectorizer\n",
    "matrix = tfidf.fit_transform(merge_df['all_reviews'])\n",
    "matrix = matrix.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are adding the normalized count to the original matrix with TFIDFvectorizer\n",
    "x_matrix_minmax = np.column_stack((matrix, merge_df['normalized_count']))\n",
    "\n",
    "# Zscore\n",
    "x_matrix_zscore = np.column_stack((matrix, merge_df['review_count']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train test data for linear regression\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_matrix_minmax, merge_df['stars'] , test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# linear regression\n",
    "\n",
    "lin_reg_model = LinearRegression()\n",
    "\n",
    "lin_reg_model.fit(x_train, y_train)\n",
    "\n",
    "y_pred_linear = lin_reg_model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "business id - hTF1Qo6PRFnDgg1rh9a9BQ actual stars  - 4.000000 predicted - 3.630765\n",
      "business id - bAPjkuNJ67j2F4C5HQQHhQ actual stars  - 4.000000 predicted - 3.837557\n",
      "business id - dFcs3q8ynbFEaAnbyGSLjQ actual stars  - 4.000000 predicted - 4.120657\n",
      "business id - jrhc4s5XMR8S8kpGdU08og actual stars  - 4.500000 predicted - 4.811266\n",
      "business id - e7207sqC-pSn6GIf31ikhQ actual stars  - 4.000000 predicted - 3.747301\n",
      "business id - CF9TxeEdP5QxihYFAl4sUg actual stars  - 4.000000 predicted - 4.036702\n",
      "business id - zZPCAFK85NtitSNVP_wfYg actual stars  - 3.500000 predicted - 3.527224\n",
      "business id - 42U4Vlzr7nmQa1Bk8J4flw actual stars  - 5.000000 predicted - 4.826022\n",
      "business id - TTrYd662CZFRPaiwl-sUqA actual stars  - 2.000000 predicted - 1.793183\n",
      "business id - -lBIxCbHxuN3YO_sUkWeUQ actual stars  - 2.500000 predicted - 2.515123\n"
     ]
    }
   ],
   "source": [
    "# list  the business with the stars and prediction\n",
    "\n",
    "for i in range(0,10):\n",
    "    idx=y_test.index[i]\n",
    "    print(\"business id - %s actual stars  - %f predicted - %f\" \n",
    "          %(merge_df['business_id'][idx], y_test[idx], y_pred_linear[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error: 0.55\n",
      "Variance score: 0.70\n"
     ]
    }
   ],
   "source": [
    "# RMS value\n",
    "\n",
    "score_lin_classic = np.sqrt(mean_squared_error(y_test, y_pred_linear))\n",
    "print(\"Root Mean Squared Error: %.2f\" % score_lin_classic)\n",
    "print('Variance score: %.2f' % r2_score(y_test, y_pred_linear))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label encoding data for logistic regression\n",
    "\n",
    "label_encoder = preprocessing.LabelEncoder()\n",
    "\n",
    "merge_df['encoded_stars'] = label_encoder.fit_transform(merge_df['stars'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train test data afor other models\n",
    "\n",
    "x_train_lr, x_test_lr, y_train_lr, y_test_lr = train_test_split(x_matrix_minmax, merge_df['encoded_stars'] , test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logistic Regression\n",
    "\n",
    "Log_reg_model = LogisticRegression()\n",
    "\n",
    "Log_reg_model.fit(x_train_lr, y_train_lr)\n",
    "\n",
    "y_pred_logistic = Log_reg_model.predict(x_test_lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error: 1.38\n",
      "Variance score: 0.54\n"
     ]
    }
   ],
   "source": [
    "# RMs for logistic\n",
    "\n",
    "score_log_classic = np.sqrt(mean_squared_error(y_test_lr, y_pred_logistic))\n",
    "print(\"Root Mean Squared Error: %.2f\" % score_log_classic)\n",
    "print('Variance score: %.2f' % r2_score(y_test_lr, y_pred_logistic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow Model for Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** Training without early stopping and Model Checkpoint and RELU ***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tensor flow works well with 32 bit\n",
    "y_stars_regression = merge_df['stars'].values.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train test data\n",
    "x_train_reg, x_test_reg, y_train_reg, y_test_reg = train_test_split(x_matrix_minmax, y_stars_regression , test_size=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensorflow model for Regression with RELU\n",
    "\n",
    "model_reg_relu = Sequential()\n",
    "\n",
    "model_reg_relu.add(Dense(60, input_dim=x_train_reg.shape[1], activation='relu')) \n",
    "model_reg_relu.add(Dense(30, activation='relu')) # Hidden 2\n",
    "model_reg_relu.add(Dense(1)) # Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      " - 1s - loss: 1.9915\n",
      "Epoch 2/100\n",
      " - 0s - loss: 0.3267\n",
      "Epoch 3/100\n",
      " - 0s - loss: 0.2635\n",
      "Epoch 4/100\n",
      " - 0s - loss: 0.2459\n",
      "Epoch 5/100\n",
      " - 0s - loss: 0.2334\n",
      "Epoch 6/100\n",
      " - 0s - loss: 0.2141\n",
      "Epoch 7/100\n",
      " - 0s - loss: 0.1893\n",
      "Epoch 8/100\n",
      " - 0s - loss: 0.1650\n",
      "Epoch 9/100\n",
      " - 0s - loss: 0.1452\n",
      "Epoch 10/100\n",
      " - 0s - loss: 0.1240\n",
      "Epoch 11/100\n",
      " - 0s - loss: 0.1057\n",
      "Epoch 12/100\n",
      " - 0s - loss: 0.0904\n",
      "Epoch 13/100\n",
      " - 0s - loss: 0.0759\n",
      "Epoch 14/100\n",
      " - 0s - loss: 0.0632\n",
      "Epoch 15/100\n",
      " - 0s - loss: 0.0537\n",
      "Epoch 16/100\n",
      " - 0s - loss: 0.0459\n",
      "Epoch 17/100\n",
      " - 0s - loss: 0.0387\n",
      "Epoch 18/100\n",
      " - 0s - loss: 0.0338\n",
      "Epoch 19/100\n",
      " - 0s - loss: 0.0293\n",
      "Epoch 20/100\n",
      " - 0s - loss: 0.0260\n",
      "Epoch 21/100\n",
      " - 0s - loss: 0.0241\n",
      "Epoch 22/100\n",
      " - 0s - loss: 0.0224\n",
      "Epoch 23/100\n",
      " - 0s - loss: 0.0205\n",
      "Epoch 24/100\n",
      " - 0s - loss: 0.0186\n",
      "Epoch 25/100\n",
      " - 0s - loss: 0.0175\n",
      "Epoch 26/100\n",
      " - 0s - loss: 0.0163\n",
      "Epoch 27/100\n",
      " - 0s - loss: 0.0157\n",
      "Epoch 28/100\n",
      " - 0s - loss: 0.0151\n",
      "Epoch 29/100\n",
      " - 0s - loss: 0.0158\n",
      "Epoch 30/100\n",
      " - 0s - loss: 0.0146\n",
      "Epoch 31/100\n",
      " - 0s - loss: 0.0142\n",
      "Epoch 32/100\n",
      " - 0s - loss: 0.0132\n",
      "Epoch 33/100\n",
      " - 0s - loss: 0.0128\n",
      "Epoch 34/100\n",
      " - 0s - loss: 0.0133\n",
      "Epoch 35/100\n",
      " - 0s - loss: 0.0135\n",
      "Epoch 36/100\n",
      " - 0s - loss: 0.0123\n",
      "Epoch 37/100\n",
      " - 0s - loss: 0.0106\n",
      "Epoch 38/100\n",
      " - 0s - loss: 0.0104\n",
      "Epoch 39/100\n",
      " - 0s - loss: 0.0100\n",
      "Epoch 40/100\n",
      " - 0s - loss: 0.0095\n",
      "Epoch 41/100\n",
      " - 0s - loss: 0.0101\n",
      "Epoch 42/100\n",
      " - 0s - loss: 0.0100\n",
      "Epoch 43/100\n",
      " - 0s - loss: 0.0101\n",
      "Epoch 44/100\n",
      " - 0s - loss: 0.0098\n",
      "Epoch 45/100\n",
      " - 0s - loss: 0.0094\n",
      "Epoch 46/100\n",
      " - 0s - loss: 0.0097\n",
      "Epoch 47/100\n",
      " - 0s - loss: 0.0092\n",
      "Epoch 48/100\n",
      " - 0s - loss: 0.0085\n",
      "Epoch 49/100\n",
      " - 0s - loss: 0.0082\n",
      "Epoch 50/100\n",
      " - 0s - loss: 0.0086\n",
      "Epoch 51/100\n",
      " - 0s - loss: 0.0084\n",
      "Epoch 52/100\n",
      " - 0s - loss: 0.0078\n",
      "Epoch 53/100\n",
      " - 0s - loss: 0.0069\n",
      "Epoch 54/100\n",
      " - 0s - loss: 0.0075\n",
      "Epoch 55/100\n",
      " - 0s - loss: 0.0089\n",
      "Epoch 56/100\n",
      " - 0s - loss: 0.0086\n",
      "Epoch 57/100\n",
      " - 0s - loss: 0.0083\n",
      "Epoch 58/100\n",
      " - 0s - loss: 0.0076\n",
      "Epoch 59/100\n",
      " - 0s - loss: 0.0069\n",
      "Epoch 60/100\n",
      " - 0s - loss: 0.0066\n",
      "Epoch 61/100\n",
      " - 0s - loss: 0.0062\n",
      "Epoch 62/100\n",
      " - 0s - loss: 0.0058\n",
      "Epoch 63/100\n",
      " - 0s - loss: 0.0064\n",
      "Epoch 64/100\n",
      " - 0s - loss: 0.0070\n",
      "Epoch 65/100\n",
      " - 0s - loss: 0.0070\n",
      "Epoch 66/100\n",
      " - 0s - loss: 0.0070\n",
      "Epoch 67/100\n",
      " - 0s - loss: 0.0062\n",
      "Epoch 68/100\n",
      " - 0s - loss: 0.0062\n",
      "Epoch 69/100\n",
      " - 0s - loss: 0.0061\n",
      "Epoch 70/100\n",
      " - 0s - loss: 0.0061\n",
      "Epoch 71/100\n",
      " - 0s - loss: 0.0060\n",
      "Epoch 72/100\n",
      " - 0s - loss: 0.0061\n",
      "Epoch 73/100\n",
      " - 0s - loss: 0.0057\n",
      "Epoch 74/100\n",
      " - 0s - loss: 0.0054\n",
      "Epoch 75/100\n",
      " - 0s - loss: 0.0058\n",
      "Epoch 76/100\n",
      " - 0s - loss: 0.0058\n",
      "Epoch 77/100\n",
      " - 0s - loss: 0.0054\n",
      "Epoch 78/100\n",
      " - 0s - loss: 0.0051\n",
      "Epoch 79/100\n",
      " - 0s - loss: 0.0049\n",
      "Epoch 80/100\n",
      " - 0s - loss: 0.0048\n",
      "Epoch 81/100\n",
      " - 0s - loss: 0.0051\n",
      "Epoch 82/100\n",
      " - 0s - loss: 0.0062\n",
      "Epoch 83/100\n",
      " - 0s - loss: 0.0057\n",
      "Epoch 84/100\n",
      " - 0s - loss: 0.0050\n",
      "Epoch 85/100\n",
      " - 0s - loss: 0.0046\n",
      "Epoch 86/100\n",
      " - 0s - loss: 0.0046\n",
      "Epoch 87/100\n",
      " - 0s - loss: 0.0049\n",
      "Epoch 88/100\n",
      " - 0s - loss: 0.0052\n",
      "Epoch 89/100\n",
      " - 0s - loss: 0.0049\n",
      "Epoch 90/100\n",
      " - 0s - loss: 0.0051\n",
      "Epoch 91/100\n",
      " - 0s - loss: 0.0046\n",
      "Epoch 92/100\n",
      " - 0s - loss: 0.0042\n",
      "Epoch 93/100\n",
      " - 0s - loss: 0.0043\n",
      "Epoch 94/100\n",
      " - 0s - loss: 0.0044\n",
      "Epoch 95/100\n",
      " - 0s - loss: 0.0044\n",
      "Epoch 96/100\n",
      " - 0s - loss: 0.0042\n",
      "Epoch 97/100\n",
      " - 0s - loss: 0.0042\n",
      "Epoch 98/100\n",
      " - 0s - loss: 0.0039\n",
      "Epoch 99/100\n",
      " - 0s - loss: 0.0041\n",
      "Epoch 100/100\n",
      " - 0s - loss: 0.0041\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1b1805ac18>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Model training with Optimizer = adam\n",
    "\n",
    "model_reg_relu.compile(loss='mean_squared_error', optimizer='adam')\n",
    "model_reg_relu.fit(x_train_reg,y_train_reg,verbose=2,epochs=100) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (2000, 1)\n"
     ]
    }
   ],
   "source": [
    "# Predict stars\n",
    "pred_reg_simple = model_reg_relu.predict(x_test_reg)\n",
    "print(\"Shape: {}\".format(pred_reg_simple.shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Business ID: diaiQrxYFU1V5qxrFnW9fg, Actual Stars: 3.0, predicted Stars: [4.02945]\n",
      "2. Business ID: TDTASGFy_aGp6vy0i23mDA, Actual Stars: 4.5, predicted Stars: [4.2402587]\n",
      "3. Business ID: VuKJ2s_JP8weQ54NfsXJXQ, Actual Stars: 4.5, predicted Stars: [4.6323643]\n",
      "4. Business ID: aGiBg2WKOpXS5-1DRnBiAQ, Actual Stars: 3.0, predicted Stars: [3.6203978]\n",
      "5. Business ID: ZMmgFw2P4LWsFXNn1ZGc1g, Actual Stars: 2.0, predicted Stars: [1.7348386]\n",
      "6. Business ID: sEKFq5u8P_s0-2mAZnx0JQ, Actual Stars: 4.5, predicted Stars: [4.168972]\n",
      "7. Business ID: rYziPPEILDXJ_F5uKR--YQ, Actual Stars: 2.0, predicted Stars: [2.8198276]\n",
      "8. Business ID: Swm_uMOWNcJDZz5lXWyzKA, Actual Stars: 5.0, predicted Stars: [5.320364]\n",
      "9. Business ID: 6nGnVP7M4qQRiclXxeqXSQ, Actual Stars: 3.0, predicted Stars: [3.9572725]\n",
      "10. Business ID: Tc24GX9-ZPr4_SHU0nJZZA, Actual Stars: 4.0, predicted Stars: [4.4626665]\n"
     ]
    }
   ],
   "source": [
    "#Display 10 business\n",
    "\n",
    "for i in range(10):\n",
    "    print(\"{}. Business ID: {}, Actual Stars: {}, predicted Stars: {}\".format(i+1,merge_df['business_id'][2000+i],y_test_reg[i],pred_reg_simple[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final score (RMSE): 0.587897777557373\n",
      "R2 score: 0.66\n"
     ]
    }
   ],
   "source": [
    "# Measure RMSE error.  RMSE is common for regression.\n",
    "score_reg_relu = np.sqrt(mean_squared_error(y_test_reg,pred_reg_simple))\n",
    "print(\"Final score (RMSE): {}\".format(score_reg_relu))\n",
    "print('R2 score: %.2f' % r2_score(y_test_reg, pred_reg_simple))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training with early stopping and Model Checkpoint ReLU **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup early stopping monitor\n",
    "\n",
    "monitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=3, verbose=2, mode='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up checkpointer\n",
    "checkpointer_relu = ModelCheckpoint(filepath=\"./best_weights_relu.hdf5\", verbose=2, save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/100\n",
      " - 1s - loss: 0.0046 - val_loss: 0.3496\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.34958, saving model to ./best_weights_relu.hdf5\n",
      "Epoch 2/100\n",
      " - 0s - loss: 0.0042 - val_loss: 0.3448\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.34958 to 0.34476, saving model to ./best_weights_relu.hdf5\n",
      "Epoch 3/100\n",
      " - 0s - loss: 0.0039 - val_loss: 0.3496\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.34476\n",
      "Epoch 4/100\n",
      " - 0s - loss: 0.0036 - val_loss: 0.3430\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.34476 to 0.34305, saving model to ./best_weights_relu.hdf5\n",
      "Epoch 5/100\n",
      " - 0s - loss: 0.0039 - val_loss: 0.3485\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.34305\n",
      "Epoch 6/100\n",
      " - 0s - loss: 0.0045 - val_loss: 0.3475\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.34305\n",
      "Epoch 7/100\n",
      " - 0s - loss: 0.0045 - val_loss: 0.3474\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.34305\n",
      "Epoch 00007: early stopping\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/100\n",
      " - 1s - loss: 0.0043 - val_loss: 0.3445\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.34305\n",
      "Epoch 2/100\n",
      " - 0s - loss: 0.0034 - val_loss: 0.3480\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.34305\n",
      "Epoch 3/100\n",
      " - 0s - loss: 0.0035 - val_loss: 0.3484\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.34305\n",
      "Epoch 4/100\n",
      " - 0s - loss: 0.0040 - val_loss: 0.3463\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.34305\n",
      "Epoch 00004: early stopping\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/100\n",
      " - 1s - loss: 0.0043 - val_loss: 0.3469\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.34305\n",
      "Epoch 2/100\n",
      " - 0s - loss: 0.0038 - val_loss: 0.3497\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.34305\n",
      "Epoch 3/100\n",
      " - 0s - loss: 0.0039 - val_loss: 0.3471\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.34305\n",
      "Epoch 4/100\n",
      " - 0s - loss: 0.0036 - val_loss: 0.3468\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.34305\n",
      "Epoch 00004: early stopping\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/100\n",
      " - 1s - loss: 0.0036 - val_loss: 0.3451\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.34305\n",
      "Epoch 2/100\n",
      " - 0s - loss: 0.0039 - val_loss: 0.3434\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.34305\n",
      "Epoch 3/100\n",
      " - 0s - loss: 0.0042 - val_loss: 0.3457\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.34305\n",
      "Epoch 4/100\n",
      " - 0s - loss: 0.0038 - val_loss: 0.3467\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.34305\n",
      "Epoch 5/100\n",
      " - 0s - loss: 0.0034 - val_loss: 0.3460\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.34305\n",
      "Epoch 00005: early stopping\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/100\n",
      " - 1s - loss: 0.0036 - val_loss: 0.3469\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.34305\n",
      "Epoch 2/100\n",
      " - 0s - loss: 0.0034 - val_loss: 0.3426\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.34305 to 0.34263, saving model to ./best_weights_relu.hdf5\n",
      "Epoch 3/100\n",
      " - 0s - loss: 0.0034 - val_loss: 0.3438\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.34263\n",
      "Epoch 4/100\n",
      " - 0s - loss: 0.0034 - val_loss: 0.3440\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.34263\n",
      "Epoch 5/100\n",
      " - 0s - loss: 0.0035 - val_loss: 0.3453\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.34263\n",
      "Epoch 00005: early stopping\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/100\n",
      " - 1s - loss: 0.0035 - val_loss: 0.3478\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.34263\n",
      "Epoch 2/100\n",
      " - 0s - loss: 0.0034 - val_loss: 0.3454\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.34263\n",
      "Epoch 3/100\n",
      " - 0s - loss: 0.0033 - val_loss: 0.3442\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.34263\n",
      "Epoch 4/100\n",
      " - 0s - loss: 0.0031 - val_loss: 0.3439\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.34263\n",
      "Epoch 5/100\n",
      " - 0s - loss: 0.0034 - val_loss: 0.3439\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.34263\n",
      "Epoch 6/100\n",
      " - 0s - loss: 0.0034 - val_loss: 0.3449\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.34263\n",
      "Epoch 00006: early stopping\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/100\n",
      " - 1s - loss: 0.0033 - val_loss: 0.3451\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.34263\n",
      "Epoch 2/100\n",
      " - 0s - loss: 0.0031 - val_loss: 0.3443\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.34263\n",
      "Epoch 3/100\n",
      " - 0s - loss: 0.0032 - val_loss: 0.3452\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.34263\n",
      "Epoch 4/100\n",
      " - 0s - loss: 0.0034 - val_loss: 0.3441\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.34263\n",
      "Epoch 5/100\n",
      " - 0s - loss: 0.0034 - val_loss: 0.3448\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.34263\n",
      "Epoch 6/100\n",
      " - 0s - loss: 0.0030 - val_loss: 0.3437\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.34263\n",
      "Epoch 7/100\n",
      " - 0s - loss: 0.0032 - val_loss: 0.3413\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.34263 to 0.34127, saving model to ./best_weights_relu.hdf5\n",
      "Epoch 8/100\n",
      " - 0s - loss: 0.0032 - val_loss: 0.3458\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.34127\n",
      "Epoch 9/100\n",
      " - 0s - loss: 0.0030 - val_loss: 0.3433\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.34127\n",
      "Epoch 10/100\n",
      " - 0s - loss: 0.0031 - val_loss: 0.3439\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.34127\n",
      "Epoch 00010: early stopping\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/100\n",
      " - 1s - loss: 0.0031 - val_loss: 0.3444\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.34127\n",
      "Epoch 2/100\n",
      " - 0s - loss: 0.0027 - val_loss: 0.3449\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.34127\n",
      "Epoch 3/100\n",
      " - 0s - loss: 0.0028 - val_loss: 0.3426\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.34127\n",
      "Epoch 4/100\n",
      " - 0s - loss: 0.0030 - val_loss: 0.3431\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.34127\n",
      "Epoch 5/100\n",
      " - 0s - loss: 0.0033 - val_loss: 0.3422\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.34127\n",
      "Epoch 6/100\n",
      " - 0s - loss: 0.0031 - val_loss: 0.3414\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.34127\n",
      "Epoch 7/100\n",
      " - 0s - loss: 0.0028 - val_loss: 0.3439\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.34127\n",
      "Epoch 8/100\n",
      " - 0s - loss: 0.0028 - val_loss: 0.3412\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.34127 to 0.34123, saving model to ./best_weights_relu.hdf5\n",
      "Epoch 9/100\n",
      " - 0s - loss: 0.0027 - val_loss: 0.3408\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.34123 to 0.34077, saving model to ./best_weights_relu.hdf5\n",
      "Epoch 00009: early stopping\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/100\n",
      " - 2s - loss: 0.0030 - val_loss: 0.3415\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.34077\n",
      "Epoch 2/100\n",
      " - 0s - loss: 0.0024 - val_loss: 0.3403\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.34077 to 0.34035, saving model to ./best_weights_relu.hdf5\n",
      "Epoch 3/100\n",
      " - 0s - loss: 0.0023 - val_loss: 0.3437\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.34035\n",
      "Epoch 4/100\n",
      " - 0s - loss: 0.0032 - val_loss: 0.3391\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.34035 to 0.33910, saving model to ./best_weights_relu.hdf5\n",
      "Epoch 5/100\n",
      " - 0s - loss: 0.0032 - val_loss: 0.3416\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.33910\n",
      "Epoch 6/100\n",
      " - 0s - loss: 0.0031 - val_loss: 0.3405\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.33910\n",
      "Epoch 7/100\n",
      " - 0s - loss: 0.0028 - val_loss: 0.3396\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.33910\n",
      "Epoch 00007: early stopping\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/100\n",
      " - 1s - loss: 0.0026 - val_loss: 0.3389\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.33910 to 0.33887, saving model to ./best_weights_relu.hdf5\n",
      "Epoch 2/100\n",
      " - 0s - loss: 0.0029 - val_loss: 0.3420\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.33887\n",
      "Epoch 3/100\n",
      " - 0s - loss: 0.0028 - val_loss: 0.3401\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.33887\n",
      "Epoch 4/100\n",
      " - 0s - loss: 0.0026 - val_loss: 0.3400\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.33887\n",
      "Epoch 00004: early stopping\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    model_reg_relu.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    model_reg_relu.fit(x_train_reg,y_train_reg,validation_data=(x_test_reg,y_test_reg),callbacks=[monitor,checkpointer_relu],verbose=2,epochs=100) \n",
    "\n",
    "model_reg_relu.load_weights('./best_weights_relu.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (2000, 1)\n"
     ]
    }
   ],
   "source": [
    "# Predict stars\n",
    "pred_reg_stopping = model_reg_relu.predict(x_test_reg)\n",
    "print(\"Shape: {}\".format(pred_reg_stopping.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Business ID: diaiQrxYFU1V5qxrFnW9fg, Actual Stars: 3.0, predicted Stars: [4.074934]\n",
      "2. Business ID: TDTASGFy_aGp6vy0i23mDA, Actual Stars: 4.5, predicted Stars: [4.138316]\n",
      "3. Business ID: VuKJ2s_JP8weQ54NfsXJXQ, Actual Stars: 4.5, predicted Stars: [4.816916]\n",
      "4. Business ID: aGiBg2WKOpXS5-1DRnBiAQ, Actual Stars: 3.0, predicted Stars: [3.5320487]\n",
      "5. Business ID: ZMmgFw2P4LWsFXNn1ZGc1g, Actual Stars: 2.0, predicted Stars: [1.9106985]\n",
      "6. Business ID: sEKFq5u8P_s0-2mAZnx0JQ, Actual Stars: 4.5, predicted Stars: [4.1183686]\n",
      "7. Business ID: rYziPPEILDXJ_F5uKR--YQ, Actual Stars: 2.0, predicted Stars: [2.8297057]\n",
      "8. Business ID: Swm_uMOWNcJDZz5lXWyzKA, Actual Stars: 5.0, predicted Stars: [5.2395654]\n",
      "9. Business ID: 6nGnVP7M4qQRiclXxeqXSQ, Actual Stars: 3.0, predicted Stars: [3.9169335]\n",
      "10. Business ID: Tc24GX9-ZPr4_SHU0nJZZA, Actual Stars: 4.0, predicted Stars: [4.296971]\n"
     ]
    }
   ],
   "source": [
    "#Display 10 business\n",
    "\n",
    "for i in range(10):\n",
    "    print(\"{}. Business ID: {}, Actual Stars: {}, predicted Stars: {}\".format(i+1,merge_df['business_id'][2000+i],y_test_reg[i],pred_reg_stopping[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final score (RMSE): 0.5821277499198914\n",
      "R2 score: 0.67\n"
     ]
    }
   ],
   "source": [
    "# Measure RMSE error.  RMSE is common for regression.\n",
    "score_reg_relu_stopping = np.sqrt(mean_squared_error(y_test_reg,pred_reg_stopping))\n",
    "print(\"Final score (RMSE): {}\".format(score_reg_relu_stopping))\n",
    "print('R2 score: %.2f' % r2_score(y_test_reg, pred_reg_stopping))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training without early stopping and Model Checkpoint and Sigmoid **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensorflow model for Regression with sigmoid\n",
    "model_reg_sig = Sequential()\n",
    "\n",
    "model_reg_sig.add(Dense(25, input_dim=x_train_reg.shape[1], activation='sigmoid'))  \n",
    "model_reg_sig.add(Dense(10, activation='sigmoid')) # Hidden 2\n",
    "model_reg_sig.add(Dense(1)) # Output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      " - 1s - loss: 3.7544\n",
      "Epoch 2/100\n",
      " - 0s - loss: 1.1214\n",
      "Epoch 3/100\n",
      " - 0s - loss: 1.0386\n",
      "Epoch 4/100\n",
      " - 0s - loss: 1.0128\n",
      "Epoch 5/100\n",
      " - 0s - loss: 0.9033\n",
      "Epoch 6/100\n",
      " - 0s - loss: 0.6225\n",
      "Epoch 7/100\n",
      " - 0s - loss: 0.4114\n",
      "Epoch 8/100\n",
      " - 0s - loss: 0.3156\n",
      "Epoch 9/100\n",
      " - 0s - loss: 0.2734\n",
      "Epoch 10/100\n",
      " - 0s - loss: 0.2509\n",
      "Epoch 11/100\n",
      " - 0s - loss: 0.2376\n",
      "Epoch 12/100\n",
      " - 0s - loss: 0.2288\n",
      "Epoch 13/100\n",
      " - 0s - loss: 0.2219\n",
      "Epoch 14/100\n",
      " - 0s - loss: 0.2165\n",
      "Epoch 15/100\n",
      " - 0s - loss: 0.2119\n",
      "Epoch 16/100\n",
      " - 0s - loss: 0.2082\n",
      "Epoch 17/100\n",
      " - 0s - loss: 0.2047\n",
      "Epoch 18/100\n",
      " - 0s - loss: 0.2011\n",
      "Epoch 19/100\n",
      " - 0s - loss: 0.1984\n",
      "Epoch 20/100\n",
      " - 0s - loss: 0.1954\n",
      "Epoch 21/100\n",
      " - 0s - loss: 0.1930\n",
      "Epoch 22/100\n",
      " - 0s - loss: 0.1906\n",
      "Epoch 23/100\n",
      " - 0s - loss: 0.1886\n",
      "Epoch 24/100\n",
      " - 0s - loss: 0.1869\n",
      "Epoch 25/100\n",
      " - 0s - loss: 0.1845\n",
      "Epoch 26/100\n",
      " - 0s - loss: 0.1834\n",
      "Epoch 27/100\n",
      " - 0s - loss: 0.1812\n",
      "Epoch 28/100\n",
      " - 0s - loss: 0.1795\n",
      "Epoch 29/100\n",
      " - 0s - loss: 0.1782\n",
      "Epoch 30/100\n",
      " - 0s - loss: 0.1769\n",
      "Epoch 31/100\n",
      " - 0s - loss: 0.1756\n",
      "Epoch 32/100\n",
      " - 0s - loss: 0.1737\n",
      "Epoch 33/100\n",
      " - 0s - loss: 0.1724\n",
      "Epoch 34/100\n",
      " - 0s - loss: 0.1714\n",
      "Epoch 35/100\n",
      " - 0s - loss: 0.1702\n",
      "Epoch 36/100\n",
      " - 0s - loss: 0.1690\n",
      "Epoch 37/100\n",
      " - 0s - loss: 0.1681\n",
      "Epoch 38/100\n",
      " - 0s - loss: 0.1664\n",
      "Epoch 39/100\n",
      " - 0s - loss: 0.1656\n",
      "Epoch 40/100\n",
      " - 0s - loss: 0.1645\n",
      "Epoch 41/100\n",
      " - 0s - loss: 0.1637\n",
      "Epoch 42/100\n",
      " - 0s - loss: 0.1623\n",
      "Epoch 43/100\n",
      " - 0s - loss: 0.1618\n",
      "Epoch 44/100\n",
      " - 0s - loss: 0.1605\n",
      "Epoch 45/100\n",
      " - 0s - loss: 0.1598\n",
      "Epoch 46/100\n",
      " - 0s - loss: 0.1589\n",
      "Epoch 47/100\n",
      " - 0s - loss: 0.1580\n",
      "Epoch 48/100\n",
      " - 0s - loss: 0.1568\n",
      "Epoch 49/100\n",
      " - 0s - loss: 0.1562\n",
      "Epoch 50/100\n",
      " - 0s - loss: 0.1552\n",
      "Epoch 51/100\n",
      " - 0s - loss: 0.1546\n",
      "Epoch 52/100\n",
      " - 0s - loss: 0.1535\n",
      "Epoch 53/100\n",
      " - 0s - loss: 0.1528\n",
      "Epoch 54/100\n",
      " - 0s - loss: 0.1522\n",
      "Epoch 55/100\n",
      " - 0s - loss: 0.1515\n",
      "Epoch 56/100\n",
      " - 0s - loss: 0.1508\n",
      "Epoch 57/100\n",
      " - 0s - loss: 0.1502\n",
      "Epoch 58/100\n",
      " - 0s - loss: 0.1492\n",
      "Epoch 59/100\n",
      " - 0s - loss: 0.1484\n",
      "Epoch 60/100\n",
      " - 0s - loss: 0.1482\n",
      "Epoch 61/100\n",
      " - 0s - loss: 0.1472\n",
      "Epoch 62/100\n",
      " - 0s - loss: 0.1465\n",
      "Epoch 63/100\n",
      " - 0s - loss: 0.1459\n",
      "Epoch 64/100\n",
      " - 0s - loss: 0.1451\n",
      "Epoch 65/100\n",
      " - 0s - loss: 0.1445\n",
      "Epoch 66/100\n",
      " - 0s - loss: 0.1438\n",
      "Epoch 67/100\n",
      " - 0s - loss: 0.1433\n",
      "Epoch 68/100\n",
      " - 0s - loss: 0.1422\n",
      "Epoch 69/100\n",
      " - 0s - loss: 0.1422\n",
      "Epoch 70/100\n",
      " - 0s - loss: 0.1413\n",
      "Epoch 71/100\n",
      " - 0s - loss: 0.1408\n",
      "Epoch 72/100\n",
      " - 0s - loss: 0.1406\n",
      "Epoch 73/100\n",
      " - 0s - loss: 0.1393\n",
      "Epoch 74/100\n",
      " - 0s - loss: 0.1389\n",
      "Epoch 75/100\n",
      " - 0s - loss: 0.1382\n",
      "Epoch 76/100\n",
      " - 0s - loss: 0.1373\n",
      "Epoch 77/100\n",
      " - 0s - loss: 0.1372\n",
      "Epoch 78/100\n",
      " - 0s - loss: 0.1367\n",
      "Epoch 79/100\n",
      " - 0s - loss: 0.1360\n",
      "Epoch 80/100\n",
      " - 0s - loss: 0.1350\n",
      "Epoch 81/100\n",
      " - 0s - loss: 0.1348\n",
      "Epoch 82/100\n",
      " - 0s - loss: 0.1342\n",
      "Epoch 83/100\n",
      " - 0s - loss: 0.1337\n",
      "Epoch 84/100\n",
      " - 0s - loss: 0.1335\n",
      "Epoch 85/100\n",
      " - 0s - loss: 0.1325\n",
      "Epoch 86/100\n",
      " - 0s - loss: 0.1322\n",
      "Epoch 87/100\n",
      " - 0s - loss: 0.1315\n",
      "Epoch 88/100\n",
      " - 0s - loss: 0.1309\n",
      "Epoch 89/100\n",
      " - 0s - loss: 0.1304\n",
      "Epoch 90/100\n",
      " - 0s - loss: 0.1304\n",
      "Epoch 91/100\n",
      " - 0s - loss: 0.1292\n",
      "Epoch 92/100\n",
      " - 0s - loss: 0.1287\n",
      "Epoch 93/100\n",
      " - 0s - loss: 0.1286\n",
      "Epoch 94/100\n",
      " - 0s - loss: 0.1280\n",
      "Epoch 95/100\n",
      " - 0s - loss: 0.1273\n",
      "Epoch 96/100\n",
      " - 0s - loss: 0.1270\n",
      "Epoch 97/100\n",
      " - 0s - loss: 0.1260\n",
      "Epoch 98/100\n",
      " - 0s - loss: 0.1255\n",
      "Epoch 99/100\n",
      " - 0s - loss: 0.1253\n",
      "Epoch 100/100\n",
      " - 0s - loss: 0.1247\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1b145a3630>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model training with sigmoid \n",
    "model_reg_sig.compile(loss='mean_squared_error', optimizer='adam')\n",
    "model_reg_sig.fit(x_train_reg,y_train_reg,verbose=2,epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (2000, 1)\n"
     ]
    }
   ],
   "source": [
    "# Predict stars\n",
    "pred_reg_sig_simple = model_reg_sig.predict(x_test_reg)\n",
    "print(\"Shape: {}\".format(pred_reg_sig_simple.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Business ID: diaiQrxYFU1V5qxrFnW9fg, Actual Stars: 3.0, predicted Stars: [3.790456]\n",
      "2. Business ID: TDTASGFy_aGp6vy0i23mDA, Actual Stars: 4.5, predicted Stars: [4.0833592]\n",
      "3. Business ID: VuKJ2s_JP8weQ54NfsXJXQ, Actual Stars: 4.5, predicted Stars: [4.6271396]\n",
      "4. Business ID: aGiBg2WKOpXS5-1DRnBiAQ, Actual Stars: 3.0, predicted Stars: [3.466667]\n",
      "5. Business ID: ZMmgFw2P4LWsFXNn1ZGc1g, Actual Stars: 2.0, predicted Stars: [1.3305807]\n",
      "6. Business ID: sEKFq5u8P_s0-2mAZnx0JQ, Actual Stars: 4.5, predicted Stars: [4.1867976]\n",
      "7. Business ID: rYziPPEILDXJ_F5uKR--YQ, Actual Stars: 2.0, predicted Stars: [2.0521252]\n",
      "8. Business ID: Swm_uMOWNcJDZz5lXWyzKA, Actual Stars: 5.0, predicted Stars: [4.908711]\n",
      "9. Business ID: 6nGnVP7M4qQRiclXxeqXSQ, Actual Stars: 3.0, predicted Stars: [4.003834]\n",
      "10. Business ID: Tc24GX9-ZPr4_SHU0nJZZA, Actual Stars: 4.0, predicted Stars: [4.623581]\n"
     ]
    }
   ],
   "source": [
    "#Display 10 business\n",
    "\n",
    "for i in range(10):\n",
    "    print(\"{}. Business ID: {}, Actual Stars: {}, predicted Stars: {}\".format(i+1,merge_df['business_id'][2000+i],y_test_reg[i],pred_reg_sig_simple[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final score (RMSE): 0.5145193338394165\n",
      "R2 score: 0.74\n"
     ]
    }
   ],
   "source": [
    "# Measure RMSE error.  RMSE is common for regression.\n",
    "score_reg_sig = np.sqrt(mean_squared_error(y_test_reg,pred_reg_sig_simple))\n",
    "print(\"Final score (RMSE): {}\".format(score_reg_sig))\n",
    "print('R2 score: %.2f' % r2_score(y_test_reg, pred_reg_sig_simple))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training with early stopping and Model Checkpoint and Sigmoid **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup early stopping monitor\n",
    "\n",
    "monitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=3, verbose=2, mode='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up checkpointer\n",
    "checkpointer_sigmoid = ModelCheckpoint(filepath=\"./best_weights_sigmoid.hdf5\", verbose=2, save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/100\n",
      " - 1s - loss: 0.1241 - val_loss: 0.2657\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.26569, saving model to ./best_weights_sigmoid.hdf5\n",
      "Epoch 2/100\n",
      " - 0s - loss: 0.1234 - val_loss: 0.2644\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.26569 to 0.26444, saving model to ./best_weights_sigmoid.hdf5\n",
      "Epoch 3/100\n",
      " - 0s - loss: 0.1229 - val_loss: 0.2677\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.26444\n",
      "Epoch 4/100\n",
      " - 0s - loss: 0.1222 - val_loss: 0.2678\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.26444\n",
      "Epoch 5/100\n",
      " - 0s - loss: 0.1218 - val_loss: 0.2665\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.26444\n",
      "Epoch 00005: early stopping\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/100\n",
      " - 1s - loss: 0.1216 - val_loss: 0.2672\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.26444\n",
      "Epoch 2/100\n",
      " - 0s - loss: 0.1208 - val_loss: 0.2701\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.26444\n",
      "Epoch 3/100\n",
      " - 0s - loss: 0.1205 - val_loss: 0.2759\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.26444\n",
      "Epoch 4/100\n",
      " - 0s - loss: 0.1194 - val_loss: 0.2700\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.26444\n",
      "Epoch 00004: early stopping\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/100\n",
      " - 1s - loss: 0.1192 - val_loss: 0.2716\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.26444\n",
      "Epoch 2/100\n",
      " - 0s - loss: 0.1186 - val_loss: 0.2731\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.26444\n",
      "Epoch 3/100\n",
      " - 0s - loss: 0.1181 - val_loss: 0.2717\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.26444\n",
      "Epoch 4/100\n",
      " - 0s - loss: 0.1175 - val_loss: 0.2771\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.26444\n",
      "Epoch 00004: early stopping\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/100\n",
      " - 1s - loss: 0.1171 - val_loss: 0.2743\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.26444\n",
      "Epoch 2/100\n",
      " - 0s - loss: 0.1166 - val_loss: 0.2734\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.26444\n",
      "Epoch 3/100\n",
      " - 0s - loss: 0.1161 - val_loss: 0.2719\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.26444\n",
      "Epoch 4/100\n",
      " - 0s - loss: 0.1154 - val_loss: 0.2741\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.26444\n",
      "Epoch 5/100\n",
      " - 0s - loss: 0.1146 - val_loss: 0.2730\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.26444\n",
      "Epoch 6/100\n",
      " - 0s - loss: 0.1139 - val_loss: 0.2768\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.26444\n",
      "Epoch 00006: early stopping\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/100\n",
      " - 1s - loss: 0.1138 - val_loss: 0.2783\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.26444\n",
      "Epoch 2/100\n",
      " - 0s - loss: 0.1131 - val_loss: 0.2748\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.26444\n",
      "Epoch 3/100\n",
      " - 0s - loss: 0.1128 - val_loss: 0.2756\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.26444\n",
      "Epoch 4/100\n",
      " - 0s - loss: 0.1118 - val_loss: 0.2773\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.26444\n",
      "Epoch 5/100\n",
      " - 0s - loss: 0.1116 - val_loss: 0.2757\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.26444\n",
      "Epoch 00005: early stopping\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/100\n",
      " - 1s - loss: 0.1110 - val_loss: 0.2833\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.26444\n",
      "Epoch 2/100\n",
      " - 0s - loss: 0.1101 - val_loss: 0.2755\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.26444\n",
      "Epoch 3/100\n",
      " - 0s - loss: 0.1096 - val_loss: 0.2787\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.26444\n",
      "Epoch 4/100\n",
      " - 0s - loss: 0.1091 - val_loss: 0.2795\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.26444\n",
      "Epoch 5/100\n",
      " - 0s - loss: 0.1084 - val_loss: 0.2787\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.26444\n",
      "Epoch 00005: early stopping\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/100\n",
      " - 1s - loss: 0.1079 - val_loss: 0.2832\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.26444\n",
      "Epoch 2/100\n",
      " - 0s - loss: 0.1072 - val_loss: 0.2808\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.26444\n",
      "Epoch 3/100\n",
      " - 0s - loss: 0.1069 - val_loss: 0.2827\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.26444\n",
      "Epoch 4/100\n",
      " - 0s - loss: 0.1059 - val_loss: 0.2816\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.26444\n",
      "Epoch 5/100\n",
      " - 0s - loss: 0.1052 - val_loss: 0.2821\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.26444\n",
      "Epoch 00005: early stopping\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/100\n",
      " - 1s - loss: 0.1053 - val_loss: 0.2820\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.26444\n",
      "Epoch 2/100\n",
      " - 0s - loss: 0.1041 - val_loss: 0.2834\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.26444\n",
      "Epoch 3/100\n",
      " - 0s - loss: 0.1034 - val_loss: 0.2824\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.26444\n",
      "Epoch 4/100\n",
      " - 0s - loss: 0.1027 - val_loss: 0.2837\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.26444\n",
      "Epoch 00004: early stopping\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/100\n",
      " - 1s - loss: 0.1023 - val_loss: 0.2913\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.26444\n",
      "Epoch 2/100\n",
      " - 0s - loss: 0.1018 - val_loss: 0.2843\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.26444\n",
      "Epoch 3/100\n",
      " - 0s - loss: 0.1008 - val_loss: 0.2872\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.26444\n",
      "Epoch 4/100\n",
      " - 0s - loss: 0.1001 - val_loss: 0.2907\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.26444\n",
      "Epoch 5/100\n",
      " - 0s - loss: 0.0996 - val_loss: 0.2852\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.26444\n",
      "Epoch 00005: early stopping\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/100\n",
      " - 1s - loss: 0.0990 - val_loss: 0.2839\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.26444\n",
      "Epoch 2/100\n",
      " - 0s - loss: 0.0982 - val_loss: 0.2876\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.26444\n",
      "Epoch 3/100\n",
      " - 0s - loss: 0.0977 - val_loss: 0.2922\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.26444\n",
      "Epoch 4/100\n",
      " - 0s - loss: 0.0969 - val_loss: 0.2930\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.26444\n",
      "Epoch 00004: early stopping\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    model_reg_sig.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    model_reg_sig.fit(x_train_reg,y_train_reg,validation_data=(x_test_reg,y_test_reg),callbacks=[monitor,checkpointer_sigmoid],verbose=2,epochs=100) \n",
    "\n",
    "model_reg_sig.load_weights('./best_weights_sigmoid.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (2000, 1)\n"
     ]
    }
   ],
   "source": [
    "# Predict stars\n",
    "pred_reg_sig_stopping = model_reg_sig.predict(x_test_reg)\n",
    "print(\"Shape: {}\".format(pred_reg_sig_stopping.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Business ID: diaiQrxYFU1V5qxrFnW9fg, Actual Stars: 3.0, predicted Stars: [3.7838674]\n",
      "2. Business ID: TDTASGFy_aGp6vy0i23mDA, Actual Stars: 4.5, predicted Stars: [4.077084]\n",
      "3. Business ID: VuKJ2s_JP8weQ54NfsXJXQ, Actual Stars: 4.5, predicted Stars: [4.6271048]\n",
      "4. Business ID: aGiBg2WKOpXS5-1DRnBiAQ, Actual Stars: 3.0, predicted Stars: [3.479156]\n",
      "5. Business ID: ZMmgFw2P4LWsFXNn1ZGc1g, Actual Stars: 2.0, predicted Stars: [1.3433753]\n",
      "6. Business ID: sEKFq5u8P_s0-2mAZnx0JQ, Actual Stars: 4.5, predicted Stars: [4.165315]\n",
      "7. Business ID: rYziPPEILDXJ_F5uKR--YQ, Actual Stars: 2.0, predicted Stars: [2.0821998]\n",
      "8. Business ID: Swm_uMOWNcJDZz5lXWyzKA, Actual Stars: 5.0, predicted Stars: [4.9142914]\n",
      "9. Business ID: 6nGnVP7M4qQRiclXxeqXSQ, Actual Stars: 3.0, predicted Stars: [4.005081]\n",
      "10. Business ID: Tc24GX9-ZPr4_SHU0nJZZA, Actual Stars: 4.0, predicted Stars: [4.6286573]\n"
     ]
    }
   ],
   "source": [
    "#Display 10 business\n",
    "\n",
    "for i in range(10):\n",
    "    print(\"{}. Business ID: {}, Actual Stars: {}, predicted Stars: {}\".format(i+1,merge_df['business_id'][2000+i],y_test_reg[i],pred_reg_sig_stopping[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final score (RMSE): 0.5142343640327454\n",
      "R2 score: 0.74\n"
     ]
    }
   ],
   "source": [
    "# Measure RMSE error.  RMSE is common for regression.\n",
    "score_reg_sig_stopping = np.sqrt(mean_squared_error(y_test_reg,pred_reg_sig_stopping))\n",
    "print(\"Final score (RMSE): {}\".format(score_reg_sig_stopping))\n",
    "print('R2 score: %.2f' % r2_score(y_test_reg, pred_reg_sig_stopping))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Training without early stopping and Model Checkpoint and Tanh **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train test data\n",
    "x_train_reg_tanh, x_test_reg_tanh, y_train_reg_tanh, y_test_reg_tanh = train_test_split(x_matrix_zscore, y_stars_regression , test_size=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensorflow model for Regression with sigmoid\n",
    "model_reg_tanh = Sequential()\n",
    "\n",
    "model_reg_tanh.add(Dense(25, input_dim=x_train_reg_tanh.shape[1], activation='tanh'))  \n",
    "model_reg_tanh.add(Dense(10, activation='tanh')) # Hidden 2\n",
    "model_reg_tanh.add(Dense(1)) # Output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      " - 1s - loss: 2.1944\n",
      "Epoch 2/100\n",
      " - 0s - loss: 0.4490\n",
      "Epoch 3/100\n",
      " - 0s - loss: 0.2472\n",
      "Epoch 4/100\n",
      " - 0s - loss: 0.2231\n",
      "Epoch 5/100\n",
      " - 0s - loss: 0.2110\n",
      "Epoch 6/100\n",
      " - 0s - loss: 0.2029\n",
      "Epoch 7/100\n",
      " - 0s - loss: 0.1966\n",
      "Epoch 8/100\n",
      " - 0s - loss: 0.1922\n",
      "Epoch 9/100\n",
      " - 0s - loss: 0.1883\n",
      "Epoch 10/100\n",
      " - 0s - loss: 0.1829\n",
      "Epoch 11/100\n",
      " - 0s - loss: 0.1796\n",
      "Epoch 12/100\n",
      " - 0s - loss: 0.1764\n",
      "Epoch 13/100\n",
      " - 0s - loss: 0.1722\n",
      "Epoch 14/100\n",
      " - 0s - loss: 0.1702\n",
      "Epoch 15/100\n",
      " - 0s - loss: 0.1661\n",
      "Epoch 16/100\n",
      " - 0s - loss: 0.1625\n",
      "Epoch 17/100\n",
      " - 0s - loss: 0.1599\n",
      "Epoch 18/100\n",
      " - 0s - loss: 0.1572\n",
      "Epoch 19/100\n",
      " - 0s - loss: 0.1538\n",
      "Epoch 20/100\n",
      " - 0s - loss: 0.1525\n",
      "Epoch 21/100\n",
      " - 0s - loss: 0.1490\n",
      "Epoch 22/100\n",
      " - 0s - loss: 0.1468\n",
      "Epoch 23/100\n",
      " - 0s - loss: 0.1442\n",
      "Epoch 24/100\n",
      " - 0s - loss: 0.1427\n",
      "Epoch 25/100\n",
      " - 0s - loss: 0.1389\n",
      "Epoch 26/100\n",
      " - 0s - loss: 0.1358\n",
      "Epoch 27/100\n",
      " - 0s - loss: 0.1336\n",
      "Epoch 28/100\n",
      " - 0s - loss: 0.1310\n",
      "Epoch 29/100\n",
      " - 0s - loss: 0.1285\n",
      "Epoch 30/100\n",
      " - 0s - loss: 0.1263\n",
      "Epoch 31/100\n",
      " - 0s - loss: 0.1237\n",
      "Epoch 32/100\n",
      " - 0s - loss: 0.1212\n",
      "Epoch 33/100\n",
      " - 0s - loss: 0.1195\n",
      "Epoch 34/100\n",
      " - 0s - loss: 0.1162\n",
      "Epoch 35/100\n",
      " - 0s - loss: 0.1139\n",
      "Epoch 36/100\n",
      " - 0s - loss: 0.1118\n",
      "Epoch 37/100\n",
      " - 0s - loss: 0.1096\n",
      "Epoch 38/100\n",
      " - 0s - loss: 0.1075\n",
      "Epoch 39/100\n",
      " - 0s - loss: 0.1050\n",
      "Epoch 40/100\n",
      " - 0s - loss: 0.1027\n",
      "Epoch 41/100\n",
      " - 0s - loss: 0.1009\n",
      "Epoch 42/100\n",
      " - 0s - loss: 0.0996\n",
      "Epoch 43/100\n",
      " - 0s - loss: 0.0973\n",
      "Epoch 44/100\n",
      " - 0s - loss: 0.0949\n",
      "Epoch 45/100\n",
      " - 0s - loss: 0.0930\n",
      "Epoch 46/100\n",
      " - 0s - loss: 0.0918\n",
      "Epoch 47/100\n",
      " - 0s - loss: 0.0898\n",
      "Epoch 48/100\n",
      " - 0s - loss: 0.0877\n",
      "Epoch 49/100\n",
      " - 0s - loss: 0.0858\n",
      "Epoch 50/100\n",
      " - 0s - loss: 0.0837\n",
      "Epoch 51/100\n",
      " - 0s - loss: 0.0823\n",
      "Epoch 52/100\n",
      " - 0s - loss: 0.0802\n",
      "Epoch 53/100\n",
      " - 0s - loss: 0.0779\n",
      "Epoch 54/100\n",
      " - 0s - loss: 0.0763\n",
      "Epoch 55/100\n",
      " - 0s - loss: 0.0746\n",
      "Epoch 56/100\n",
      " - 0s - loss: 0.0732\n",
      "Epoch 57/100\n",
      " - 0s - loss: 0.0711\n",
      "Epoch 58/100\n",
      " - 0s - loss: 0.0696\n",
      "Epoch 59/100\n",
      " - 0s - loss: 0.0682\n",
      "Epoch 60/100\n",
      " - 0s - loss: 0.0656\n",
      "Epoch 61/100\n",
      " - 0s - loss: 0.0645\n",
      "Epoch 62/100\n",
      " - 0s - loss: 0.0628\n",
      "Epoch 63/100\n",
      " - 0s - loss: 0.0609\n",
      "Epoch 64/100\n",
      " - 0s - loss: 0.0594\n",
      "Epoch 65/100\n",
      " - 0s - loss: 0.0578\n",
      "Epoch 66/100\n",
      " - 0s - loss: 0.0564\n",
      "Epoch 67/100\n",
      " - 0s - loss: 0.0545\n",
      "Epoch 68/100\n",
      " - 0s - loss: 0.0530\n",
      "Epoch 69/100\n",
      " - 0s - loss: 0.0518\n",
      "Epoch 70/100\n",
      " - 0s - loss: 0.0504\n",
      "Epoch 71/100\n",
      " - 0s - loss: 0.0490\n",
      "Epoch 72/100\n",
      " - 0s - loss: 0.0475\n",
      "Epoch 73/100\n",
      " - 0s - loss: 0.0460\n",
      "Epoch 74/100\n",
      " - 0s - loss: 0.0453\n",
      "Epoch 75/100\n",
      " - 0s - loss: 0.0438\n",
      "Epoch 76/100\n",
      " - 0s - loss: 0.0428\n",
      "Epoch 77/100\n",
      " - 0s - loss: 0.0419\n",
      "Epoch 78/100\n",
      " - 0s - loss: 0.0407\n",
      "Epoch 79/100\n",
      " - 0s - loss: 0.0398\n",
      "Epoch 80/100\n",
      " - 0s - loss: 0.0385\n",
      "Epoch 81/100\n",
      " - 0s - loss: 0.0375\n",
      "Epoch 82/100\n",
      " - 0s - loss: 0.0370\n",
      "Epoch 83/100\n",
      " - 0s - loss: 0.0361\n",
      "Epoch 84/100\n",
      " - 0s - loss: 0.0351\n",
      "Epoch 85/100\n",
      " - 0s - loss: 0.0339\n",
      "Epoch 86/100\n",
      " - 0s - loss: 0.0333\n",
      "Epoch 87/100\n",
      " - 0s - loss: 0.0325\n",
      "Epoch 88/100\n",
      " - 0s - loss: 0.0318\n",
      "Epoch 89/100\n",
      " - 0s - loss: 0.0310\n",
      "Epoch 90/100\n",
      " - 0s - loss: 0.0305\n",
      "Epoch 91/100\n",
      " - 0s - loss: 0.0294\n",
      "Epoch 92/100\n",
      " - 0s - loss: 0.0287\n",
      "Epoch 93/100\n",
      " - 0s - loss: 0.0278\n",
      "Epoch 94/100\n",
      " - 0s - loss: 0.0275\n",
      "Epoch 95/100\n",
      " - 0s - loss: 0.0268\n",
      "Epoch 96/100\n",
      " - 0s - loss: 0.0262\n",
      "Epoch 97/100\n",
      " - 0s - loss: 0.0253\n",
      "Epoch 98/100\n",
      " - 0s - loss: 0.0246\n",
      "Epoch 99/100\n",
      " - 0s - loss: 0.0241\n",
      "Epoch 100/100\n",
      " - 0s - loss: 0.0233\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1b457ece80>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model training with sigmoid \n",
    "model_reg_tanh.compile(loss='mean_squared_error', optimizer='adam')\n",
    "model_reg_tanh.fit(x_train_reg_tanh,y_train_reg_tanh,verbose=2,epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (2000, 1)\n"
     ]
    }
   ],
   "source": [
    "# Predict stars\n",
    "pred_reg_tanh_simple = model_reg_tanh.predict(x_test_reg_tanh)\n",
    "print(\"Shape: {}\".format(pred_reg_tanh_simple.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Business ID: diaiQrxYFU1V5qxrFnW9fg, Actual Stars: 4.5, predicted Stars: [3.6931036]\n",
      "2. Business ID: TDTASGFy_aGp6vy0i23mDA, Actual Stars: 4.0, predicted Stars: [3.3532267]\n",
      "3. Business ID: VuKJ2s_JP8weQ54NfsXJXQ, Actual Stars: 3.5, predicted Stars: [3.4379675]\n",
      "4. Business ID: aGiBg2WKOpXS5-1DRnBiAQ, Actual Stars: 5.0, predicted Stars: [4.592262]\n",
      "5. Business ID: ZMmgFw2P4LWsFXNn1ZGc1g, Actual Stars: 4.0, predicted Stars: [3.8700895]\n",
      "6. Business ID: sEKFq5u8P_s0-2mAZnx0JQ, Actual Stars: 5.0, predicted Stars: [5.117735]\n",
      "7. Business ID: rYziPPEILDXJ_F5uKR--YQ, Actual Stars: 5.0, predicted Stars: [4.5631523]\n",
      "8. Business ID: Swm_uMOWNcJDZz5lXWyzKA, Actual Stars: 2.5, predicted Stars: [2.7859426]\n",
      "9. Business ID: 6nGnVP7M4qQRiclXxeqXSQ, Actual Stars: 4.0, predicted Stars: [3.7619247]\n",
      "10. Business ID: Tc24GX9-ZPr4_SHU0nJZZA, Actual Stars: 3.5, predicted Stars: [3.4295485]\n"
     ]
    }
   ],
   "source": [
    "#Display 10 business\n",
    "\n",
    "for i in range(10):\n",
    "    print(\"{}. Business ID: {}, Actual Stars: {}, predicted Stars: {}\".format(i+1,merge_df['business_id'][2000+i],y_test_reg_tanh[i],pred_reg_tanh_simple[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final score (RMSE): 0.6855879426002502\n",
      "R2 score: 0.56\n"
     ]
    }
   ],
   "source": [
    "# Measure RMSE error.  RMSE is common for regression.\n",
    "score_reg_tanh = np.sqrt(mean_squared_error(y_test_reg_tanh,pred_reg_tanh_simple))\n",
    "print(\"Final score (RMSE): {}\".format(score_reg_tanh))\n",
    "print('R2 score: %.2f' % r2_score(y_test_reg_tanh, pred_reg_tanh_simple))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Training with early stopping and Model Checkpoint and Tanh **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup early stopping monitor\n",
    "\n",
    "monitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=3, verbose=2, mode='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up checkpointer\n",
    "checkpointer_tanh = ModelCheckpoint(filepath=\"./best_weights_tanh.hdf5\", verbose=2, save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/100\n",
      " - 1s - loss: 0.0235 - val_loss: 0.4838\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.48381, saving model to ./best_weights_tanh.hdf5\n",
      "Epoch 2/100\n",
      " - 0s - loss: 0.0228 - val_loss: 0.4708\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.48381 to 0.47084, saving model to ./best_weights_tanh.hdf5\n",
      "Epoch 3/100\n",
      " - 0s - loss: 0.0220 - val_loss: 0.4725\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.47084\n",
      "Epoch 4/100\n",
      " - 0s - loss: 0.0214 - val_loss: 0.4769\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.47084\n",
      "Epoch 5/100\n",
      " - 0s - loss: 0.0210 - val_loss: 0.4794\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.47084\n",
      "Epoch 00005: early stopping\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/100\n",
      " - 1s - loss: 0.0207 - val_loss: 0.4726\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.47084\n",
      "Epoch 2/100\n",
      " - 0s - loss: 0.0201 - val_loss: 0.4735\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.47084\n",
      "Epoch 3/100\n",
      " - 0s - loss: 0.0194 - val_loss: 0.4800\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.47084\n",
      "Epoch 4/100\n",
      " - 0s - loss: 0.0192 - val_loss: 0.4888\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.47084\n",
      "Epoch 00004: early stopping\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/100\n",
      " - 1s - loss: 0.0190 - val_loss: 0.4852\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.47084\n",
      "Epoch 2/100\n",
      " - 0s - loss: 0.0180 - val_loss: 0.4882\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.47084\n",
      "Epoch 3/100\n",
      " - 0s - loss: 0.0173 - val_loss: 0.4849\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.47084\n",
      "Epoch 4/100\n",
      " - 0s - loss: 0.0171 - val_loss: 0.4851\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.47084\n",
      "Epoch 00004: early stopping\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/100\n",
      " - 1s - loss: 0.0172 - val_loss: 0.4892\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.47084\n",
      "Epoch 2/100\n",
      " - 0s - loss: 0.0164 - val_loss: 0.4832\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.47084\n",
      "Epoch 3/100\n",
      " - 0s - loss: 0.0161 - val_loss: 0.4931\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.47084\n",
      "Epoch 4/100\n",
      " - 0s - loss: 0.0153 - val_loss: 0.4966\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.47084\n",
      "Epoch 5/100\n",
      " - 0s - loss: 0.0154 - val_loss: 0.4917\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.47084\n",
      "Epoch 00005: early stopping\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/100\n",
      " - 1s - loss: 0.0152 - val_loss: 0.5032\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.47084\n",
      "Epoch 2/100\n",
      " - 0s - loss: 0.0145 - val_loss: 0.4999\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.47084\n",
      "Epoch 3/100\n",
      " - 0s - loss: 0.0143 - val_loss: 0.5003\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.47084\n",
      "Epoch 4/100\n",
      " - 0s - loss: 0.0141 - val_loss: 0.4941\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.47084\n",
      "Epoch 5/100\n",
      " - 0s - loss: 0.0137 - val_loss: 0.4997\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.47084\n",
      "Epoch 6/100\n",
      " - 0s - loss: 0.0133 - val_loss: 0.4954\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.47084\n",
      "Epoch 7/100\n",
      " - 0s - loss: 0.0131 - val_loss: 0.4977\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.47084\n",
      "Epoch 00007: early stopping\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/100\n",
      " - 1s - loss: 0.0131 - val_loss: 0.4972\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.47084\n",
      "Epoch 2/100\n",
      " - 0s - loss: 0.0122 - val_loss: 0.5029\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.47084\n",
      "Epoch 3/100\n",
      " - 0s - loss: 0.0122 - val_loss: 0.4976\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.47084\n",
      "Epoch 4/100\n",
      " - 0s - loss: 0.0117 - val_loss: 0.5058\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.47084\n",
      "Epoch 00004: early stopping\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/100\n",
      " - 1s - loss: 0.0118 - val_loss: 0.5112\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.47084\n",
      "Epoch 2/100\n",
      " - 0s - loss: 0.0113 - val_loss: 0.5017\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.47084\n",
      "Epoch 3/100\n",
      " - 0s - loss: 0.0110 - val_loss: 0.5030\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.47084\n",
      "Epoch 4/100\n",
      " - 0s - loss: 0.0114 - val_loss: 0.5076\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.47084\n",
      "Epoch 5/100\n",
      " - 0s - loss: 0.0106 - val_loss: 0.5151\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.47084\n",
      "Epoch 00005: early stopping\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/100\n",
      " - 1s - loss: 0.0106 - val_loss: 0.5045\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.47084\n",
      "Epoch 2/100\n",
      " - 0s - loss: 0.0100 - val_loss: 0.5010\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.47084\n",
      "Epoch 3/100\n",
      " - 0s - loss: 0.0100 - val_loss: 0.5040\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.47084\n",
      "Epoch 4/100\n",
      " - 0s - loss: 0.0098 - val_loss: 0.5114\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.47084\n",
      "Epoch 5/100\n",
      " - 0s - loss: 0.0095 - val_loss: 0.5091\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.47084\n",
      "Epoch 00005: early stopping\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/100\n",
      " - 1s - loss: 0.0098 - val_loss: 0.5169\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.47084\n",
      "Epoch 2/100\n",
      " - 0s - loss: 0.0091 - val_loss: 0.5036\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.47084\n",
      "Epoch 3/100\n",
      " - 0s - loss: 0.0088 - val_loss: 0.5117\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.47084\n",
      "Epoch 4/100\n",
      " - 0s - loss: 0.0086 - val_loss: 0.5145\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.47084\n",
      "Epoch 5/100\n",
      " - 0s - loss: 0.0089 - val_loss: 0.5135\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.47084\n",
      "Epoch 00005: early stopping\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/100\n",
      " - 1s - loss: 0.0086 - val_loss: 0.5143\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.47084\n",
      "Epoch 2/100\n",
      " - 0s - loss: 0.0083 - val_loss: 0.5039\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.47084\n",
      "Epoch 3/100\n",
      " - 0s - loss: 0.0081 - val_loss: 0.5138\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.47084\n",
      "Epoch 4/100\n",
      " - 0s - loss: 0.0078 - val_loss: 0.5154\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.47084\n",
      "Epoch 5/100\n",
      " - 0s - loss: 0.0078 - val_loss: 0.5074\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.47084\n",
      "Epoch 00005: early stopping\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    model_reg_tanh.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    model_reg_tanh.fit(x_train_reg_tanh,y_train_reg_tanh,validation_data=(x_test_reg_tanh,y_test_reg_tanh),callbacks=[monitor,checkpointer_tanh],verbose=2,epochs=100) \n",
    "\n",
    "model_reg_tanh.load_weights('./best_weights_tanh.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (2000, 1)\n"
     ]
    }
   ],
   "source": [
    "# Predict stars\n",
    "pred_reg_tanh_stopping = model_reg_tanh.predict(x_test_reg_tanh)\n",
    "print(\"Shape: {}\".format(pred_reg_tanh_stopping.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Business ID: diaiQrxYFU1V5qxrFnW9fg, Actual Stars: 4.5, predicted Stars: [3.6914728]\n",
      "2. Business ID: TDTASGFy_aGp6vy0i23mDA, Actual Stars: 4.0, predicted Stars: [3.3225281]\n",
      "3. Business ID: VuKJ2s_JP8weQ54NfsXJXQ, Actual Stars: 3.5, predicted Stars: [3.5386271]\n",
      "4. Business ID: aGiBg2WKOpXS5-1DRnBiAQ, Actual Stars: 5.0, predicted Stars: [4.5618243]\n",
      "5. Business ID: ZMmgFw2P4LWsFXNn1ZGc1g, Actual Stars: 4.0, predicted Stars: [3.8573842]\n",
      "6. Business ID: sEKFq5u8P_s0-2mAZnx0JQ, Actual Stars: 5.0, predicted Stars: [5.104616]\n",
      "7. Business ID: rYziPPEILDXJ_F5uKR--YQ, Actual Stars: 5.0, predicted Stars: [4.4667773]\n",
      "8. Business ID: Swm_uMOWNcJDZz5lXWyzKA, Actual Stars: 2.5, predicted Stars: [2.7601504]\n",
      "9. Business ID: 6nGnVP7M4qQRiclXxeqXSQ, Actual Stars: 4.0, predicted Stars: [3.7418845]\n",
      "10. Business ID: Tc24GX9-ZPr4_SHU0nJZZA, Actual Stars: 3.5, predicted Stars: [3.4225194]\n"
     ]
    }
   ],
   "source": [
    "#Display 10 business\n",
    "\n",
    "for i in range(10):\n",
    "    print(\"{}. Business ID: {}, Actual Stars: {}, predicted Stars: {}\".format(i+1,merge_df['business_id'][2000+i],y_test_reg_tanh[i],pred_reg_tanh_stopping[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final score (RMSE): 0.6861766576766968\n",
      "R2 score: 0.55\n"
     ]
    }
   ],
   "source": [
    "# Measure RMSE error.  RMSE is common for regression.\n",
    "score_reg_tanh_stopping = np.sqrt(mean_squared_error(y_test_reg_tanh,pred_reg_tanh_stopping))\n",
    "print(\"Final score (RMSE): {}\".format(score_reg_tanh_stopping))\n",
    "print('R2 score: %.2f' % r2_score(y_test_reg_tanh, pred_reg_tanh_stopping))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAE9CAYAAADwAyL0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAIABJREFUeJzt3Xu8ZXP9x/HXey4uueQy45JLQyaRSxiEMHKbcZtJFKHItVAiUQrp10WKSiQhlRqVbu4qiXKpGT+Ry09NiEllRCKSy+f3x+e7zeo0M2efc/bZ65w17+fjcR7nrL3X2eu71977s7/re/l8FRGYmVmzjKi7AGZm1nkO7mZmDeTgbmbWQA7uZmYN5OBuZtZADu5mZg3k4G5m1kAO7mZmDeTgbmbWQKPqOvCYMWNi3LhxdR3ezGxYuvXWWx+NiLG97VdbcB83bhwzZsyo6/BmZsOSpD+2s5+bZczMGsjB3cysgXoN7pIukPSIpDt72W9jSS9I2qNzxTMzs/5op+Z+ITBpfjtIGgmcClzTgTKZmdkA9RrcI+IG4LFedjsS+B7wSCcKZWZmAzPgNndJKwFvAs5pY99DJM2QNGP27NkDPbSZmc1DJzpUPwccFxEv9LZjRJwbERMiYsLYsb0O0zQzs37qxDj3CcDFkgDGADtJej4iftiBxzYzs34YcHCPiNVaf0u6ELh8QQ7s446/otbjP/CpnWs9vpkNDb0Gd0nTgInAGEmzgJOA0QAR0Ws7u5mZdV+vwT0i9m73wSJi/wGVxszMOsIzVM3MGsjB3cysgRzczcwayMHdzKyBHNzNzBrIwd3MrIEc3M3MGsjB3cysgRzczcwayMHdzKyBHNzNzBrIwd3MrIEc3M3MGsjB3cysgRzczcwayMHdzKyBHNzNzBrIwd3MrIEc3M3MGsjB3cysgRzczcwaqNfgLukCSY9IunMe9+8j6Y7yc5Ok9TtfTDMz64t2au4XApPmc//9wNYRsR7wMeDcDpTLzMwGYFRvO0TEDZLGzef+myqbtwArD7xYZmY2EJ1ucz8QuGped0o6RNIMSTNmz57d4UObmVlLx4K7pG3I4H7cvPaJiHMjYkJETBg7dmynDm1mZj302izTDknrAecBkyPib514TDMz678B19wlrQp8H9gvIn438CKZmdlA9VpzlzQNmAiMkTQLOAkYDRAR5wAnAssCZ0sCeD4iJgxWgc3MrHftjJbZu5f7DwIO6liJzMxswDxD1cysgRzczcwayMHdzKyBHNzNzBrIwd3MrIEc3M3MGsjB3cysgRzczcwayMHdzKyBHNzNzBrIwd3MrIEc3M3MGsjB3cysgRzczcwayMHdzKyBHNzNzBrIwd3MrIEc3M3MGsjB3cysgRzczcwayMHdzKyBeg3uki6Q9IikO+dxvyR9QdJMSXdI2rDzxTQzs75op+Z+ITBpPvdPBsaXn0OALw28WGZmNhC9BveIuAF4bD67TAG+HukWYClJK3aqgGZm1nedaHNfCXiosj2r3PZfJB0iaYakGbNnz+7Aoc3MbG46Edw1l9tibjtGxLkRMSEiJowdO7YDhzYzs7npRHCfBaxS2V4ZeLgDj2tmZv3UieB+KfD2Mmrm9cATEfHnDjyumZn106jedpA0DZgIjJE0CzgJGA0QEecAVwI7ATOBp4EDBquwZmbWnl6De0Ts3cv9ARzesRKZmdmAeYaqmVkDObibmTWQg7uZWQM5uJuZNZCDu5lZAzm4m5k1kIO7mVkDObibmTWQg7uZWQM5uJuZNZCDu5lZAzm4m5k1kIO7mVkDObibmTWQg7uZWQM5uJuZNZCDu5lZAzm4m5k1kIO7mVkDObibmTWQg7uZWQO1FdwlTZJ0r6SZko6fy/2rSrpO0m2S7pC0U+eLamZm7eo1uEsaCZwFTAbWBvaWtHaP3T4MfCciNgD2As7udEHNzKx97dTcNwFmRsR9EfFv4GJgSo99Aliy/P1y4OHOFdHMzPqqneC+EvBQZXtWua3qZGBfSbOAK4Ej5/ZAkg6RNEPSjNmzZ/ejuGZm1o52grvmclv02N4buDAiVgZ2Ar4h6b8eOyLOjYgJETFh7NixfS+tmZm1pZ3gPgtYpbK9Mv/d7HIg8B2AiLgZWAQY04kCmplZ37UT3KcD4yWtJmkhssP00h77PAhsCyBpLTK4u93FzKwmvQb3iHgeOAK4BriHHBVzl6RTJO1WdjsGOFjS7cA0YP+I6Nl0Y2ZmXTKqnZ0i4kqyo7R624mVv+8Gtuhs0czMrL88Q9XMrIEc3M3MGsjB3cysgRzczcwayMHdzKyBHNzNzBrIwd3MrIEc3M3MGsjB3cysgRzczcwayMHdzKyB2sotY2bWH+OOv6K2Yz/wqZ3ne/9QLlsnuOZuZtZADu5mZg3k4G5m1kAO7mZmDeTgbmbWQA7uZmYN5OBuZtZADu5mZg3k4G5m1kBtBXdJkyTdK2mmpOPnsc9bJN0t6S5J3+psMc3MrC96TT8gaSRwFrA9MAuYLunSiLi7ss944IPAFhHxuKTlBqvAZmbWu3Zq7psAMyPivoj4N3AxMKXHPgcDZ0XE4wAR8Uhni2lmZn3RTnBfCXiosj2r3Fb1auDVkm6UdIukSXN7IEmHSJohacbs2bP7V2IzM+tVO8Fdc7ktemyPAsYDE4G9gfMkLfVf/xRxbkRMiIgJY8eO7WtZzcysTe2k/J0FrFLZXhl4eC773BIRzwH3S7qXDPbTO1JK64g6U5xCd9KcmllqJ7hPB8ZLWg34E7AX8LYe+/yQrLFfKGkM2UxzXycLas3mLx6zzuq1WSYingeOAK4B7gG+ExF3STpF0m5lt2uAv0m6G7gOODYi/jZYhTYzs/lrayWmiLgSuLLHbSdW/g7g6PJjZmY18wxVM7MGGpZrqLp91sxs/lxzNzNrIAd3M7MGcnA3M2sgB3czswZycDcza6BhOVrGrNvqHKHV2+isoVw2q49r7mZmDeTgbmbWQA7uZmYN5OBuZtZADu5mZg3k4G5m1kAO7mZmDeTgbmbWQA7uZmYN5OBuZtZADu5mZg3k4G5m1kAO7mZmDdRWcJc0SdK9kmZKOn4+++0hKSRN6FwRzcysr3oN7pJGAmcBk4G1gb0lrT2X/ZYA3gP8qtOFNDOzvmmn5r4JMDMi7ouIfwMXA1Pmst/HgE8D/+pg+czMrB/aCe4rAQ9VtmeV214iaQNglYi4fH4PJOkQSTMkzZg9e3afC2tmZu1pJ7hrLrfFS3dKI4AzgGN6e6CIODciJkTEhLFjx7ZfSjMz65N2gvssYJXK9srAw5XtJYB1gJ9LegB4PXCpO1XNzOrTTnCfDoyXtJqkhYC9gEtbd0bEExExJiLGRcQ44BZgt4iYMSglNjOzXvUa3CPieeAI4BrgHuA7EXGXpFMk7TbYBTQzs74b1c5OEXElcGWP206cx74TB14sMzMbCM9QNTNrIAd3M7MGcnA3M2sgB3czswZycDczayAHdzOzBnJwNzNrIAd3M7MGcnA3M2sgB3czswZycDczayAHdzOzBnJwNzNrIAd3M7MGcnA3M2sgB3czswZycDczayAHdzOzBnJwNzNrIAd3M7MGcnA3M2ugtoK7pEmS7pU0U9Lxc7n/aEl3S7pD0rWSXtn5opqZWbt6De6SRgJnAZOBtYG9Ja3dY7fbgAkRsR5wCfDpThfUzMza107NfRNgZkTcFxH/Bi4GplR3iIjrIuLpsnkLsHJni2lmZn3RTnBfCXiosj2r3DYvBwJXze0OSYdImiFpxuzZs9svpZmZ9Uk7wV1zuS3muqO0LzABOG1u90fEuRExISImjB07tv1SmplZn4xqY59ZwCqV7ZWBh3vuJGk74ARg64h4tjPFMzOz/min5j4dGC9pNUkLAXsBl1Z3kLQB8GVgt4h4pPPFNDOzvug1uEfE88ARwDXAPcB3IuIuSadI2q3sdhqwOPBdSb+RdOk8Hs7MzLqgnWYZIuJK4Moet51Y+Xu7DpfLzMwGwDNUzcwayMHdzKyBHNzNzBrIwd3MrIEc3M3MGsjB3cysgRzczcwayMHdzKyBHNzNzBrIwd3MrIEc3M3MGsjB3cysgRzczcwayMHdzKyBHNzNzBrIwd3MrIEc3M3MGsjB3cysgRzczcwayMHdzKyBHNzNzBqoreAuaZKkeyXNlHT8XO5fWNK3y/2/kjSu0wU1M7P29RrcJY0EzgImA2sDe0tau8duBwKPR8QawBnAqZ0uqJmZta+dmvsmwMyIuC8i/g1cDEzpsc8U4Gvl70uAbSWpc8U0M7O+UETMfwdpD2BSRBxUtvcDNo2IIyr73Fn2mVW2/1D2ebTHYx0CHFI21wTu7dQT6aMxwKO97lUPl61/XLb+cdn6p86yvTIixva206g2HmhuNfCe3wjt7ENEnAuc28YxB5WkGRExoe5yzI3L1j8uW/+4bP0zlMvW0k6zzCxglcr2ysDD89pH0ijg5cBjnSigmZn1XTvBfTowXtJqkhYC9gIu7bHPpcA7yt97AD+L3tp7zMxs0PTaLBMRz0s6ArgGGAlcEBF3SToFmBERlwLnA9+QNJOsse81mIXugNqbhubDZesfl61/XLb+GcplA9roUDUzs+HHM1TNzBrIwd3MrIEc3M3MGsjB3WweJC3SxWMt2q1j1WEgM9YlLTBxStLoTj3WAnPSrHOanFqi9dwkvRb4uqRVevmXThxzWeA4SZMG+1h1iYiQtPncEg/Oj6RREfFi+Xv1alLCpr0PS86uPSQt0YnHc3AfZJI2kfR6SevUXZZ5kbSppG0lrd/O/q05DJJeXQJTY5QgtB1wOLAW8IkuZDldlByWvJWkNw7ysbqq8mW5CbAncIykY9v833WBncrf7wV+CFws6WR46bUa1gG+cn62Bj5HJl7cTdLSA31sB/dBUHnBtiQneB0MfFrS7rUWbC7Km+qHwO7ANyX1TAo3r/9bEziRDr0Rh4oSUM4nE+F9CLgfOG2wavCSRpScTL8CVgMOlLT5YByrDiUATwS+BVwLnAxMlXRSG/++NbCvpIOALYFtgL2Bg8s8m2Ef4Ev5NwPOBI4jM/C+CZgkafGBPHY7uWWsjyov2BuBN0XEzZLeDBwpKSLiBzUXEQBJG5CpnPeOiJ9L2g34nCQi4keV/dRzxnFE3CvpZ2TW0BckXRYRj3f1CXRQ5TkuDvw8In5Vbr+HrE19UtJxEfGnDh/zRUk7Ah8Fvkp+sHeXtEhE/KxTx6rZK4BzI+LykkL8l8A0SU9GxOk9d269FhHxRUnPAzsDzwHPRsT95bN1g6TFI+LoBsyGXx+YHhG3AbdJejv5JThC0o8i4qn+PKhr7h1W6fw5HHh35a4ryW/nD0ras+sFq6iU8VAyXfPSkkaW2cbvBc7rcZXxisr/7izpGICIuAC4maxR7SRpqa48gQ6q1PpGlt8PAJuV7KdExEzgf4F/k7XqAXd4SVpV0stKJWA02fRwRkScCRwEPA3sV678hp251KRHAPuXL6wXgDuB64E9SyD7j/+tBuuIOIdMM74YsLWkpSPiQWAiWbsdO9xq7pUr+1a5fwMsImk9gIj4OnAbsAuwRn+P4+DeIZUXanGAiNgX+C5wsqSFI+IZ4CrgNOChmsu4RCnjYcD3ydriquWDdRnZjPS40suB/5N0WPnfkcB2kt5dHuPrwJ+BDwI7apiNbCgBdgfgq5L2B/5FfjHvLekDJcBuB9wKLB8Rz3XgsAcBa5bz/Rwwm/LlGBH3AdOANwBTJI3pwPG6qpzTbSQdI2mDiLgIuBy4VtKKwOuBJcnmwBVb/1cN7JIOlnRKuZr8DnlO3ga8QdIyEfEAsF5EzB5uNfdyft4IHCbpTRFxC5m2ZYqkqZImkP0wT5B9FP378ooI/wzwhzlpHHYAfgScDRxWbvsKcAWwaHXfmsv4Y+Ac4CPlts+STQJrVMtX+Z8tydzV+5XtHYEfAEeW7d2AbwLL1f1a9OF8jCq/NyWbCT4AXAYcD0wANiyv28XAesC2ZH6lJTp0/FWA68gg92rgdPJKajTwynJ+X1P3eerne2wT4LdkO/vXgSPJCsWnynvvVnJVtwOBb5AVhur7bntgBvBxMofLyWWfvck+rElkxbSWz9IAzs/I8ntz8grxw2TF6AgymB8LfA+4kWyq2RH4Uuv/+ny8up/wcP6pnnRgI+A+sg377cAXgU+X+34AXA2MqKGMoyp/bwj8jmwG2KZ8+L5S7ruA7ET8jy+hyhtyS+DvwDvK9vbk5eTl5YM8vu7Xo83zsUrl71cBNwH7VM7PmcAJrf3ItQq2Ae4B1u1wWS4pXyBLAbsCXwZuKedz17rPVR+ex2KVvzcmO043KNtvJvssDi/nckQJ9NsA/we8tsdj7U9+6a1RtrcCPk923o8kR9y8ou7n3MfzsyywcPl7PeALwO5le3XgEeA9lf2XIq+mbwPW7/dx637iw/WHbIeeAowu29sCp5W/FwLGl+C5ZrltwxrKuFz5sCxStjcHPlP+HkE2IX0feH25bb3yu1qLWrv1YSJrtH8H3l62VwT2AVar+/Xowzk5F3hd5TW8gqwpvbzctj45Wuaj5fyMJpsD1hjgcVtflq8t75VlyvY5ZH/MkmX7dQM9VpfP5xLkFc+yZXsr4Bngo5V93kR+cb2fHMSxHHBMORfq8XhbAi8CJ5TtkeW284EP1v18+3F+FivPdVzZ3o9Mo35y5T3wKrKf5WOV//t46/PY72PX/eSH6095w60NLE1+M7+ObEvforLPN4GpNZZxAtnUMhYYB6xLXgZuVNnnLODN8/j/Y8l+gu8CHy+3bUG2ER9Z92swgPPyKuCa8vcy5FXWV4Glym3r06NG2aHjTiFrY1eSbcjvK7efQ15BdKTJp4bzOYZcNnO3sr0defVxSGWfPYC1Ktuj+c9KxAqVYNe6Snxn2R5JVkyWr/u59uPcjCjvsVeULzcBbwUuIq+gW5WKVwHbdfTYdT/54fxDtpNdTLYpLkY2x/y4fIg3LB/kCTWXcWGyqeGUUt59gD+Qw8smljJuzn/XoLYBflz+/jZ5FdK6StmSbDNcquf/DZcfst33ivL38uQEkm+3AvwgHG9xsr143bK9LdlcsUvZvphcd7j2c9OH51QNzm8lOwV3rbx/pvesBDDnCuaVldveT/ZV/aLyBbE5WRE5vO7n2aHzsxPZ7HlU2d6/bE+tvuc6+Xmq/QQMt5/Km3P18i28C3Ah8E6ylrwncANlFErNZXwN2UQ0EfgMOaJlGbIW9SOy86b1YVq4x2NMKvu3au+tNsMJ5feidb8W/Tgf46sBtDyvVg1+RbIjfECXwvMpw1LkRKVJZXsRsm3/C3WfnwGe05Uobe5kP8z9lffU9mQNfhUq/U1kv9TvyeU43wX8tNx+Ndkn1Gr225qsiLy8k0Gvy+dnBeBl5e83lPfYMWX7YPIKblAGItR+EobjD1kz/ylz2qh3JZtgDqT0/FeCYV2jY3YjxxK3Ora2ITtyjmPOpWCrJr4D2el7EqWJpnwxXFu+qFr7HUl2oC42DD9sO5Ljq39FfhlPLrdfBVxf/l6og8drfbhXY0579AFkm//GlTJdVM5n1zvbO/DcJpfz+ePy3lmRnLj3O+Z0GC4zl9fhbub08xxUztHR5NDIt5E19neV+xep+/kO4DztRHbEX05emY0uX1hfpPQfACsN2vHrPgHD7YccDXA7ZZgaebm9MNkM80NyOFvHgkQ/y7g+WWNqdea+vASQ8eTQzBOAl5UvoUnlA3o42Yn4lfJhW4xsyjmDHLL17vK816n7NejH+VinfMDWKM/5ULIZ5lXl/htbAbfDx90JuIOc6LUP2Zx1RLntE2Qtd3Ld56efz21jskKwXgnoR5N9BwuRV4YPkh2n1aaJHYC/kqOEXlO5fUWyH6LV53ElObZ9WPZBlOewJlmJ2Iy8SrsU+F65b3tyiOOgDkRw+oG+W478Nl6iJEDalmyz3Yl8c/8lIv5dY/kgO7j+CCws6QSyWWYdYAPyDfdYRDwtaRnygzQlIi6TtDLZS79C5DTvU5mT02NxMk3B3V1/NgOgzLC3Jzn2ekxEzJQ0jeyHeCc5KmOLDh5PERHKdMF7kDXRVYC3kG3Kl5NjuMcBl0XEzZ06dreUmcjvA1aPiDvKbX8mhwNvExGXSLoxIh6p/M+2ZI31aPLzsr+kKyLiFxHxZ0mPA0dL+gPwJPCBiHiyy09twMokvuXIK7IngAci4l9kDqbrJR1JDmL4TUTMHsyyDKvZhHWoTBXetEwPvpV80c4nJ/YcSw4FWyciro6I39RYxteXGZXXAf8gmwD+RAaYbwCbRcSNEXEPQEQ8RjYpfUrSkpEJrMaSeVS+QHb6zIiI48jLyGER2CvnY0QJEF8iL4sPkLRuRPyDHAI5RtIinZxVWwL7zmQah+WAmRFxFVlb3YrsQPt9RFw8nAJ7dZZkRPydPKfPSfqfcts9ZK18QtntkR4P8Q9g/4j4JnnunyNn5b6h3H8h+d57F3BKRPxxkJ7KoGidn4h4MSL+Qo7LXxzYXHNS+H6XnDfy4mAHdkph/NP7JdZkcoLS6yu3tSb7TCAnY9Q60oG8cvg98IbKba1ZmBvPr4zM6eA6k+xL2JNsuphOTm5asu7XoB/nYzey9vQDcgjoBPIDdys5OuMmYOdBOO7ryJr5MeSEpO/3KNM0YNW6z08/n9t2wEfIJroxZAfhxeU5bVXO7ba9PMaI8ns82ez3Kf5zaO7L636eAzg/O5DNfUcAq5Lt6zeSM8D3Bu6idKh3pTx1n5Ch/kO2P9/BnI7J9cqLtkoJGvdShrPVWMZXlg/WOmV7IzKF7/LkRJGZvZWxfHBfpDKWmLyyG1P3a9CP87FO+VBtS/YjXEE2Ta1M1ji/C+zReo4dPO66ZA306LK9JDmE9NuVfcbWfX76+dw2I/tx3lXO7cfIOQ+bl8/Hz5gzkmpUm485nuyIPRPYvNw23Drqq5PTbiD7sz5OJptbi2zWvI0czLBJN8vWKpjNQ2k7/Rg5sy7IjtPHyHwk3yKHMd1RXwlfuiQ8gxxu9zg50uUZsrb+CbLj5rdtPM5kcsjkGyPir4NX4sGjXM3mBLJf4chy20FkW+8W5NC9XcnO1VMj4ncdOu4i5Bf+aWSb8ckR8QdJS5L5VYiIqaWp6MVOHLNbNCd3//URca6ksWQuHkXE+yVtQfZfPBgRH+3jY7+GnMF6XnSjqWIQSNqUnAT3sYiYVj6P+5Bj/6eQFa3Dyc/olRHxfDfK5Tb3HirttWso852PJmvFI8hhc5PJ8bjLR8Rf6gjslTKurVzY4RVkb/wT5febyS+ehSPiqXYCO0Bk2/CHgKuGW3bHiqfIL7ZXS9oYICLOI0f6jI+IO8mhe78j24H7rfI6rEnWPp8mv0QgM/yNi2zfb+XnZrgF9mINshlmsqRXliD8CTI76CrkaKtvkJlF+5TFMiL+j0yJMSwDe3E3OSLmYHhppbKryffiMhFxCTlh6dCyX1e45j4XJc3oyeSomJeRtbDby32bkhMRPlyCYV1lnEoOUbydHOp4RkTcWO7bkpJsKSIu78djLx79XCCg2yqjUyaQH5x/kB+2z5M16DvIQP59clTQbeX/Fo6IZztw3B3J4L0hOa/gk6Ucx5P9GNMi4v5+P8EaVJ7basA/ySvVdcgx6Q+Qk99GkP0ZO0fEQ5JGkfMhnqmp2F1TOT+vIvve7pT0MrKv5Vdk09Xa5HDOXaMMYCiDFgZUoeiTutushtoPOfP0p2TAfCvZztjKebEe2V47NaK+9kGy7fhqciz6fmT73rJk09GaZK291jJ2+XxMIicofaG8XseRV1xfIDuFvwTsUPbtV/rUyrGqMy3XIvsz1iWbek4kh8O+ggyG3yKHC9Z+jvrxPCeTX5LTyvtrZbLT9BLg12Rg33FBeY/N5fxMJQP59eTckClkeo97yC/1/yGHhb70nun2ear9JA2lH7IDclGyzfZYcvLJ6uW+1mSE5ep4oSplXIGcBftZsgnlph5lHMOcGZGN/tCRE5JeBvyEOTNOlyWnrB9Mdmh+iRyVMeBMi2Sb+qHMmbG7BSU/Tdl+HZnz/SvlvVTrZLYBPs8ZlCR4ZKdnqwKxEdkEdSxzso02+n02l/fcWHLOwlrlPbZneZ+tQw5//DVwUfV/6ijrcG1X7ThJ25DthuuQtd+9yKx09ykX+D0PWDnKxIwor1qXyziRbFNfg6yl70UmZrqvlP988irjb3WVcbCVcekrlc0VyBr6H4GHAcpzP4iccfoP4FQy496ekhYewHGXIztjpwNLKVeoai2P1mpr/Q05/PEF8qrvxeHQdyFp4dKsQJnY9g/ySugBgMhO0lvICV+3kn0Wa5LzBkY18X1W1eN9sxSZsXJR4J/lPXYt+ZrvHNmcuS2wraQzoL7PoWeoApLGk3lhPhIR0yVdSr6Ib5X0LLAvcFzkepp1lXFN4CjgiMjFqa8la62HSroPeAdwbHRo9McQtgGwXglC25Fjx58ga8ublH0WB1ZQrlP6gKTjgeein23sZUTHD8l29F+QlYDfkZfeZwE7lH0uI5uIvkUuJXfmUA98pVN4M3LZvyfJoY2fJ2vpO5HnFXKY37oAkbOZnwNuiy6N/KhLOT9vKb//SDa9TSID+pGSzoiIhyX9GpigXCf2yRJTlquv5Cy4zTLM6UwWmf3wTsr45HL7dmTAPwHYuo7Lq+rxSlnupyxiUG7bmPzieQ+wVR1l7OK5WIGcGLIk2e/xGCV9arn/IrJ2+YnyWu5cbh/QOHYyTcCdwIGV28aQwf4Ysga7AXOWlNuAnNxzNUN88hewdOU5Xk0m7Gr11WxKXpl8qnw+bgd2qrvMXT4/S5X4sDjwF3LWbSvp26bkEOmbyEld9wPbl/sG1K/TqZ8FerSMpI3Ib9cfk7XiVYGro8ZRMD2V6dlLR9aWDiAnUF0TEdNqLlrXSFqLHEf8pYj4mqT9yEvfmcDNEXFt2W8PcoTMUxFxY2tUwwCPfQC5ctN7SxPLBuSksdXI3DHfBL4eEf+QNLKU6zQybe3tAzn2YCrn9ExyXsM15PMYTQbxr0bEn0rtczsyyP06Iq7txDkdDsr5OY+sqd9AXqXtCXwxIk4vNfnFyDH6ywK3R8R1dZV3bha44F4ZxrSrvQMQAAATlklEQVQR2em2GTkd/edkTWwM8PPoxxDCQSjj68ipzLuQNccrJL2TvOT/RUR8o64ydksJMNOAsyLiq5Xbx5D5WxYq9z9LjmO/tMPH35q8GjiFbEdflMy6eRmZ0mAx8r3z4Yh4XtK+wK8i4vedLEcnlSa+i8ghvd+I0rRSJoAdDPw7Io6TtDSZOXNGfaXtvnJ+vg2cGxFnV25fhmyW+0FEfLjMo3gqylDHoWbId/Z0WgmaO5KX0DPIb+WjyHa008ma3/Z9nYwxCGXcgaxN/ZTsKP2kpN0j4gJyUtUbJa1YVxm7aCvyw/RVSSMlva7U3Nchhzo+S2YovImcvNRp08lmoFPJJqGzyVEyF5Pvn/2A77QCZERcNJQDe/EeMufNV8sX0tjSWf9Hcj7AiNLvNJ0cmbXAKFdf7wS+FhFnSxotaQVlVsuR5JXzVElfJoeDrjSfh6vVAldzB5D0QXKq9DdLEH8jWUM+iWy3XTEi7qu5jO8DnomIc8obrvXl896IuFrSypFZHBtNmSJ1N3Jc8efJlaSWJ0cMnRIRZ0l6LTks79ZBLMcykVk0W9sTyfbonaOMThouJH2M7BC+hJwItwbZT3A3Gfj/TuYc/1OryWtBokzlvRZZafgIOcZ/c7LJ6jzKcoDA/0XELXWVszcLRM29tI/11Bq+9ig5LvUJcqWh9esI7HMpo8gc4ETEC+SY+9uBUyRNXBACO0BEnEnm9LmWnBx0VmT+9YnkULyXR8RdrcA+j9e6E+V4rDz+aEk7kV80Hxtugb2YzpyslRuRw2tXJ5PgfTgi/hwRX2+1sddYzq6qPNdrgefJL7vWpMBtKWmby3vha0M5sMMCMBSy0n69JVlDeZD89l1B0tkR8W5gCTK4P1726eqLVinjG8kP2RNkk8PrJX0rIt4GvLqU7/fk1Oafd7OMdShjqJ+PiB0krRIRD1XuXh74G5nJ8iWD2dknaTQ53PJoMgheMVjH6qTK+2tEZC7xSyXdSabXvU3SQhHx79IU86bWNjRzrsS8VJ7rb8j+nK9GxM2SRkbEC8rUI6uUce91L8jTq8bX3MubejeySWMJ8jLr7eSqMGMl/YJsUz2FbHNcp6Yy7kK26z5DTp9/H7lYxlKSLiNTyX4emE2mSm280h48qvz9EGR+DmWytNPJkQtdW60nIp4jr/L2LaOXhnytVjn56h2Slo6IF0sTHxFxX5Q8OyWwb0aO8rki6l9JrDbli/DFiHgmymIqJbBPJGflXhARzw6HL71G19zLh29hsr12R3K5uJeRIwQeIWctrgz8izmJkaZ2uYytBbWnkqNi3kCuUjMtIp4mV6tZmvwiXo9sTtqjm2XsNmVCpsUj4vb470ky25FpF05uBdhuftBKgP9L+XvIf8DJfOJbAQtJ+nZEPFE9Z8pVgt5GJrv6SERcvqAMd4SXVut66eqveoVT7l+abG8/lVyN7OqaitpnjetQVeaaHgU8GxGPKadVn0WOqlgbOCAyz/bOwKMR8StJq5JtkF+JTAnbjTIuUY7/D0mLkqMw/kkm/T84cq3PqcCTpe1zZfIDeHG0mcJ3uFGOI38Z+UH6O5Xn2iMgrR6ZcmGBCUIDIek95OzSW8iFQ57qcT7XJDukb19Qzqmkxcj491QZCbMmuQbAxeX+EeVKR2SuncUj4u5hdX5iCMyk6tQP+QLdTg5T+xVlhXWyZvIHYPeyvRU5WmCTyv8u1sUy/pZMm/pHYKVy+15kk8ubyvYbyA6uTSv/u0jd53iQzkkrEVdrWcD1yeXKTiQ7uKne758+ndsdycRqV5HDfg+lLGVHB1ehGk4/ZMbXs8hMnm8kZ5d+CJgFfLSy35CYadrv51l3ATr4go0nx3+/vbx4J5Hjw0eRoyyOLoH/TDItZ0emp/exjGuUwP72sv1Zctr3wqWch5cvoc+X/Xbpdhm7/JqtXPl7HXJizbKV7bPIGZRr113W4fJD1jBbf48hF0tvLb+4fzmnBwBL1F3Wms/TMeSs589XKn2rkgnoTqq7fJ34aUSHarmcPxb4W+QQrifI5E5Pk80fT0bE6eSEk4uBt0XO9lR0aWWcUsaDyNr6j8vNx5NXEMuRqya1ahNfB/aLOe2fw3H1nnZ8XlJrbPoj5Ov1GUnLRjaPfY2cFbqncrk6mw9JiwPTNGcC3rNkpWE1gIi4kDzHRwH7tTqrFySVDuXPks1U6wMblvfcg+Ts72MkfbzGYnZEI4J7CX4nAU9XXpSpZDCdBvymTExYMSJujDmjBLrZEfcimfP5QeDdkpYnJ069i6xB3F7KPj4ibo1MH9vVMnZbRLwZeFTSNZEd3B8i081+trR1PkzmEf9BdHMFm2EqMt3s/sCykt4cOZLoW8DGypWqAH5Ejt/+aTQ8o2NPpaL0gqRXAkTEl8m+rtWAzSQtVQL8OuSC38NaIzpUK50fKwLnlptXIicB/YmsDa8LXBYRv66pjK2xxq8kg9gYsv19SmQH7+5kdr7pEfGLOspYF0k/Jb/HWmkfPkHWoEYBx8QQSuQ2VPXoIH0zmap3H7IJ8gByxMddZBv84REx7INXfygXgf8i2f9wJ7lo9duAHchMn9dFxONl3+HTeToXjQju8B8BfnmyHe3RiDiiMgHhpYkZQ6CMK5NNMs8Dn4yIv1bvr7OMg63yJbcWsDSZbfB5SVeTfQs7lP22Jl/Du+os73BQOafLA/+IiGfKCJCvAIeRS8FtTA6lvSMiflljcWsjaUPyC+8KMgHcduSV4kfJvrpJwHvKVeSwN+yDuzJr4Mzy5q7W4L9Edk6eWteLVfnQvfTFUrltVTLAPwV8M4ZwethOK0M8TyAXr16OzFF/RwnwS0fEprUWcBiStCuZPiOAXwIXkIMMziXP73drLF6tSn/XEmQ8+HVE7CRpIfLqcCo52/QEYPmIeLi+knbWsGxzb80MVKbt/S4lM1sJ7CMi4s/kyJO1yFzUtZSxBPGJwGFlXC3lNpW2vdPIRFhP11HGOpQvtUPJ7Ho3kCOZ/goQEZPIfpPX11fC4adM+vo0OajgM2SF4SNkpszjgNMlLd/63CwIqs81csbpE+Tkvy0lHVAqWzcCl5OLcazepMAOw3SGagmQG5JDCT8QEbM0Jw9JK8D/SdLUOppiKk1B2wPnkBOn/tmj/CMi4n5JR0TEv7pdxm6RtHjkRJGRkQnQXmBO9sHdgLdExF8lbRMR10XENrUWeBiptAkvDTzQuvqT9CCZA2diRPxA0s2tpr8FRaX/YVOyL+vuiPh5aXP/kSQi00hfTy4X+HiNxR0Uw7LmXjxH9mrvCS/lIRlR/n6xsk/XSHpFOf4LkhYB9iY7BG9oDcFqaZWx4YH9NeQH6Tzgg5KWIoc8jiI7sY4onclvBM5UzpS0XlRqpYuV378lc7AfDhAR95ITcl5b7p/d3RLWR9LakvYqf08mhxWvCvxE0ttKf8MuwJclHRQRLzQxsMMwCu6VppgllWlef0vmnN5I0gkwp1mm9T/d7Okux32PcjWbVtB+gswiN6rUWpG0Xun4arRyHs4jh6LeRGZx3DoyN8t1ZNrZQyV9gJxYc3wJStaLcuU3Cfi2pI+SzZJnA2tJOkfSNsAUcpY2rfde00l6NbnAzWKl3+1EcpHvW8l29XdLOjAyIdg25JyTxhoWwb3Sfj2FbGO/RNLbI8er7w9MknQK/EetvavKcU8A/i7pe+Xm/yVrT6sDKJfN+yy5ok9jlc6qy8hcHeeR4/hfyrgZET8ks3DeTH7oDosyYaumIg8rksaR77WvAq8hPwNPAV8mP9NTySvGm+opYfeVq74ryDWQzy/9bu8g1zf9NNk0cx7wxVKDvzEiftLk99ywCO4lsO9AfhMfQKYR+JKkwyPiDrL9drKkV3X7xZK0mObMCFyj/F5Y0jmRa5w+BnxM0vfJtL1nx9Bfhm1ASj/HXmQ++neXK6hFgXdK+qGkrwErAj+MiM9FxPXl/4b30K0ukLQBGaiuiohLyBEyS5LD+kZGxCHA+2KYpCTuhHKVeBHwAPBEGUZLRPyOXMx8Zql83UGuovTSmqdNfs8Nm6GQkt5CvngrkHkhPk8O8/pURHxG0hLRxdzelXJtSA5pvJ68BHwXuQ7r+cAjEXGYpFWAV5HpEX5b6QhrNOWsyJ+QzTDLk/l9ViAn1GwEvLt8AK0NkrYiF26+DngzMCkirlMu3PwJMt3AR2IBms2rzKh6JVkrv5yMDQsBl0fEL5VDpT9DjmdflxzHfkNd5e2mIRvcK00xi0bEM+W2Rck23NNLJ+V5ZNvZlnUOY5J0LrAvGawuLLctS46UUUQ0Ov/6/JSa5s+AMyLilMrrOiZyiUNrg6R1yeG90yLiekkHklesR5UAvyywXETcM98HaiBJK0TEX8rfa5ITlUYDP4qIWyStRy6Td3ssQDNzh+xQyBIAdgV2k/Qc2es9nUy0NVG5yMDCwFvrCOyVILUYcBvZ5vkuSbdFLjLxtzJ64XRJG0bE/3a7jENB5DJu2wFXSXoyIs4odw3HtUfrtCs5vPH2MrTxfEkvAOdLOjhyIesF8pxWAvuIiLhX0jfIJIG7ldtuIptkhn1Kgb4YyjX3LcgcEFPITI73A+8khzFtBkwGjouIy2soWyuwTyUnRhwVEY9Kej85/HEX8otnR+DCiHi222Ucasp445+SHcwPLSgfsIGS9FoyZe+3y/vrVWQysJvKkNuDgHtjActH1BtJa5Crlolsun2s5iJ13VAO7ocDj5Ljoj8J7BURD0haMnL1ouUjJ7/U8k2snKD0aXKs9o3KxZNfJHN5HE2OAnlvRPx4Pg+zQGm9dnWXY6irVB42Jys06wIfj1zY+sNkv8X3gF9Eyey4INVI21Xa22n6AIZ5GcrBfRcyJe4ywJ4R8UdJ+5AdcccCL3Z5HPtKwJERcXzZPpbspLm+lGkfMk/72cCGwL+ipO21VAlaDkS9UE7sOoscMro12Yb8E7JD9X/IrKLHR0Mn4NjADangrlyBfUngz+TImO+Ql/I/IIP8+eQitVfUULbFyWFVT0bEg2XM/dvI5ExfI2cLrkaOVvhLt8tnw1sZUbVERNxdto8lKwhnlqG2k8lO+zPJ0SHjIuK+2gpsQ17twV1zMjm2hnldQq5r+F6ynf0oclzvaHKM+KV11vzKBCVFxO6SViBXUPpj6aX/NrBPOE2t9ZGkd5IzKe+NiH9J2p9MsLZHZJ6kpciRYg8CF0Qu7O4rIJun2oK7SkKp8veG5MIa15RhXTuRl6SHRsSPyxDIJSLikRrb2NcsPfEvI2cGRkS0cljsCpxKXiZf2u2y2fClzEekEsDHkKPCTiEXRz+KTFX7GfLK8LPA34EbI1cRMpunWmaolkkXh0party0N/AmYIwyD8uV5GSgiyTtGxHPRMnJ3uV29lY+m/HAdElfjIinyeneIyRNK7s+BRzSuqroVvlseFMmVvsJsEWZhPcoOfv6A+SomCuAZ4CryKvCY8gZlq+WNMLvNZufWmruZWz4MmT613Uj4hpJnyCzt50E3Fc63iaT7Y7Xdb2Qc8q6C3lV8TC5WstlEXGoMuvjJWQb/N51lc+GJ2V+mMvJyV3nq7IKl6TjyFm8p0TEraUz/59kR/05wNRW27zZvHQ9uGtOrnMB7yPHPU+LiJ9K+hyZm/oTwO9atfQam2IWI2tPn43M1bE0mWnv6oh4T2miWTsiZnS7bDa8SToAeF1EvFeZUXQ9cv7Gw2TQP5SsVBwbEdOVmURPBM5yYLd2dG2GqjJN7xMlsLc6Uc8nM7dNKQH8KEnnACcDB1JWKKqx0+hpslP34VKOxyUdRaZafTIiTgBmuGPL+uE+4CBJOwJvJROrrUNmEt05Ig4pzZYBUOZ0vD9KKg6z3nSlzV3SwsD/SnofvJR3fVTk0ldfI9/ou0iaHBGHkZejXV96rtLGPk6Zq2MhMuXBt0otHeBx4HPAdpK2LM/Hgd36ajqZvvpUcvjv2cBWZOfpIgARcXJEzNCcRWgc2K1tXam5R8SzkvYlV+V5JiLOiVw5aXREPCHpC2Qn0vaSpkdNyY9KO/8k8gP3G+DVZE6PpYCbJV1Drvw0hfwA1pI73oa/Unn5nKSvV6fGK5NcrV5G0fw5kt9n1mdda5aJiJvLEMefKNcvPIc5wXENctWi70SNmQIlvRL4FDkz9pfkULRbgfXJUQpLkKlFlydXgfpSPSW1pmgF9pK+Ynsy1caHomGLNVv3dTUrZLnE3J4M8CMi4mxJE4HvA7tHxB+6WR74r87aJ8kMjzeSnc1nlBrUuyLik2X/1wKnAe/wDEHrhBLYNyFzEn24jhnY1jxdT/lbCfBXSlqfzJtxUET8vJvlqHTwVnOdvEh2ah0dEZ8pu95PJmpqmUV2eC2Q6VWt8yLiOUm/BvaNiL+4g946oc4ZqhuTizi8MyK+2+rM7MabunTw3g18MUp+8dLB+3yZsPRzctLIH8i0oR8IZ3c0s2Gk1twyrRQEddRUlEnKfgScWNr/kbRw6fwdQ6568y/gNxFxTTfLZmY2UHWvxPTPug48jw7e58vdy5KTqKaBc2Wb2fBTS26ZllbArCtwlpml2wOflPTuMsFqItmhOrtnOc3MhovaU/4OBZImkDmyf0B28H4oIr5fb6nMzPrPwb2YWweva+xmNlw5uFfU2cFrZtZJtba5D0G1dfCamXWSa+5mZg3kmruZWQM5uJuZNZCDu5lZAzm4m5k1kIO7mVkD/T/HlF+J0v1zwAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plotting RMSE score for all regression models\n",
    "\n",
    "score_list_reg=[score_lin_classic,score_log_classic,score_reg_relu,score_reg_relu_stopping,score_reg_sig,score_reg_sig_stopping,score_reg_tanh,score_reg_tanh_stopping]\n",
    "names =['Linear Regression','Logistic Regression','Relu','Relu Stopping','Sigmoid','Sigmoid Stopping','Tanh','Tanh Stopping']\n",
    "tick_marks = np.arange(len(names))\n",
    "plt.bar(range(len(score_list_reg)), score_list_reg)\n",
    "plt.xticks(tick_marks, names, rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Relu with Postal Code and Categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-hot cooding of postal codes \n",
    "\n",
    "postal_hotcoded_df = pd.get_dummies(merge_df['postal code'], sparse = 'true')\n",
    "\n",
    "x_matrix_postal = np.column_stack((x_matrix_minmax, postal_hotcoded_df))\n",
    "x_matrix_final = np.column_stack((x_matrix_postal, category_matrix))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000,)\n"
     ]
    }
   ],
   "source": [
    "print(y_stars_regression.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train test data\n",
    "x_train_reg, x_test_reg, y_train_reg, y_test_reg = train_test_split(x_matrix_final, y_stars_regression , test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up checkpointer\n",
    "checkpointer_relu_postal = ModelCheckpoint(filepath=\"./best_weights_relu_postal.hdf5\", verbose=2, save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tensorflow model for regression\n",
    "model_reg_relu = Sequential()\n",
    "\n",
    "model_reg_relu.add(Dense(60, input_dim=x_train_reg.shape[1], activation='relu')) \n",
    "model_reg_relu.add(Dense(30, activation='relu')) # Hidden 2\n",
    "model_reg_relu.add(Dense(1)) # Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/100\n",
      " - 3s - loss: 2.2098 - val_loss: 0.5011\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.50115, saving model to ./best_weights_relu_postal.hdf5\n",
      "Epoch 2/100\n",
      " - 1s - loss: 0.3419 - val_loss: 0.4082\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.50115 to 0.40819, saving model to ./best_weights_relu_postal.hdf5\n",
      "Epoch 3/100\n",
      " - 1s - loss: 0.2196 - val_loss: 0.3995\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.40819 to 0.39951, saving model to ./best_weights_relu_postal.hdf5\n",
      "Epoch 4/100\n",
      " - 1s - loss: 0.1702 - val_loss: 0.3856\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.39951 to 0.38562, saving model to ./best_weights_relu_postal.hdf5\n",
      "Epoch 5/100\n",
      " - 1s - loss: 0.1372 - val_loss: 0.3893\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.38562\n",
      "Epoch 6/100\n",
      " - 1s - loss: 0.1134 - val_loss: 0.4001\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.38562\n",
      "Epoch 7/100\n",
      " - 1s - loss: 0.0931 - val_loss: 0.4023\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.38562\n",
      "Epoch 00007: early stopping\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/100\n",
      " - 2s - loss: 0.0847 - val_loss: 0.4040\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.38562\n",
      "Epoch 2/100\n",
      " - 1s - loss: 0.0699 - val_loss: 0.4087\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.38562\n",
      "Epoch 3/100\n",
      " - 1s - loss: 0.0551 - val_loss: 0.4090\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.38562\n",
      "Epoch 4/100\n",
      " - 1s - loss: 0.0433 - val_loss: 0.4033\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.38562\n",
      "Epoch 00004: early stopping\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/100\n",
      " - 2s - loss: 0.0413 - val_loss: 0.4034\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.38562\n",
      "Epoch 2/100\n",
      " - 1s - loss: 0.0347 - val_loss: 0.3998\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.38562\n",
      "Epoch 3/100\n",
      " - 1s - loss: 0.0311 - val_loss: 0.3940\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.38562\n",
      "Epoch 4/100\n",
      " - 1s - loss: 0.0261 - val_loss: 0.4033\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.38562\n",
      "Epoch 5/100\n",
      " - 1s - loss: 0.0223 - val_loss: 0.3930\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.38562\n",
      "Epoch 6/100\n",
      " - 1s - loss: 0.0197 - val_loss: 0.3994\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.38562\n",
      "Epoch 00006: early stopping\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/100\n",
      " - 2s - loss: 0.0215 - val_loss: 0.3975\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.38562\n",
      "Epoch 2/100\n",
      " - 1s - loss: 0.0201 - val_loss: 0.3979\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.38562\n",
      "Epoch 3/100\n",
      " - 1s - loss: 0.0203 - val_loss: 0.3886\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.38562\n",
      "Epoch 4/100\n",
      " - 1s - loss: 0.0176 - val_loss: 0.3903\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.38562\n",
      "Epoch 5/100\n",
      " - 1s - loss: 0.0154 - val_loss: 0.3918\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.38562\n",
      "Epoch 6/100\n",
      " - 1s - loss: 0.0140 - val_loss: 0.3897\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.38562\n",
      "Epoch 00006: early stopping\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/100\n",
      " - 2s - loss: 0.0150 - val_loss: 0.3843\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.38562 to 0.38434, saving model to ./best_weights_relu_postal.hdf5\n",
      "Epoch 2/100\n",
      " - 1s - loss: 0.0157 - val_loss: 0.3851\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.38434\n",
      "Epoch 3/100\n",
      " - 1s - loss: 0.0142 - val_loss: 0.3816\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.38434 to 0.38155, saving model to ./best_weights_relu_postal.hdf5\n",
      "Epoch 4/100\n",
      " - 1s - loss: 0.0124 - val_loss: 0.3857\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.38155\n",
      "Epoch 5/100\n",
      " - 1s - loss: 0.0117 - val_loss: 0.3797\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.38155 to 0.37975, saving model to ./best_weights_relu_postal.hdf5\n",
      "Epoch 6/100\n",
      " - 1s - loss: 0.0114 - val_loss: 0.3848\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.37975\n",
      "Epoch 7/100\n",
      " - 1s - loss: 0.0108 - val_loss: 0.3776\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.37975 to 0.37758, saving model to ./best_weights_relu_postal.hdf5\n",
      "Epoch 8/100\n",
      " - 1s - loss: 0.0108 - val_loss: 0.3775\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.37758 to 0.37748, saving model to ./best_weights_relu_postal.hdf5\n",
      "Epoch 9/100\n",
      " - 1s - loss: 0.0105 - val_loss: 0.3810\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.37748\n",
      "Epoch 10/100\n",
      " - 1s - loss: 0.0101 - val_loss: 0.3776\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.37748\n",
      "Epoch 00010: early stopping\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/100\n",
      " - 2s - loss: 0.0096 - val_loss: 0.3725\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.37748 to 0.37252, saving model to ./best_weights_relu_postal.hdf5\n",
      "Epoch 2/100\n",
      " - 1s - loss: 0.0101 - val_loss: 0.3761\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.37252\n",
      "Epoch 3/100\n",
      " - 1s - loss: 0.0100 - val_loss: 0.3749\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.37252\n",
      "Epoch 4/100\n",
      " - 1s - loss: 0.0096 - val_loss: 0.3736\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.37252\n",
      "Epoch 00004: early stopping\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/100\n",
      " - 2s - loss: 0.0093 - val_loss: 0.3768\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.37252\n",
      "Epoch 2/100\n",
      " - 1s - loss: 0.0083 - val_loss: 0.3727\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.37252\n",
      "Epoch 3/100\n",
      " - 1s - loss: 0.0088 - val_loss: 0.3704\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.37252 to 0.37043, saving model to ./best_weights_relu_postal.hdf5\n",
      "Epoch 4/100\n",
      " - 1s - loss: 0.0089 - val_loss: 0.3782\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.37043\n",
      "Epoch 5/100\n",
      " - 1s - loss: 0.0084 - val_loss: 0.3689\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.37043 to 0.36893, saving model to ./best_weights_relu_postal.hdf5\n",
      "Epoch 6/100\n",
      " - 1s - loss: 0.0074 - val_loss: 0.3747\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.36893\n",
      "Epoch 7/100\n",
      " - 1s - loss: 0.0072 - val_loss: 0.3684\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.36893 to 0.36835, saving model to ./best_weights_relu_postal.hdf5\n",
      "Epoch 8/100\n",
      " - 1s - loss: 0.0069 - val_loss: 0.3672\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.36835 to 0.36724, saving model to ./best_weights_relu_postal.hdf5\n",
      "Epoch 9/100\n",
      " - 1s - loss: 0.0070 - val_loss: 0.3672\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.36724\n",
      "Epoch 10/100\n",
      " - 1s - loss: 0.0070 - val_loss: 0.3671\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.36724 to 0.36712, saving model to ./best_weights_relu_postal.hdf5\n",
      "Epoch 11/100\n",
      " - 1s - loss: 0.0070 - val_loss: 0.3669\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.36712 to 0.36691, saving model to ./best_weights_relu_postal.hdf5\n",
      "Epoch 00011: early stopping\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/100\n",
      " - 2s - loss: 0.0068 - val_loss: 0.3633\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.36691 to 0.36334, saving model to ./best_weights_relu_postal.hdf5\n",
      "Epoch 2/100\n",
      " - 1s - loss: 0.0061 - val_loss: 0.3688\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.36334\n",
      "Epoch 3/100\n",
      " - 1s - loss: 0.0067 - val_loss: 0.3669\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.36334\n",
      "Epoch 4/100\n",
      " - 1s - loss: 0.0066 - val_loss: 0.3681\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.36334\n",
      "Epoch 00004: early stopping\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/100\n",
      " - 2s - loss: 0.0066 - val_loss: 0.3633\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.36334 to 0.36325, saving model to ./best_weights_relu_postal.hdf5\n",
      "Epoch 2/100\n",
      " - 1s - loss: 0.0056 - val_loss: 0.3690\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.36325\n",
      "Epoch 3/100\n",
      " - 1s - loss: 0.0058 - val_loss: 0.3603\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.36325 to 0.36025, saving model to ./best_weights_relu_postal.hdf5\n",
      "Epoch 4/100\n",
      " - 1s - loss: 0.0060 - val_loss: 0.3622\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.36025\n",
      "Epoch 5/100\n",
      " - 1s - loss: 0.0058 - val_loss: 0.3616\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.36025\n",
      "Epoch 6/100\n",
      " - 1s - loss: 0.0054 - val_loss: 0.3666\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.36025\n",
      "Epoch 00006: early stopping\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/100\n",
      " - 2s - loss: 0.0052 - val_loss: 0.3636\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.36025\n",
      "Epoch 2/100\n",
      " - 1s - loss: 0.0049 - val_loss: 0.3629\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.36025\n",
      "Epoch 3/100\n",
      " - 1s - loss: 0.0059 - val_loss: 0.3644\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.36025\n",
      "Epoch 4/100\n",
      " - 1s - loss: 0.0060 - val_loss: 0.3592\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.36025 to 0.35923, saving model to ./best_weights_relu_postal.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/100\n",
      " - 1s - loss: 0.0051 - val_loss: 0.3619\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.35923\n",
      "Epoch 6/100\n",
      " - 1s - loss: 0.0048 - val_loss: 0.3579\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.35923 to 0.35795, saving model to ./best_weights_relu_postal.hdf5\n",
      "Epoch 7/100\n",
      " - 1s - loss: 0.0045 - val_loss: 0.3601\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.35795\n",
      "Epoch 8/100\n",
      " - 1s - loss: 0.0045 - val_loss: 0.3563\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.35795 to 0.35626, saving model to ./best_weights_relu_postal.hdf5\n",
      "Epoch 9/100\n",
      " - 1s - loss: 0.0048 - val_loss: 0.3605\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.35626\n",
      "Epoch 10/100\n",
      " - 1s - loss: 0.0047 - val_loss: 0.3603\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.35626\n",
      "Epoch 11/100\n",
      " - 1s - loss: 0.0044 - val_loss: 0.3604\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.35626\n",
      "Epoch 00011: early stopping\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    model_reg_relu.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    model_reg_relu.fit(x_train_reg,y_train_reg,validation_data=(x_test_reg,y_test_reg),callbacks=[monitor,checkpointer_relu_postal],verbose=2,epochs=100) \n",
    "\n",
    "model_reg_relu.load_weights('./best_weights_relu_postal.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (2000, 1)\n"
     ]
    }
   ],
   "source": [
    "# Predict stars\n",
    "pred_reg_stopping = model_reg_relu.predict(x_test_reg)\n",
    "print(\"Shape: {}\".format(pred_reg_stopping.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Business ID: diaiQrxYFU1V5qxrFnW9fg, Actual Stars: 4.0, predicted Stars: [4.0271177]\n",
      "2. Business ID: TDTASGFy_aGp6vy0i23mDA, Actual Stars: 4.0, predicted Stars: [4.6701283]\n",
      "3. Business ID: VuKJ2s_JP8weQ54NfsXJXQ, Actual Stars: 3.0, predicted Stars: [4.4788647]\n",
      "4. Business ID: aGiBg2WKOpXS5-1DRnBiAQ, Actual Stars: 4.5, predicted Stars: [4.633781]\n",
      "5. Business ID: ZMmgFw2P4LWsFXNn1ZGc1g, Actual Stars: 3.0, predicted Stars: [3.2558086]\n",
      "6. Business ID: sEKFq5u8P_s0-2mAZnx0JQ, Actual Stars: 4.0, predicted Stars: [3.9144561]\n",
      "7. Business ID: rYziPPEILDXJ_F5uKR--YQ, Actual Stars: 3.5, predicted Stars: [3.488705]\n",
      "8. Business ID: Swm_uMOWNcJDZz5lXWyzKA, Actual Stars: 5.0, predicted Stars: [4.6495156]\n",
      "9. Business ID: 6nGnVP7M4qQRiclXxeqXSQ, Actual Stars: 4.5, predicted Stars: [4.9873514]\n",
      "10. Business ID: Tc24GX9-ZPr4_SHU0nJZZA, Actual Stars: 2.5, predicted Stars: [2.0438144]\n"
     ]
    }
   ],
   "source": [
    "#Display 10 business\n",
    "\n",
    "for i in range(10):\n",
    "    print(\"{}. Business ID: {}, Actual Stars: {}, predicted Stars: {}\".format(i+1,merge_df['business_id'][2000+i],y_test_reg[i],pred_reg_stopping[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final score (RMSE): 0.5968785881996155\n",
      "R2 score: 0.67\n"
     ]
    }
   ],
   "source": [
    "# Measure RMSE error.  RMSE is common for regression.\n",
    "score_relu_postal = np.sqrt(mean_squared_error(y_test_reg,pred_reg_stopping))\n",
    "print(\"Final score (RMSE): {}\".format(score_relu_postal))\n",
    "print('R2 score: %.2f' % r2_score(y_test_reg, pred_reg_stopping))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimenting with different optimizers for ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up checkpointer\n",
    "checkpointer_relu_sgd = ModelCheckpoint(filepath=\"./best_weights_relu_sgd.hdf5\", verbose=2, save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/100\n",
      " - 2s - loss: 0.0040 - val_loss: 0.3564\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.35644, saving model to ./best_weights_relu_sgd.hdf5\n",
      "Epoch 2/100\n",
      " - 1s - loss: 0.0037 - val_loss: 0.3566\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.35644\n",
      "Epoch 3/100\n",
      " - 1s - loss: 0.0036 - val_loss: 0.3566\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.35644\n",
      "Epoch 4/100\n",
      " - 1s - loss: 0.0034 - val_loss: 0.3564\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.35644 to 0.35638, saving model to ./best_weights_relu_sgd.hdf5\n",
      "Epoch 00004: early stopping\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/100\n",
      " - 2s - loss: 0.0033 - val_loss: 0.3566\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.35638\n",
      "Epoch 2/100\n",
      " - 1s - loss: 0.0032 - val_loss: 0.3564\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.35638 to 0.35635, saving model to ./best_weights_relu_sgd.hdf5\n",
      "Epoch 3/100\n",
      " - 1s - loss: 0.0032 - val_loss: 0.3566\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.35635\n",
      "Epoch 4/100\n",
      " - 1s - loss: 0.0031 - val_loss: 0.3562\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.35635 to 0.35620, saving model to ./best_weights_relu_sgd.hdf5\n",
      "Epoch 00004: early stopping\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/100\n",
      " - 2s - loss: 0.0030 - val_loss: 0.3566\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.35620\n",
      "Epoch 2/100\n",
      " - 1s - loss: 0.0029 - val_loss: 0.3564\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.35620\n",
      "Epoch 3/100\n",
      " - 1s - loss: 0.0029 - val_loss: 0.3573\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.35620\n",
      "Epoch 4/100\n",
      " - 1s - loss: 0.0028 - val_loss: 0.3565\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.35620\n",
      "Epoch 00004: early stopping\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/100\n",
      " - 2s - loss: 0.0028 - val_loss: 0.3568\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.35620\n",
      "Epoch 2/100\n",
      " - 1s - loss: 0.0027 - val_loss: 0.3565\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.35620\n",
      "Epoch 3/100\n",
      " - 1s - loss: 0.0027 - val_loss: 0.3568\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.35620\n",
      "Epoch 4/100\n",
      " - 1s - loss: 0.0026 - val_loss: 0.3563\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.35620\n",
      "Epoch 00004: early stopping\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/100\n",
      " - 2s - loss: 0.0026 - val_loss: 0.3564\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.35620\n",
      "Epoch 2/100\n",
      " - 1s - loss: 0.0025 - val_loss: 0.3560\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.35620 to 0.35595, saving model to ./best_weights_relu_sgd.hdf5\n",
      "Epoch 3/100\n",
      " - 1s - loss: 0.0025 - val_loss: 0.3561\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.35595\n",
      "Epoch 4/100\n",
      " - 1s - loss: 0.0025 - val_loss: 0.3565\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.35595\n",
      "Epoch 00004: early stopping\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/100\n",
      " - 2s - loss: 0.0024 - val_loss: 0.3564\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.35595\n",
      "Epoch 2/100\n",
      " - 1s - loss: 0.0024 - val_loss: 0.3561\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.35595\n",
      "Epoch 3/100\n",
      " - 1s - loss: 0.0023 - val_loss: 0.3566\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.35595\n",
      "Epoch 4/100\n",
      " - 1s - loss: 0.0023 - val_loss: 0.3565\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.35595\n",
      "Epoch 00004: early stopping\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/100\n",
      " - 2s - loss: 0.0023 - val_loss: 0.3560\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.35595\n",
      "Epoch 2/100\n",
      " - 1s - loss: 0.0022 - val_loss: 0.3563\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.35595\n",
      "Epoch 3/100\n",
      " - 1s - loss: 0.0022 - val_loss: 0.3560\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.35595\n",
      "Epoch 4/100\n",
      " - 1s - loss: 0.0022 - val_loss: 0.3569\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.35595\n",
      "Epoch 00004: early stopping\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/100\n",
      " - 2s - loss: 0.0022 - val_loss: 0.3559\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.35595 to 0.35590, saving model to ./best_weights_relu_sgd.hdf5\n",
      "Epoch 2/100\n",
      " - 1s - loss: 0.0021 - val_loss: 0.3565\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.35590\n",
      "Epoch 3/100\n",
      " - 1s - loss: 0.0021 - val_loss: 0.3563\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.35590\n",
      "Epoch 4/100\n",
      " - 1s - loss: 0.0021 - val_loss: 0.3561\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.35590\n",
      "Epoch 00004: early stopping\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/100\n",
      " - 2s - loss: 0.0020 - val_loss: 0.3567\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.35590\n",
      "Epoch 2/100\n",
      " - 1s - loss: 0.0020 - val_loss: 0.3561\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.35590\n",
      "Epoch 3/100\n",
      " - 1s - loss: 0.0020 - val_loss: 0.3565\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.35590\n",
      "Epoch 4/100\n",
      " - 1s - loss: 0.0020 - val_loss: 0.3564\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.35590\n",
      "Epoch 00004: early stopping\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/100\n",
      " - 2s - loss: 0.0019 - val_loss: 0.3563\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.35590\n",
      "Epoch 2/100\n",
      " - 1s - loss: 0.0019 - val_loss: 0.3564\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.35590\n",
      "Epoch 3/100\n",
      " - 1s - loss: 0.0019 - val_loss: 0.3563\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.35590\n",
      "Epoch 4/100\n",
      " - 1s - loss: 0.0019 - val_loss: 0.3564\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.35590\n",
      "Epoch 00004: early stopping\n"
     ]
    }
   ],
   "source": [
    "#Model training with Stochastic gradient descent optimizer(SGD).\n",
    "for i in range(10):\n",
    "    model_reg_relu.compile(loss='mean_squared_error', optimizer='sgd')\n",
    "    model_reg_relu.fit(x_train_reg,y_train_reg,validation_data=(x_test_reg,y_test_reg),callbacks=[monitor,checkpointer_relu_sgd],verbose=2,epochs=100) \n",
    "\n",
    "model_reg_relu.load_weights('./best_weights_relu_sgd.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict stars\n",
    "pred_reg_sgd = model_reg_relu.predict(x_test_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Business ID: diaiQrxYFU1V5qxrFnW9fg, Actual Stars: 4.0, predicted Stars: [4.0224066]\n",
      "2. Business ID: TDTASGFy_aGp6vy0i23mDA, Actual Stars: 4.0, predicted Stars: [4.6558514]\n",
      "3. Business ID: VuKJ2s_JP8weQ54NfsXJXQ, Actual Stars: 3.0, predicted Stars: [4.3980303]\n",
      "4. Business ID: aGiBg2WKOpXS5-1DRnBiAQ, Actual Stars: 4.5, predicted Stars: [4.6580343]\n",
      "5. Business ID: ZMmgFw2P4LWsFXNn1ZGc1g, Actual Stars: 3.0, predicted Stars: [3.2512727]\n",
      "6. Business ID: sEKFq5u8P_s0-2mAZnx0JQ, Actual Stars: 4.0, predicted Stars: [3.8860996]\n",
      "7. Business ID: rYziPPEILDXJ_F5uKR--YQ, Actual Stars: 3.5, predicted Stars: [3.5032425]\n",
      "8. Business ID: Swm_uMOWNcJDZz5lXWyzKA, Actual Stars: 5.0, predicted Stars: [4.6358366]\n",
      "9. Business ID: 6nGnVP7M4qQRiclXxeqXSQ, Actual Stars: 4.5, predicted Stars: [4.955144]\n",
      "10. Business ID: Tc24GX9-ZPr4_SHU0nJZZA, Actual Stars: 2.5, predicted Stars: [2.0325363]\n"
     ]
    }
   ],
   "source": [
    "#Display 10 business\n",
    "\n",
    "for i in range(10):\n",
    "    print(\"{}. Business ID: {}, Actual Stars: {}, predicted Stars: {}\".format(i+1,merge_df['business_id'][2000+i],y_test_reg[i],pred_reg_sgd[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final score (RMSE): 0.5965737104415894\n",
      "R2 score: 0.67\n"
     ]
    }
   ],
   "source": [
    "# Measure RMSE error.  RMSE is common for regression.\n",
    "score_relu_sgd = np.sqrt(mean_squared_error(y_test_reg,pred_reg_sgd))\n",
    "print(\"Final score (RMSE): {}\".format(score_relu_sgd))\n",
    "print('R2 score: %.2f' % r2_score(y_test_reg, pred_reg_sgd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up checkpointer\n",
    "checkpointer_relu_rmsprop = ModelCheckpoint(filepath=\"./best_weights_relu_rmsprop.hdf5\", verbose=2, save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/100\n",
      " - 3s - loss: 0.0068 - val_loss: 0.3563\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.35634, saving model to ./best_weights_relu_rmsprop.hdf5\n",
      "Epoch 2/100\n",
      " - 1s - loss: 0.0062 - val_loss: 0.3570\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.35634\n",
      "Epoch 3/100\n",
      " - 1s - loss: 0.0058 - val_loss: 0.3704\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.35634\n",
      "Epoch 4/100\n",
      " - 1s - loss: 0.0056 - val_loss: 0.3597\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.35634\n",
      "Epoch 00004: early stopping\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/100\n",
      " - 3s - loss: 0.0057 - val_loss: 0.3615\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.35634\n",
      "Epoch 2/100\n",
      " - 1s - loss: 0.0055 - val_loss: 0.3666\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.35634\n",
      "Epoch 3/100\n",
      " - 1s - loss: 0.0052 - val_loss: 0.3606\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.35634\n",
      "Epoch 4/100\n",
      " - 1s - loss: 0.0051 - val_loss: 0.3678\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.35634\n",
      "Epoch 00004: early stopping\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/100\n",
      " - 3s - loss: 0.0054 - val_loss: 0.3597\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.35634\n",
      "Epoch 2/100\n",
      " - 1s - loss: 0.0050 - val_loss: 0.3657\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.35634\n",
      "Epoch 3/100\n",
      " - 1s - loss: 0.0048 - val_loss: 0.3608\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.35634\n",
      "Epoch 4/100\n",
      " - 1s - loss: 0.0049 - val_loss: 0.3598\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.35634\n",
      "Epoch 00004: early stopping\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/100\n",
      " - 3s - loss: 0.0049 - val_loss: 0.3586\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.35634\n",
      "Epoch 2/100\n",
      " - 1s - loss: 0.0046 - val_loss: 0.3704\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.35634\n",
      "Epoch 3/100\n",
      " - 1s - loss: 0.0046 - val_loss: 0.3616\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.35634\n",
      "Epoch 4/100\n",
      " - 1s - loss: 0.0046 - val_loss: 0.3642\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.35634\n",
      "Epoch 00004: early stopping\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/100\n",
      " - 4s - loss: 0.0048 - val_loss: 0.3589\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.35634\n",
      "Epoch 2/100\n",
      " - 1s - loss: 0.0044 - val_loss: 0.3594\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.35634\n",
      "Epoch 3/100\n",
      " - 1s - loss: 0.0044 - val_loss: 0.3582\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.35634\n",
      "Epoch 4/100\n",
      " - 1s - loss: 0.0042 - val_loss: 0.3627\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.35634\n",
      "Epoch 00004: early stopping\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/100\n",
      " - 3s - loss: 0.0046 - val_loss: 0.3578\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.35634\n",
      "Epoch 2/100\n",
      " - 1s - loss: 0.0041 - val_loss: 0.3583\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.35634\n",
      "Epoch 3/100\n",
      " - 1s - loss: 0.0042 - val_loss: 0.3589\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.35634\n",
      "Epoch 4/100\n",
      " - 1s - loss: 0.0041 - val_loss: 0.3579\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.35634\n",
      "Epoch 00004: early stopping\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/100\n",
      " - 3s - loss: 0.0044 - val_loss: 0.3585\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.35634\n",
      "Epoch 2/100\n",
      " - 1s - loss: 0.0039 - val_loss: 0.3576\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.35634\n",
      "Epoch 3/100\n",
      " - 1s - loss: 0.0041 - val_loss: 0.3603\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.35634\n",
      "Epoch 4/100\n",
      " - 1s - loss: 0.0039 - val_loss: 0.3600\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.35634\n",
      "Epoch 00004: early stopping\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/100\n",
      " - 3s - loss: 0.0041 - val_loss: 0.3587\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.35634\n",
      "Epoch 2/100\n",
      " - 1s - loss: 0.0038 - val_loss: 0.3577\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.35634\n",
      "Epoch 3/100\n",
      " - 1s - loss: 0.0038 - val_loss: 0.3577\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.35634\n",
      "Epoch 4/100\n",
      " - 1s - loss: 0.0038 - val_loss: 0.3598\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.35634\n",
      "Epoch 5/100\n",
      " - 1s - loss: 0.0037 - val_loss: 0.3568\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.35634\n",
      "Epoch 00005: early stopping\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/100\n",
      " - 2s - loss: 0.0039 - val_loss: 0.3580\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.35634\n",
      "Epoch 2/100\n",
      " - 1s - loss: 0.0037 - val_loss: 0.3671\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.35634\n",
      "Epoch 3/100\n",
      " - 1s - loss: 0.0037 - val_loss: 0.3584\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.35634\n",
      "Epoch 4/100\n",
      " - 1s - loss: 0.0037 - val_loss: 0.3632\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.35634\n",
      "Epoch 00004: early stopping\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/100\n",
      " - 3s - loss: 0.0038 - val_loss: 0.3543\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.35634 to 0.35429, saving model to ./best_weights_relu_rmsprop.hdf5\n",
      "Epoch 2/100\n",
      " - 1s - loss: 0.0036 - val_loss: 0.3583\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.35429\n",
      "Epoch 3/100\n",
      " - 1s - loss: 0.0035 - val_loss: 0.3539\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.35429 to 0.35390, saving model to ./best_weights_relu_rmsprop.hdf5\n",
      "Epoch 4/100\n",
      " - 1s - loss: 0.0036 - val_loss: 0.3564\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.35390\n",
      "Epoch 00004: early stopping\n"
     ]
    }
   ],
   "source": [
    "#Model training with RMSProp optimizer.\n",
    "for i in range(10):\n",
    "    model_reg_relu.compile(loss='mean_squared_error', optimizer='rmsprop')\n",
    "    model_reg_relu.fit(x_train_reg,y_train_reg,validation_data=(x_test_reg,y_test_reg),callbacks=[monitor,checkpointer_relu_rmsprop],verbose=2,epochs=100) \n",
    "\n",
    "model_reg_relu.load_weights('./best_weights_relu_rmsprop.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict stars\n",
    "pred_reg_rmsprop = model_reg_relu.predict(x_test_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Business ID: diaiQrxYFU1V5qxrFnW9fg, Actual Stars: 4.0, predicted Stars: [4.0967507]\n",
      "2. Business ID: TDTASGFy_aGp6vy0i23mDA, Actual Stars: 4.0, predicted Stars: [4.6052527]\n",
      "3. Business ID: VuKJ2s_JP8weQ54NfsXJXQ, Actual Stars: 3.0, predicted Stars: [4.3680725]\n",
      "4. Business ID: aGiBg2WKOpXS5-1DRnBiAQ, Actual Stars: 4.5, predicted Stars: [4.6078935]\n",
      "5. Business ID: ZMmgFw2P4LWsFXNn1ZGc1g, Actual Stars: 3.0, predicted Stars: [3.254089]\n",
      "6. Business ID: sEKFq5u8P_s0-2mAZnx0JQ, Actual Stars: 4.0, predicted Stars: [3.872034]\n",
      "7. Business ID: rYziPPEILDXJ_F5uKR--YQ, Actual Stars: 3.5, predicted Stars: [3.4700384]\n",
      "8. Business ID: Swm_uMOWNcJDZz5lXWyzKA, Actual Stars: 5.0, predicted Stars: [4.5457177]\n",
      "9. Business ID: 6nGnVP7M4qQRiclXxeqXSQ, Actual Stars: 4.5, predicted Stars: [5.049957]\n",
      "10. Business ID: Tc24GX9-ZPr4_SHU0nJZZA, Actual Stars: 2.5, predicted Stars: [2.054903]\n"
     ]
    }
   ],
   "source": [
    "#Display 10 business\n",
    "\n",
    "for i in range(10):\n",
    "    print(\"{}. Business ID: {}, Actual Stars: {}, predicted Stars: {}\".format(i+1,merge_df['business_id'][2000+i],y_test_reg[i],pred_reg_rmsprop[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final score (RMSE): 0.5948983430862427\n",
      "R2 score: 0.67\n"
     ]
    }
   ],
   "source": [
    "# Measure RMSE error.  RMSE is common for regression.\n",
    "score_relu_rmsprop = np.sqrt(mean_squared_error(y_test_reg,pred_reg_rmsprop))\n",
    "print(\"Final score (RMSE): {}\".format(score_relu_rmsprop))\n",
    "print('R2 score: %.2f' % r2_score(y_test_reg, pred_reg_rmsprop))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up checkpointer\n",
    "checkpointer_relu_adagrad = ModelCheckpoint(filepath=\"./best_weights_relu_adagrad.hdf5\", verbose=2, save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/100\n",
      " - 3s - loss: 0.0055 - val_loss: 0.3569\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.35693, saving model to ./best_weights_relu_adagrad.hdf5\n",
      "Epoch 2/100\n",
      " - 1s - loss: 0.0025 - val_loss: 0.3534\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.35693 to 0.35339, saving model to ./best_weights_relu_adagrad.hdf5\n",
      "Epoch 3/100\n",
      " - 1s - loss: 6.9534e-04 - val_loss: 0.3553\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.35339\n",
      "Epoch 4/100\n",
      " - 1s - loss: 2.7535e-04 - val_loss: 0.3551\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.35339\n",
      "Epoch 5/100\n",
      " - 1s - loss: 1.3673e-04 - val_loss: 0.3557\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.35339\n",
      "Epoch 00005: early stopping\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/100\n",
      " - 3s - loss: 0.0044 - val_loss: 0.3568\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.35339\n",
      "Epoch 2/100\n",
      " - 1s - loss: 0.0028 - val_loss: 0.3543\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.35339\n",
      "Epoch 3/100\n",
      " - 1s - loss: 7.3743e-04 - val_loss: 0.3560\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.35339\n",
      "Epoch 4/100\n",
      " - 1s - loss: 2.6479e-04 - val_loss: 0.3560\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.35339\n",
      "Epoch 5/100\n",
      " - 1s - loss: 1.2235e-04 - val_loss: 0.3555\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.35339\n",
      "Epoch 00005: early stopping\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/100\n",
      " - 3s - loss: 0.0044 - val_loss: 0.3584\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.35339\n",
      "Epoch 2/100\n",
      " - 1s - loss: 0.0026 - val_loss: 0.3569\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.35339\n",
      "Epoch 3/100\n",
      " - 1s - loss: 7.2613e-04 - val_loss: 0.3562\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.35339\n",
      "Epoch 4/100\n",
      " - 1s - loss: 2.6600e-04 - val_loss: 0.3559\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.35339\n",
      "Epoch 5/100\n",
      " - 1s - loss: 1.2338e-04 - val_loss: 0.3562\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.35339\n",
      "Epoch 6/100\n",
      " - 1s - loss: 6.8383e-05 - val_loss: 0.3562\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.35339\n",
      "Epoch 7/100\n",
      " - 1s - loss: 4.2795e-05 - val_loss: 0.3564\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.35339\n",
      "Epoch 00007: early stopping\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/100\n",
      " - 3s - loss: 0.0044 - val_loss: 0.3566\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.35339\n",
      "Epoch 2/100\n",
      " - 1s - loss: 0.0026 - val_loss: 0.3572\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.35339\n",
      "Epoch 3/100\n",
      " - 1s - loss: 6.9348e-04 - val_loss: 0.3569\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.35339\n",
      "Epoch 4/100\n",
      " - 1s - loss: 2.5653e-04 - val_loss: 0.3566\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.35339\n",
      "Epoch 00004: early stopping\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/100\n",
      " - 3s - loss: 0.0046 - val_loss: 0.3576\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.35339\n",
      "Epoch 2/100\n",
      " - 1s - loss: 0.0025 - val_loss: 0.3571\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.35339\n",
      "Epoch 3/100\n",
      " - 1s - loss: 6.7383e-04 - val_loss: 0.3560\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.35339\n",
      "Epoch 4/100\n",
      " - 1s - loss: 2.5266e-04 - val_loss: 0.3559\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.35339\n",
      "Epoch 5/100\n",
      " - 1s - loss: 1.1837e-04 - val_loss: 0.3556\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.35339\n",
      "Epoch 6/100\n",
      " - 1s - loss: 6.5955e-05 - val_loss: 0.3557\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.35339\n",
      "Epoch 00006: early stopping\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/100\n",
      " - 3s - loss: 0.0044 - val_loss: 0.3578\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.35339\n",
      "Epoch 2/100\n",
      " - 1s - loss: 0.0025 - val_loss: 0.3572\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.35339\n",
      "Epoch 3/100\n",
      " - 1s - loss: 6.5149e-04 - val_loss: 0.3571\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.35339\n",
      "Epoch 4/100\n",
      " - 1s - loss: 2.4287e-04 - val_loss: 0.3567\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.35339\n",
      "Epoch 5/100\n",
      " - 1s - loss: 1.1477e-04 - val_loss: 0.3571\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.35339\n",
      "Epoch 6/100\n",
      " - 1s - loss: 6.4413e-05 - val_loss: 0.3572\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.35339\n",
      "Epoch 7/100\n",
      " - 1s - loss: 3.9413e-05 - val_loss: 0.3571\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.35339\n",
      "Epoch 00007: early stopping\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/100\n",
      " - 3s - loss: 0.0040 - val_loss: 0.3593\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.35339\n",
      "Epoch 2/100\n",
      " - 1s - loss: 0.0026 - val_loss: 0.3583\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.35339\n",
      "Epoch 3/100\n",
      " - 1s - loss: 7.1031e-04 - val_loss: 0.3580\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.35339\n",
      "Epoch 4/100\n",
      " - 1s - loss: 2.5490e-04 - val_loss: 0.3566\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.35339\n",
      "Epoch 5/100\n",
      " - 1s - loss: 1.1578e-04 - val_loss: 0.3567\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.35339\n",
      "Epoch 6/100\n",
      " - 1s - loss: 6.0299e-05 - val_loss: 0.3567\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.35339\n",
      "Epoch 7/100\n",
      " - 1s - loss: 3.7454e-05 - val_loss: 0.3567\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.35339\n",
      "Epoch 00007: early stopping\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/100\n",
      " - 3s - loss: 0.0040 - val_loss: 0.3574\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.35339\n",
      "Epoch 2/100\n",
      " - 1s - loss: 0.0024 - val_loss: 0.3577\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.35339\n",
      "Epoch 3/100\n",
      " - 1s - loss: 6.3702e-04 - val_loss: 0.3571\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.35339\n",
      "Epoch 4/100\n",
      " - 1s - loss: 2.3606e-04 - val_loss: 0.3573\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.35339\n",
      "Epoch 00004: early stopping\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/100\n",
      " - 3s - loss: 0.0045 - val_loss: 0.3590\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.35339\n",
      "Epoch 2/100\n",
      " - 1s - loss: 0.0023 - val_loss: 0.3547\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.35339\n",
      "Epoch 3/100\n",
      " - 1s - loss: 6.0615e-04 - val_loss: 0.3557\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.35339\n",
      "Epoch 4/100\n",
      " - 1s - loss: 2.2931e-04 - val_loss: 0.3561\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.35339\n",
      "Epoch 5/100\n",
      " - 1s - loss: 1.0626e-04 - val_loss: 0.3563\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.35339\n",
      "Epoch 00005: early stopping\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/100\n",
      " - 3s - loss: 0.0041 - val_loss: 0.3564\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.35339\n",
      "Epoch 2/100\n",
      " - 1s - loss: 0.0025 - val_loss: 0.3550\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.35339\n",
      "Epoch 3/100\n",
      " - 1s - loss: 6.5324e-04 - val_loss: 0.3541\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.35339\n",
      "Epoch 4/100\n",
      " - 1s - loss: 2.4229e-04 - val_loss: 0.3543\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.35339\n",
      "Epoch 5/100\n",
      " - 1s - loss: 1.1225e-04 - val_loss: 0.3540\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.35339\n",
      "Epoch 6/100\n",
      " - 1s - loss: 5.9689e-05 - val_loss: 0.3542\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.35339\n",
      "Epoch 7/100\n",
      " - 1s - loss: 3.7148e-05 - val_loss: 0.3540\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.35339\n",
      "Epoch 8/100\n",
      " - 1s - loss: 2.4361e-05 - val_loss: 0.3541\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.35339\n",
      "Epoch 00008: early stopping\n"
     ]
    }
   ],
   "source": [
    "#Model training with Adagrad.\n",
    "for i in range(10):\n",
    "    model_reg_relu.compile(loss='mean_squared_error', optimizer='adagrad')\n",
    "    model_reg_relu.fit(x_train_reg,y_train_reg,validation_data=(x_test_reg,y_test_reg),callbacks=[monitor,checkpointer_relu_adagrad],verbose=2,epochs=100) \n",
    "\n",
    "model_reg_relu.load_weights('./best_weights_relu_adagrad.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict stars\n",
    "pred_reg_adagrad = model_reg_relu.predict(x_test_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Business ID: diaiQrxYFU1V5qxrFnW9fg, Actual Stars: 4.0, predicted Stars: [4.0515122]\n",
      "2. Business ID: TDTASGFy_aGp6vy0i23mDA, Actual Stars: 4.0, predicted Stars: [4.673735]\n",
      "3. Business ID: VuKJ2s_JP8weQ54NfsXJXQ, Actual Stars: 3.0, predicted Stars: [4.376206]\n",
      "4. Business ID: aGiBg2WKOpXS5-1DRnBiAQ, Actual Stars: 4.5, predicted Stars: [4.5739026]\n",
      "5. Business ID: ZMmgFw2P4LWsFXNn1ZGc1g, Actual Stars: 3.0, predicted Stars: [3.2332797]\n",
      "6. Business ID: sEKFq5u8P_s0-2mAZnx0JQ, Actual Stars: 4.0, predicted Stars: [3.8514984]\n",
      "7. Business ID: rYziPPEILDXJ_F5uKR--YQ, Actual Stars: 3.5, predicted Stars: [3.4683263]\n",
      "8. Business ID: Swm_uMOWNcJDZz5lXWyzKA, Actual Stars: 5.0, predicted Stars: [4.568422]\n",
      "9. Business ID: 6nGnVP7M4qQRiclXxeqXSQ, Actual Stars: 4.5, predicted Stars: [5.029643]\n",
      "10. Business ID: Tc24GX9-ZPr4_SHU0nJZZA, Actual Stars: 2.5, predicted Stars: [2.089767]\n"
     ]
    }
   ],
   "source": [
    "#Display 10 business\n",
    "\n",
    "for i in range(10):\n",
    "    print(\"{}. Business ID: {}, Actual Stars: {}, predicted Stars: {}\".format(i+1,merge_df['business_id'][2000+i],y_test_reg[i],pred_reg_adagrad[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final score (RMSE): 0.594461977481842\n",
      "R2 score: 0.67\n"
     ]
    }
   ],
   "source": [
    "# Measure RMSE error.  RMSE is common for regression.\n",
    "score_relu_adagrad = np.sqrt(mean_squared_error(y_test_reg,pred_reg_adagrad))\n",
    "print(\"Final score (RMSE): {}\".format(score_relu_adagrad))\n",
    "print('R2 score: %.2f' % r2_score(y_test_reg, pred_reg_adagrad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up checkpointer\n",
    "checkpointer_relu_adadelta = ModelCheckpoint(filepath=\"./best_weights_relu_adadelta.hdf5\", verbose=2, save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/100\n",
      " - 3s - loss: 0.0025 - val_loss: 0.3594\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.35942, saving model to ./best_weights_relu_adadelta.hdf5\n",
      "Epoch 2/100\n",
      " - 1s - loss: 0.0035 - val_loss: 0.3581\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.35942 to 0.35807, saving model to ./best_weights_relu_adadelta.hdf5\n",
      "Epoch 3/100\n",
      " - 1s - loss: 0.0026 - val_loss: 0.3558\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.35807 to 0.35578, saving model to ./best_weights_relu_adadelta.hdf5\n",
      "Epoch 4/100\n",
      " - 1s - loss: 0.0023 - val_loss: 0.3560\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.35578\n",
      "Epoch 5/100\n",
      " - 1s - loss: 0.0023 - val_loss: 0.3571\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.35578\n",
      "Epoch 6/100\n",
      " - 1s - loss: 0.0033 - val_loss: 0.3570\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.35578\n",
      "Epoch 00006: early stopping\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/100\n",
      " - 3s - loss: 0.0020 - val_loss: 0.3581\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.35578\n",
      "Epoch 2/100\n",
      " - 2s - loss: 0.0026 - val_loss: 0.3558\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.35578\n",
      "Epoch 3/100\n",
      " - 2s - loss: 0.0034 - val_loss: 0.3553\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.35578 to 0.35535, saving model to ./best_weights_relu_adadelta.hdf5\n",
      "Epoch 4/100\n",
      " - 2s - loss: 0.0023 - val_loss: 0.3567\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.35535\n",
      "Epoch 5/100\n",
      " - 2s - loss: 0.0027 - val_loss: 0.3574\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.35535\n",
      "Epoch 00005: early stopping\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/100\n",
      " - 3s - loss: 0.0028 - val_loss: 0.3568\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.35535\n",
      "Epoch 2/100\n",
      " - 2s - loss: 0.0026 - val_loss: 0.3634\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.35535\n",
      "Epoch 3/100\n",
      " - 2s - loss: 0.0018 - val_loss: 0.3570\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.35535\n",
      "Epoch 4/100\n",
      " - 2s - loss: 0.0015 - val_loss: 0.3571\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.35535\n",
      "Epoch 00004: early stopping\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/100\n",
      " - 3s - loss: 0.0023 - val_loss: 0.3606\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.35535\n",
      "Epoch 2/100\n",
      " - 2s - loss: 0.0025 - val_loss: 0.3583\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.35535\n",
      "Epoch 3/100\n",
      " - 2s - loss: 0.0030 - val_loss: 0.3585\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.35535\n",
      "Epoch 4/100\n",
      " - 2s - loss: 0.0022 - val_loss: 0.3576\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.35535\n",
      "Epoch 5/100\n",
      " - 2s - loss: 0.0026 - val_loss: 0.3586\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.35535\n",
      "Epoch 00005: early stopping\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/100\n",
      " - 3s - loss: 0.0024 - val_loss: 0.3565\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.35535\n",
      "Epoch 2/100\n",
      " - 2s - loss: 0.0021 - val_loss: 0.3561\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.35535\n",
      "Epoch 3/100\n",
      " - 2s - loss: 0.0022 - val_loss: 0.3595\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.35535\n",
      "Epoch 4/100\n",
      " - 2s - loss: 0.0020 - val_loss: 0.3563\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.35535\n",
      "Epoch 00004: early stopping\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/100\n",
      " - 3s - loss: 0.0021 - val_loss: 0.3633\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.35535\n",
      "Epoch 2/100\n",
      " - 2s - loss: 0.0019 - val_loss: 0.3591\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.35535\n",
      "Epoch 3/100\n",
      " - 1s - loss: 0.0024 - val_loss: 0.3606\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.35535\n",
      "Epoch 4/100\n",
      " - 1s - loss: 0.0026 - val_loss: 0.3585\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.35535\n",
      "Epoch 5/100\n",
      " - 2s - loss: 0.0020 - val_loss: 0.3598\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.35535\n",
      "Epoch 00005: early stopping\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/100\n",
      " - 3s - loss: 0.0019 - val_loss: 0.3577\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.35535\n",
      "Epoch 2/100\n",
      " - 2s - loss: 0.0019 - val_loss: 0.3600\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.35535\n",
      "Epoch 3/100\n",
      " - 2s - loss: 0.0015 - val_loss: 0.3565\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.35535\n",
      "Epoch 4/100\n",
      " - 2s - loss: 0.0030 - val_loss: 0.3579\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.35535\n",
      "Epoch 5/100\n",
      " - 2s - loss: 0.0017 - val_loss: 0.3573\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.35535\n",
      "Epoch 6/100\n",
      " - 2s - loss: 0.0017 - val_loss: 0.3636\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.35535\n",
      "Epoch 00006: early stopping\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/100\n",
      " - 3s - loss: 0.0017 - val_loss: 0.3577\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.35535\n",
      "Epoch 2/100\n",
      " - 2s - loss: 0.0021 - val_loss: 0.3575\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.35535\n",
      "Epoch 3/100\n",
      " - 2s - loss: 0.0020 - val_loss: 0.3591\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.35535\n",
      "Epoch 4/100\n",
      " - 2s - loss: 0.0019 - val_loss: 0.3565\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.35535\n",
      "Epoch 5/100\n",
      " - 2s - loss: 0.0020 - val_loss: 0.3565\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.35535\n",
      "Epoch 6/100\n",
      " - 1s - loss: 0.0020 - val_loss: 0.3574\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.35535\n",
      "Epoch 7/100\n",
      " - 2s - loss: 0.0016 - val_loss: 0.3569\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.35535\n",
      "Epoch 00007: early stopping\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/100\n",
      " - 3s - loss: 0.0022 - val_loss: 0.3591\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.35535\n",
      "Epoch 2/100\n",
      " - 2s - loss: 0.0028 - val_loss: 0.3676\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.35535\n",
      "Epoch 3/100\n",
      " - 2s - loss: 0.0019 - val_loss: 0.3593\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.35535\n",
      "Epoch 4/100\n",
      " - 2s - loss: 0.0017 - val_loss: 0.3581\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.35535\n",
      "Epoch 5/100\n",
      " - 2s - loss: 0.0018 - val_loss: 0.3613\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.35535\n",
      "Epoch 6/100\n",
      " - 2s - loss: 0.0025 - val_loss: 0.3590\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.35535\n",
      "Epoch 7/100\n",
      " - 2s - loss: 0.0018 - val_loss: 0.3568\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.35535\n",
      "Epoch 8/100\n",
      " - 2s - loss: 0.0012 - val_loss: 0.3618\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.35535\n",
      "Epoch 9/100\n",
      " - 2s - loss: 0.0021 - val_loss: 0.3582\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.35535\n",
      "Epoch 10/100\n",
      " - 1s - loss: 0.0024 - val_loss: 0.3589\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.35535\n",
      "Epoch 00010: early stopping\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/100\n",
      " - 3s - loss: 0.0017 - val_loss: 0.3620\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.35535\n",
      "Epoch 2/100\n",
      " - 2s - loss: 0.0026 - val_loss: 0.3705\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.35535\n",
      "Epoch 3/100\n",
      " - 2s - loss: 0.0022 - val_loss: 0.3605\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.35535\n",
      "Epoch 4/100\n",
      " - 2s - loss: 0.0017 - val_loss: 0.3574\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.35535\n",
      "Epoch 5/100\n",
      " - 2s - loss: 0.0016 - val_loss: 0.3579\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.35535\n",
      "Epoch 6/100\n",
      " - 2s - loss: 0.0016 - val_loss: 0.3564\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.35535\n",
      "Epoch 7/100\n",
      " - 2s - loss: 0.0021 - val_loss: 0.3569\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.35535\n",
      "Epoch 8/100\n",
      " - 2s - loss: 0.0016 - val_loss: 0.3563\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.35535\n",
      "Epoch 9/100\n",
      " - 2s - loss: 0.0022 - val_loss: 0.3590\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.35535\n",
      "Epoch 00009: early stopping\n"
     ]
    }
   ],
   "source": [
    "#Model training with ADadelta.\n",
    "for i in range(10):\n",
    "    model_reg_relu.compile(loss='mean_squared_error', optimizer='adadelta')\n",
    "    model_reg_relu.fit(x_train_reg,y_train_reg,validation_data=(x_test_reg,y_test_reg),callbacks=[monitor,checkpointer_relu_adadelta],verbose=2,epochs=100) \n",
    "\n",
    "model_reg_relu.load_weights('./best_weights_relu_adadelta.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict stars\n",
    "pred_reg_adadelta = model_reg_relu.predict(x_test_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Business ID: diaiQrxYFU1V5qxrFnW9fg, Actual Stars: 4.0, predicted Stars: [4.0375576]\n",
      "2. Business ID: TDTASGFy_aGp6vy0i23mDA, Actual Stars: 4.0, predicted Stars: [4.6432195]\n",
      "3. Business ID: VuKJ2s_JP8weQ54NfsXJXQ, Actual Stars: 3.0, predicted Stars: [4.420752]\n",
      "4. Business ID: aGiBg2WKOpXS5-1DRnBiAQ, Actual Stars: 4.5, predicted Stars: [4.5686216]\n",
      "5. Business ID: ZMmgFw2P4LWsFXNn1ZGc1g, Actual Stars: 3.0, predicted Stars: [3.2340143]\n",
      "6. Business ID: sEKFq5u8P_s0-2mAZnx0JQ, Actual Stars: 4.0, predicted Stars: [3.8594866]\n",
      "7. Business ID: rYziPPEILDXJ_F5uKR--YQ, Actual Stars: 3.5, predicted Stars: [3.4584217]\n",
      "8. Business ID: Swm_uMOWNcJDZz5lXWyzKA, Actual Stars: 5.0, predicted Stars: [4.526681]\n",
      "9. Business ID: 6nGnVP7M4qQRiclXxeqXSQ, Actual Stars: 4.5, predicted Stars: [4.9973984]\n",
      "10. Business ID: Tc24GX9-ZPr4_SHU0nJZZA, Actual Stars: 2.5, predicted Stars: [2.0598068]\n"
     ]
    }
   ],
   "source": [
    "#Display 10 business\n",
    "\n",
    "for i in range(10):\n",
    "    print(\"{}. Business ID: {}, Actual Stars: {}, predicted Stars: {}\".format(i+1,merge_df['business_id'][2000+i],y_test_reg[i],pred_reg_adadelta[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final score (RMSE): 0.5961122512817383\n",
      "R2 score: 0.67\n"
     ]
    }
   ],
   "source": [
    "# Measure RMSE error.  RMSE is common for regression.\n",
    "score_relu_adadelta = np.sqrt(mean_squared_error(y_test_reg,pred_reg_adadelta))\n",
    "print(\"Final score (RMSE): {}\".format(score_relu_adadelta))\n",
    "print('R2 score: %.2f' % r2_score(y_test_reg, pred_reg_adadelta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up checkpointer\n",
    "checkpointer_relu_adamax = ModelCheckpoint(filepath=\"./best_weights_relu_adamax.hdf5\",verbose=2, save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/100\n",
      " - 3s - loss: 6.3042e-04 - val_loss: 0.3568\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.35680, saving model to ./best_weights_relu_adamax.hdf5\n",
      "Epoch 2/100\n",
      " - 1s - loss: 3.6580e-04 - val_loss: 0.3562\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.35680 to 0.35620, saving model to ./best_weights_relu_adamax.hdf5\n",
      "Epoch 3/100\n",
      " - 1s - loss: 2.3040e-04 - val_loss: 0.3557\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.35620 to 0.35571, saving model to ./best_weights_relu_adamax.hdf5\n",
      "Epoch 4/100\n",
      " - 1s - loss: 1.9983e-04 - val_loss: 0.3560\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.35571\n",
      "Epoch 5/100\n",
      " - 1s - loss: 2.3064e-04 - val_loss: 0.3552\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.35571 to 0.35524, saving model to ./best_weights_relu_adamax.hdf5\n",
      "Epoch 6/100\n",
      " - 1s - loss: 2.6202e-04 - val_loss: 0.3565\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.35524\n",
      "Epoch 00006: early stopping\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/100\n",
      " - 3s - loss: 4.6357e-04 - val_loss: 0.3561\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.35524\n",
      "Epoch 2/100\n",
      " - 1s - loss: 2.8782e-04 - val_loss: 0.3571\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.35524\n",
      "Epoch 3/100\n",
      " - 1s - loss: 1.7542e-04 - val_loss: 0.3562\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.35524\n",
      "Epoch 4/100\n",
      " - 1s - loss: 1.7036e-04 - val_loss: 0.3565\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.35524\n",
      "Epoch 00004: early stopping\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/100\n",
      " - 3s - loss: 4.9961e-04 - val_loss: 0.3560\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.35524\n",
      "Epoch 2/100\n",
      " - 1s - loss: 2.3605e-04 - val_loss: 0.3556\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.35524\n",
      "Epoch 3/100\n",
      " - 1s - loss: 1.2363e-04 - val_loss: 0.3565\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.35524\n",
      "Epoch 4/100\n",
      " - 1s - loss: 1.1117e-04 - val_loss: 0.3561\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.35524\n",
      "Epoch 00004: early stopping\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/100\n",
      " - 3s - loss: 4.7803e-04 - val_loss: 0.3560\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.35524\n",
      "Epoch 2/100\n",
      " - 1s - loss: 2.4770e-04 - val_loss: 0.3557\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.35524\n",
      "Epoch 3/100\n",
      " - 1s - loss: 1.3883e-04 - val_loss: 0.3561\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.35524\n",
      "Epoch 4/100\n",
      " - 1s - loss: 1.3666e-04 - val_loss: 0.3560\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.35524\n",
      "Epoch 00004: early stopping\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/100\n",
      " - 3s - loss: 3.4543e-04 - val_loss: 0.3561\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.35524\n",
      "Epoch 2/100\n",
      " - 2s - loss: 2.9761e-04 - val_loss: 0.3553\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.35524\n",
      "Epoch 3/100\n",
      " - 2s - loss: 1.8680e-04 - val_loss: 0.3559\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.35524\n",
      "Epoch 4/100\n",
      " - 2s - loss: 1.9222e-04 - val_loss: 0.3556\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.35524\n",
      "Epoch 00004: early stopping\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/100\n",
      " - 3s - loss: 3.9045e-04 - val_loss: 0.3565\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.35524\n",
      "Epoch 2/100\n",
      " - 2s - loss: 2.9406e-04 - val_loss: 0.3559\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.35524\n",
      "Epoch 3/100\n",
      " - 2s - loss: 1.7070e-04 - val_loss: 0.3559\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.35524\n",
      "Epoch 4/100\n",
      " - 2s - loss: 1.5331e-04 - val_loss: 0.3548\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.35524 to 0.35483, saving model to ./best_weights_relu_adamax.hdf5\n",
      "Epoch 5/100\n",
      " - 2s - loss: 2.3524e-04 - val_loss: 0.3563\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.35483\n",
      "Epoch 6/100\n",
      " - 2s - loss: 2.7637e-04 - val_loss: 0.3556\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.35483\n",
      "Epoch 7/100\n",
      " - 2s - loss: 2.4551e-04 - val_loss: 0.3562\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.35483\n",
      "Epoch 00007: early stopping\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/100\n",
      " - 3s - loss: 3.5320e-04 - val_loss: 0.3558\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.35483\n",
      "Epoch 2/100\n",
      " - 2s - loss: 3.0183e-04 - val_loss: 0.3554\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.35483\n",
      "Epoch 3/100\n",
      " - 2s - loss: 1.8547e-04 - val_loss: 0.3554\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.35483\n",
      "Epoch 4/100\n",
      " - 2s - loss: 1.8806e-04 - val_loss: 0.3556\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.35483\n",
      "Epoch 00004: early stopping\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/100\n",
      " - 3s - loss: 3.4187e-04 - val_loss: 0.3554\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.35483\n",
      "Epoch 2/100\n",
      " - 1s - loss: 2.7917e-04 - val_loss: 0.3556\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.35483\n",
      "Epoch 3/100\n",
      " - 1s - loss: 1.8118e-04 - val_loss: 0.3549\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.35483\n",
      "Epoch 4/100\n",
      " - 1s - loss: 1.7952e-04 - val_loss: 0.3557\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.35483\n",
      "Epoch 00004: early stopping\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/100\n",
      " - 4s - loss: 4.7487e-04 - val_loss: 0.3550\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.35483\n",
      "Epoch 2/100\n",
      " - 2s - loss: 2.5052e-04 - val_loss: 0.3551\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.35483\n",
      "Epoch 3/100\n",
      " - 2s - loss: 1.3174e-04 - val_loss: 0.3547\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.35483 to 0.35472, saving model to ./best_weights_relu_adamax.hdf5\n",
      "Epoch 4/100\n",
      " - 2s - loss: 1.2966e-04 - val_loss: 0.3551\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.35472\n",
      "Epoch 00004: early stopping\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/100\n",
      " - 4s - loss: 4.9303e-04 - val_loss: 0.3562\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.35472\n",
      "Epoch 2/100\n",
      " - 1s - loss: 2.2283e-04 - val_loss: 0.3546\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.35472 to 0.35464, saving model to ./best_weights_relu_adamax.hdf5\n",
      "Epoch 3/100\n",
      " - 1s - loss: 1.1641e-04 - val_loss: 0.3555\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.35464\n",
      "Epoch 4/100\n",
      " - 1s - loss: 1.1385e-04 - val_loss: 0.3552\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.35464\n",
      "Epoch 5/100\n",
      " - 1s - loss: 1.5790e-04 - val_loss: 0.3549\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.35464\n",
      "Epoch 00005: early stopping\n"
     ]
    }
   ],
   "source": [
    "#Model training with Stochastic gradient descent optimizer(SGD).\n",
    "for i in range(10):\n",
    "    model_reg_relu.compile(loss='mean_squared_error', optimizer='adamax')\n",
    "    model_reg_relu.fit(x_train_reg,y_train_reg,validation_data=(x_test_reg,y_test_reg),callbacks=[monitor,checkpointer_relu_adamax],verbose=2,epochs=100) \n",
    "\n",
    "model_reg_relu.load_weights('./best_weights_relu_adamax.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict stars\n",
    "pred_reg_adamax = model_reg_relu.predict(x_test_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Business ID: diaiQrxYFU1V5qxrFnW9fg, Actual Stars: 4.0, predicted Stars: [4.1020093]\n",
      "2. Business ID: TDTASGFy_aGp6vy0i23mDA, Actual Stars: 4.0, predicted Stars: [4.6861453]\n",
      "3. Business ID: VuKJ2s_JP8weQ54NfsXJXQ, Actual Stars: 3.0, predicted Stars: [4.4172535]\n",
      "4. Business ID: aGiBg2WKOpXS5-1DRnBiAQ, Actual Stars: 4.5, predicted Stars: [4.629422]\n",
      "5. Business ID: ZMmgFw2P4LWsFXNn1ZGc1g, Actual Stars: 3.0, predicted Stars: [3.2480528]\n",
      "6. Business ID: sEKFq5u8P_s0-2mAZnx0JQ, Actual Stars: 4.0, predicted Stars: [3.8422844]\n",
      "7. Business ID: rYziPPEILDXJ_F5uKR--YQ, Actual Stars: 3.5, predicted Stars: [3.4617307]\n",
      "8. Business ID: Swm_uMOWNcJDZz5lXWyzKA, Actual Stars: 5.0, predicted Stars: [4.563878]\n",
      "9. Business ID: 6nGnVP7M4qQRiclXxeqXSQ, Actual Stars: 4.5, predicted Stars: [5.0696855]\n",
      "10. Business ID: Tc24GX9-ZPr4_SHU0nJZZA, Actual Stars: 2.5, predicted Stars: [2.0938659]\n"
     ]
    }
   ],
   "source": [
    "#Display 10 business\n",
    "\n",
    "for i in range(10):\n",
    "    print(\"{}. Business ID: {}, Actual Stars: {}, predicted Stars: {}\".format(i+1,merge_df['business_id'][2000+i],y_test_reg[i],pred_reg_adamax[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final score (RMSE): 0.5955179333686829\n",
      "R2 score: 0.67\n"
     ]
    }
   ],
   "source": [
    "# Measure RMSE error.  RMSE is common for regression.\n",
    "score_relu_adamax = np.sqrt(mean_squared_error(y_test_reg,pred_reg_adamax))\n",
    "print(\"Final score (RMSE): {}\".format(score_relu_adamax))\n",
    "print('R2 score: %.2f' % r2_score(y_test_reg, pred_reg_adamax))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up checkpointer\n",
    "checkpointer_relu_nadam = ModelCheckpoint(filepath=\"./best_weights_relu_nadam.hdf5\", verbose=2, save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/100\n",
      " - 4s - loss: 0.0067 - val_loss: 0.3663\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.36634, saving model to ./best_weights_relu_nadam.hdf5\n",
      "Epoch 2/100\n",
      " - 2s - loss: 0.0231 - val_loss: 0.3696\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.36634\n",
      "Epoch 3/100\n",
      " - 2s - loss: 0.0179 - val_loss: 0.3530\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.36634 to 0.35299, saving model to ./best_weights_relu_nadam.hdf5\n",
      "Epoch 4/100\n",
      " - 2s - loss: 0.0113 - val_loss: 0.3528\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.35299 to 0.35276, saving model to ./best_weights_relu_nadam.hdf5\n",
      "Epoch 5/100\n",
      " - 2s - loss: 0.0083 - val_loss: 0.3549\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.35276\n",
      "Epoch 6/100\n",
      " - 2s - loss: 0.0079 - val_loss: 0.3525\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.35276 to 0.35245, saving model to ./best_weights_relu_nadam.hdf5\n",
      "Epoch 00006: early stopping\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/100\n",
      " - 4s - loss: 0.0100 - val_loss: 0.3521\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.35245 to 0.35213, saving model to ./best_weights_relu_nadam.hdf5\n",
      "Epoch 2/100\n",
      " - 2s - loss: 0.0116 - val_loss: 0.3518\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.35213 to 0.35180, saving model to ./best_weights_relu_nadam.hdf5\n",
      "Epoch 3/100\n",
      " - 2s - loss: 0.0117 - val_loss: 0.3604\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.35180\n",
      "Epoch 4/100\n",
      " - 2s - loss: 0.0105 - val_loss: 0.3499\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.35180 to 0.34990, saving model to ./best_weights_relu_nadam.hdf5\n",
      "Epoch 5/100\n",
      " - 2s - loss: 0.0092 - val_loss: 0.3479\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.34990 to 0.34786, saving model to ./best_weights_relu_nadam.hdf5\n",
      "Epoch 6/100\n",
      " - 2s - loss: 0.0079 - val_loss: 0.3446\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.34786 to 0.34456, saving model to ./best_weights_relu_nadam.hdf5\n",
      "Epoch 7/100\n",
      " - 2s - loss: 0.0071 - val_loss: 0.3481\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.34456\n",
      "Epoch 8/100\n",
      " - 2s - loss: 0.0072 - val_loss: 0.3507\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.34456\n",
      "Epoch 9/100\n",
      " - 2s - loss: 0.0074 - val_loss: 0.3463\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.34456\n",
      "Epoch 00009: early stopping\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/100\n",
      " - 4s - loss: 0.0084 - val_loss: 0.3428\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.34456 to 0.34277, saving model to ./best_weights_relu_nadam.hdf5\n",
      "Epoch 2/100\n",
      " - 2s - loss: 0.0085 - val_loss: 0.3552\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.34277\n",
      "Epoch 3/100\n",
      " - 2s - loss: 0.0089 - val_loss: 0.3422\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.34277 to 0.34219, saving model to ./best_weights_relu_nadam.hdf5\n",
      "Epoch 4/100\n",
      " - 2s - loss: 0.0082 - val_loss: 0.3482\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.34219\n",
      "Epoch 00004: early stopping\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/100\n",
      " - 4s - loss: 0.0077 - val_loss: 0.3505\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.34219\n",
      "Epoch 2/100\n",
      " - 2s - loss: 0.0073 - val_loss: 0.3462\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.34219\n",
      "Epoch 3/100\n",
      " - 2s - loss: 0.0082 - val_loss: 0.3466\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.34219\n",
      "Epoch 4/100\n",
      " - 2s - loss: 0.0076 - val_loss: 0.3435\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.34219\n",
      "Epoch 5/100\n",
      " - 2s - loss: 0.0064 - val_loss: 0.3481\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.34219\n",
      "Epoch 6/100\n",
      " - 2s - loss: 0.0063 - val_loss: 0.3432\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.34219\n",
      "Epoch 7/100\n",
      " - 2s - loss: 0.0063 - val_loss: 0.3452\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.34219\n",
      "Epoch 00007: early stopping\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/100\n",
      " - 5s - loss: 0.0064 - val_loss: 0.3436\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.34219\n",
      "Epoch 2/100\n",
      " - 2s - loss: 0.0069 - val_loss: 0.3424\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.34219\n",
      "Epoch 3/100\n",
      " - 2s - loss: 0.0068 - val_loss: 0.3452\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.34219\n",
      "Epoch 4/100\n",
      " - 2s - loss: 0.0064 - val_loss: 0.3436\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.34219\n",
      "Epoch 5/100\n",
      " - 2s - loss: 0.0059 - val_loss: 0.3423\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.34219\n",
      "Epoch 00005: early stopping\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/100\n",
      " - 4s - loss: 0.0062 - val_loss: 0.3397\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.34219 to 0.33968, saving model to ./best_weights_relu_nadam.hdf5\n",
      "Epoch 2/100\n",
      " - 2s - loss: 0.0057 - val_loss: 0.3538\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.33968\n",
      "Epoch 3/100\n",
      " - 2s - loss: 0.0060 - val_loss: 0.3440\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.33968\n",
      "Epoch 4/100\n",
      " - 2s - loss: 0.0059 - val_loss: 0.3405\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.33968\n",
      "Epoch 00004: early stopping\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/100\n",
      " - 4s - loss: 0.0062 - val_loss: 0.3429\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.33968\n",
      "Epoch 2/100\n",
      " - 2s - loss: 0.0053 - val_loss: 0.3428\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.33968\n",
      "Epoch 3/100\n",
      " - 2s - loss: 0.0052 - val_loss: 0.3399\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.33968\n",
      "Epoch 4/100\n",
      " - 2s - loss: 0.0056 - val_loss: 0.3451\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.33968\n",
      "Epoch 5/100\n",
      " - 2s - loss: 0.0052 - val_loss: 0.3446\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.33968\n",
      "Epoch 6/100\n",
      " - 2s - loss: 0.0049 - val_loss: 0.3413\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.33968\n",
      "Epoch 00006: early stopping\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/100\n",
      " - 4s - loss: 0.0054 - val_loss: 0.3402\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.33968\n",
      "Epoch 2/100\n",
      " - 2s - loss: 0.0051 - val_loss: 0.3417\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.33968\n",
      "Epoch 3/100\n",
      " - 2s - loss: 0.0051 - val_loss: 0.3356\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.33968 to 0.33556, saving model to ./best_weights_relu_nadam.hdf5\n",
      "Epoch 4/100\n",
      " - 2s - loss: 0.0049 - val_loss: 0.3432\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.33556\n",
      "Epoch 5/100\n",
      " - 2s - loss: 0.0048 - val_loss: 0.3347\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.33556 to 0.33473, saving model to ./best_weights_relu_nadam.hdf5\n",
      "Epoch 6/100\n",
      " - 2s - loss: 0.0045 - val_loss: 0.3395\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.33473\n",
      "Epoch 00006: early stopping\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/100\n",
      " - 4s - loss: 0.0050 - val_loss: 0.3390\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.33473\n",
      "Epoch 2/100\n",
      " - 2s - loss: 0.0048 - val_loss: 0.3401\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.33473\n",
      "Epoch 3/100\n",
      " - 2s - loss: 0.0049 - val_loss: 0.3410\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.33473\n",
      "Epoch 4/100\n",
      " - 2s - loss: 0.0046 - val_loss: 0.3393\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.33473\n",
      "Epoch 00004: early stopping\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/100\n",
      " - 4s - loss: 0.0045 - val_loss: 0.3396\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.33473\n",
      "Epoch 2/100\n",
      " - 2s - loss: 0.0042 - val_loss: 0.3374\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.33473\n",
      "Epoch 3/100\n",
      " - 2s - loss: 0.0047 - val_loss: 0.3380\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.33473\n",
      "Epoch 4/100\n",
      " - 2s - loss: 0.0046 - val_loss: 0.3404\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.33473\n",
      "Epoch 5/100\n",
      " - 2s - loss: 0.0040 - val_loss: 0.3433\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.33473\n",
      "Epoch 00005: early stopping\n"
     ]
    }
   ],
   "source": [
    "#Model training with Stochastic gradient descent optimizer(SGD).\n",
    "for i in range(10):\n",
    "    model_reg_relu.compile(loss='mean_squared_error', optimizer='nadam')\n",
    "    model_reg_relu.fit(x_train_reg,y_train_reg,validation_data=(x_test_reg,y_test_reg),callbacks=[monitor,checkpointer_relu_nadam],verbose=2,epochs=100) \n",
    "\n",
    "model_reg_relu.load_weights('./best_weights_relu_nadam.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict stars\n",
    "pred_reg_nadam = model_reg_relu.predict(x_test_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Business ID: diaiQrxYFU1V5qxrFnW9fg, Actual Stars: 4.0, predicted Stars: [3.88986]\n",
      "2. Business ID: TDTASGFy_aGp6vy0i23mDA, Actual Stars: 4.0, predicted Stars: [4.6396694]\n",
      "3. Business ID: VuKJ2s_JP8weQ54NfsXJXQ, Actual Stars: 3.0, predicted Stars: [4.2682767]\n",
      "4. Business ID: aGiBg2WKOpXS5-1DRnBiAQ, Actual Stars: 4.5, predicted Stars: [4.645605]\n",
      "5. Business ID: ZMmgFw2P4LWsFXNn1ZGc1g, Actual Stars: 3.0, predicted Stars: [3.3555214]\n",
      "6. Business ID: sEKFq5u8P_s0-2mAZnx0JQ, Actual Stars: 4.0, predicted Stars: [3.8175664]\n",
      "7. Business ID: rYziPPEILDXJ_F5uKR--YQ, Actual Stars: 3.5, predicted Stars: [3.446373]\n",
      "8. Business ID: Swm_uMOWNcJDZz5lXWyzKA, Actual Stars: 5.0, predicted Stars: [4.7038507]\n",
      "9. Business ID: 6nGnVP7M4qQRiclXxeqXSQ, Actual Stars: 4.5, predicted Stars: [4.9353976]\n",
      "10. Business ID: Tc24GX9-ZPr4_SHU0nJZZA, Actual Stars: 2.5, predicted Stars: [2.0834134]\n"
     ]
    }
   ],
   "source": [
    "#Display 10 business\n",
    "\n",
    "for i in range(10):\n",
    "    print(\"{}. Business ID: {}, Actual Stars: {}, predicted Stars: {}\".format(i+1,merge_df['business_id'][2000+i],y_test_reg[i],pred_reg_nadam[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final score (RMSE): 0.5785612463951111\n",
      "R2 score: 0.69\n"
     ]
    }
   ],
   "source": [
    "# Measure RMSE error.  RMSE is common for regression.\n",
    "score_relu_nadam = np.sqrt(mean_squared_error(y_test_reg,pred_reg_nadam))\n",
    "print(\"Final score (RMSE): {}\".format(score_relu_nadam))\n",
    "print('R2 score: %.2f' % r2_score(y_test_reg, pred_reg_nadam))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting RMSE score for different Optimizers for relu model\n",
    "\n",
    "score_list_optimizer=[score_reg_relu_stopping,score_relu_sgd,score_relu_rmsprop,score_relu_adagrad,score_relu_adadelta,score_relu_adamax,score_relu_nadam]\n",
    "names =['adam','SGD','RMSprop','Adagrad','Adadelta','Adamax','Nadam']\n",
    "tick_marks = np.arange(len(names))\n",
    "plt.bar(range(len(score_list_optimizer)), score_list_optimizer)\n",
    "plt.xticks(tick_marks, names, rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments with Hidden nodes selection in hidden layer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up checkpointer\n",
    "checkpointer_relu_2l = ModelCheckpoint(filepath=\"./best_weights_relu_2l.hdf5\", verbose=2, save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensorflow model for Regression with sigmoid and forward approach\n",
    "model_reg_relu = Sequential()\n",
    "\n",
    "model_reg_relu.add(Dense(60, input_dim=x_train_reg.shape[1], activation='relu'))  \n",
    "model_reg_relu.add(Dense(30, activation='relu')) # Hidden 2\n",
    "model_reg_relu.add(Dense(1)) # Output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/100\n",
      " - 4s - loss: 2.2041 - val_loss: 0.4717\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.47170, saving model to ./best_weights_relu_2l.hdf5\n",
      "Epoch 2/100\n",
      " - 2s - loss: 0.3230 - val_loss: 0.3951\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.47170 to 0.39510, saving model to ./best_weights_relu_2l.hdf5\n",
      "Epoch 3/100\n",
      " - 2s - loss: 0.2180 - val_loss: 0.3842\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.39510 to 0.38424, saving model to ./best_weights_relu_2l.hdf5\n",
      "Epoch 4/100\n",
      " - 2s - loss: 0.1733 - val_loss: 0.3834\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.38424 to 0.38335, saving model to ./best_weights_relu_2l.hdf5\n",
      "Epoch 5/100\n",
      " - 2s - loss: 0.1474 - val_loss: 0.3917\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.38335\n",
      "Epoch 6/100\n",
      " - 2s - loss: 0.1307 - val_loss: 0.3999\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.38335\n",
      "Epoch 00006: early stopping\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/100\n",
      " - 4s - loss: 0.1289 - val_loss: 0.3976\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.38335\n",
      "Epoch 2/100\n",
      " - 2s - loss: 0.1126 - val_loss: 0.3996\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.38335\n",
      "Epoch 3/100\n",
      " - 2s - loss: 0.0975 - val_loss: 0.4154\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.38335\n",
      "Epoch 4/100\n",
      " - 2s - loss: 0.0822 - val_loss: 0.4121\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.38335\n",
      "Epoch 00004: early stopping\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/100\n",
      " - 4s - loss: 0.0710 - val_loss: 0.3982\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.38335\n",
      "Epoch 2/100\n",
      " - 2s - loss: 0.0547 - val_loss: 0.4063\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.38335\n",
      "Epoch 3/100\n",
      " - 2s - loss: 0.0462 - val_loss: 0.4015\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.38335\n",
      "Epoch 4/100\n",
      " - 2s - loss: 0.0358 - val_loss: 0.3970\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.38335\n",
      "Epoch 5/100\n",
      " - 2s - loss: 0.0299 - val_loss: 0.4013\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.38335\n",
      "Epoch 6/100\n",
      " - 2s - loss: 0.0250 - val_loss: 0.3934\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.38335\n",
      "Epoch 7/100\n",
      " - 2s - loss: 0.0215 - val_loss: 0.4005\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.38335\n",
      "Epoch 8/100\n",
      " - 2s - loss: 0.0194 - val_loss: 0.3911\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.38335\n",
      "Epoch 9/100\n",
      " - 2s - loss: 0.0177 - val_loss: 0.3962\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.38335\n",
      "Epoch 10/100\n",
      " - 2s - loss: 0.0161 - val_loss: 0.3882\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.38335\n",
      "Epoch 11/100\n",
      " - 2s - loss: 0.0150 - val_loss: 0.3872\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.38335\n",
      "Epoch 12/100\n",
      " - 2s - loss: 0.0146 - val_loss: 0.3908\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.38335\n",
      "Epoch 13/100\n",
      " - 2s - loss: 0.0138 - val_loss: 0.3896\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.38335\n",
      "Epoch 00013: early stopping\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/100\n",
      " - 4s - loss: 0.0152 - val_loss: 0.3894\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.38335\n",
      "Epoch 2/100\n",
      " - 2s - loss: 0.0160 - val_loss: 0.3880\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.38335\n",
      "Epoch 3/100\n",
      " - 2s - loss: 0.0163 - val_loss: 0.3807\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.38335 to 0.38074, saving model to ./best_weights_relu_2l.hdf5\n",
      "Epoch 4/100\n",
      " - 2s - loss: 0.0155 - val_loss: 0.3796\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.38074 to 0.37958, saving model to ./best_weights_relu_2l.hdf5\n",
      "Epoch 5/100\n",
      " - 2s - loss: 0.0136 - val_loss: 0.3810\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.37958\n",
      "Epoch 6/100\n",
      " - 2s - loss: 0.0121 - val_loss: 0.3726\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.37958 to 0.37261, saving model to ./best_weights_relu_2l.hdf5\n",
      "Epoch 7/100\n",
      " - 2s - loss: 0.0111 - val_loss: 0.3758\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.37261\n",
      "Epoch 8/100\n",
      " - 2s - loss: 0.0106 - val_loss: 0.3715\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.37261 to 0.37145, saving model to ./best_weights_relu_2l.hdf5\n",
      "Epoch 9/100\n",
      " - 2s - loss: 0.0099 - val_loss: 0.3765\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.37145\n",
      "Epoch 10/100\n",
      " - 2s - loss: 0.0099 - val_loss: 0.3692\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.37145 to 0.36916, saving model to ./best_weights_relu_2l.hdf5\n",
      "Epoch 11/100\n",
      " - 2s - loss: 0.0100 - val_loss: 0.3681\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.36916 to 0.36809, saving model to ./best_weights_relu_2l.hdf5\n",
      "Epoch 12/100\n",
      " - 2s - loss: 0.0090 - val_loss: 0.3684\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.36809\n",
      "Epoch 13/100\n",
      " - 2s - loss: 0.0087 - val_loss: 0.3694\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.36809\n",
      "Epoch 14/100\n",
      " - 2s - loss: 0.0085 - val_loss: 0.3632\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.36809 to 0.36317, saving model to ./best_weights_relu_2l.hdf5\n",
      "Epoch 15/100\n",
      " - 2s - loss: 0.0083 - val_loss: 0.3686\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.36317\n",
      "Epoch 16/100\n",
      " - 2s - loss: 0.0084 - val_loss: 0.3711\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.36317\n",
      "Epoch 17/100\n",
      " - 2s - loss: 0.0092 - val_loss: 0.3688\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.36317\n",
      "Epoch 00017: early stopping\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/100\n",
      " - 4s - loss: 0.0089 - val_loss: 0.3645\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.36317\n",
      "Epoch 2/100\n",
      " - 2s - loss: 0.0082 - val_loss: 0.3611\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.36317 to 0.36107, saving model to ./best_weights_relu_2l.hdf5\n",
      "Epoch 3/100\n",
      " - 2s - loss: 0.0090 - val_loss: 0.3666\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.36107\n",
      "Epoch 4/100\n",
      " - 2s - loss: 0.0088 - val_loss: 0.3603\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.36107 to 0.36032, saving model to ./best_weights_relu_2l.hdf5\n",
      "Epoch 5/100\n",
      " - 2s - loss: 0.0082 - val_loss: 0.3611\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.36032\n",
      "Epoch 00005: early stopping\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/100\n",
      " - 4s - loss: 0.0081 - val_loss: 0.3569\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.36032 to 0.35686, saving model to ./best_weights_relu_2l.hdf5\n",
      "Epoch 2/100\n",
      " - 2s - loss: 0.0070 - val_loss: 0.3550\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.35686 to 0.35500, saving model to ./best_weights_relu_2l.hdf5\n",
      "Epoch 3/100\n",
      " - 2s - loss: 0.0075 - val_loss: 0.3556\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.35500\n",
      "Epoch 4/100\n",
      " - 2s - loss: 0.0080 - val_loss: 0.3571\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.35500\n",
      "Epoch 5/100\n",
      " - 2s - loss: 0.0078 - val_loss: 0.3566\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.35500\n",
      "Epoch 00005: early stopping\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/100\n",
      " - 4s - loss: 0.0070 - val_loss: 0.3539\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.35500 to 0.35390, saving model to ./best_weights_relu_2l.hdf5\n",
      "Epoch 2/100\n",
      " - 2s - loss: 0.0064 - val_loss: 0.3592\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.35390\n",
      "Epoch 3/100\n",
      " - 2s - loss: 0.0067 - val_loss: 0.3549\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.35390\n",
      "Epoch 4/100\n",
      " - 2s - loss: 0.0073 - val_loss: 0.3567\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.35390\n",
      "Epoch 00004: early stopping\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/100\n",
      " - 4s - loss: 0.0068 - val_loss: 0.3531\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.35390 to 0.35309, saving model to ./best_weights_relu_2l.hdf5\n",
      "Epoch 2/100\n",
      " - 2s - loss: 0.0055 - val_loss: 0.3554\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.35309\n",
      "Epoch 3/100\n",
      " - 2s - loss: 0.0060 - val_loss: 0.3515\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.35309 to 0.35150, saving model to ./best_weights_relu_2l.hdf5\n",
      "Epoch 4/100\n",
      " - 2s - loss: 0.0064 - val_loss: 0.3515\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.35150 to 0.35149, saving model to ./best_weights_relu_2l.hdf5\n",
      "Epoch 5/100\n",
      " - 2s - loss: 0.0065 - val_loss: 0.3530\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.35149\n",
      "Epoch 6/100\n",
      " - 2s - loss: 0.0064 - val_loss: 0.3512\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.35149 to 0.35124, saving model to ./best_weights_relu_2l.hdf5\n",
      "Epoch 00006: early stopping\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/100\n",
      " - 4s - loss: 0.0059 - val_loss: 0.3501\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.35124 to 0.35012, saving model to ./best_weights_relu_2l.hdf5\n",
      "Epoch 2/100\n",
      " - 2s - loss: 0.0050 - val_loss: 0.3490\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.35012 to 0.34896, saving model to ./best_weights_relu_2l.hdf5\n",
      "Epoch 3/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 2s - loss: 0.0052 - val_loss: 0.3496\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.34896\n",
      "Epoch 4/100\n",
      " - 2s - loss: 0.0055 - val_loss: 0.3477\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.34896 to 0.34771, saving model to ./best_weights_relu_2l.hdf5\n",
      "Epoch 5/100\n",
      " - 2s - loss: 0.0056 - val_loss: 0.3478\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.34771\n",
      "Epoch 6/100\n",
      " - 2s - loss: 0.0056 - val_loss: 0.3514\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.34771\n",
      "Epoch 7/100\n",
      " - 2s - loss: 0.0052 - val_loss: 0.3473\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.34771 to 0.34734, saving model to ./best_weights_relu_2l.hdf5\n",
      "Epoch 00007: early stopping\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/100\n",
      " - 4s - loss: 0.0049 - val_loss: 0.3469\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.34734 to 0.34686, saving model to ./best_weights_relu_2l.hdf5\n",
      "Epoch 2/100\n",
      " - 2s - loss: 0.0044 - val_loss: 0.3475\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.34686\n",
      "Epoch 3/100\n",
      " - 2s - loss: 0.0046 - val_loss: 0.3446\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.34686 to 0.34457, saving model to ./best_weights_relu_2l.hdf5\n",
      "Epoch 4/100\n",
      " - 2s - loss: 0.0054 - val_loss: 0.3467\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.34457\n",
      "Epoch 5/100\n",
      " - 2s - loss: 0.0054 - val_loss: 0.3483\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.34457\n",
      "Epoch 6/100\n",
      " - 2s - loss: 0.0052 - val_loss: 0.3460\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.34457\n",
      "Epoch 00006: early stopping\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    model_reg_relu.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    model_reg_relu.fit(x_train_reg,y_train_reg,validation_data=(x_test_reg,y_test_reg),callbacks=[monitor,checkpointer_relu_2l],verbose=2,epochs=100) \n",
    "\n",
    "model_reg_relu.load_weights('./best_weights_relu_2l.hdf5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict stars\n",
    "pred_reg_hl = model_reg_relu.predict(x_test_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Business ID: diaiQrxYFU1V5qxrFnW9fg, Actual Stars: 4.0, predicted Stars: [3.9236302]\n",
      "2. Business ID: TDTASGFy_aGp6vy0i23mDA, Actual Stars: 4.0, predicted Stars: [4.3126464]\n",
      "3. Business ID: VuKJ2s_JP8weQ54NfsXJXQ, Actual Stars: 3.0, predicted Stars: [4.5015845]\n",
      "4. Business ID: aGiBg2WKOpXS5-1DRnBiAQ, Actual Stars: 4.5, predicted Stars: [4.907193]\n",
      "5. Business ID: ZMmgFw2P4LWsFXNn1ZGc1g, Actual Stars: 3.0, predicted Stars: [3.232567]\n",
      "6. Business ID: sEKFq5u8P_s0-2mAZnx0JQ, Actual Stars: 4.0, predicted Stars: [4.096953]\n",
      "7. Business ID: rYziPPEILDXJ_F5uKR--YQ, Actual Stars: 3.5, predicted Stars: [3.5040715]\n",
      "8. Business ID: Swm_uMOWNcJDZz5lXWyzKA, Actual Stars: 5.0, predicted Stars: [4.789931]\n",
      "9. Business ID: 6nGnVP7M4qQRiclXxeqXSQ, Actual Stars: 4.5, predicted Stars: [4.587507]\n",
      "10. Business ID: Tc24GX9-ZPr4_SHU0nJZZA, Actual Stars: 2.5, predicted Stars: [2.1313505]\n"
     ]
    }
   ],
   "source": [
    "#Display 10 business\n",
    "\n",
    "for i in range(10):\n",
    "    print(\"{}. Business ID: {}, Actual Stars: {}, predicted Stars: {}\".format(i+1,merge_df['business_id'][2000+i],y_test_reg[i],pred_reg_hl[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final score (RMSE): 0.5870042443275452\n",
      "R2 score: 0.68\n"
     ]
    }
   ],
   "source": [
    "# Measure RMSE error.  RMSE is common for regression.\n",
    "score_2l = np.sqrt(mean_squared_error(y_test_reg,pred_reg_hl))\n",
    "print(\"Final score (RMSE): {}\".format(score_2l))\n",
    "print('R2 score: %.2f' % r2_score(y_test_reg, pred_reg_hl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up checkpointer\n",
    "checkpointer_relu_3l = ModelCheckpoint(filepath=\"./best_weights_relu_3l.hdf5\", verbose=2, save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensorflow model for Regression with relu and forward approach with 3 hidden layers\n",
    "model_reg_relu = Sequential()\n",
    "\n",
    "model_reg_relu.add(Dense(60, input_dim=x_train_reg.shape[1], activation='relu'))  \n",
    "model_reg_relu.add(Dense(30, activation='relu')) # Hidden 2\n",
    "model_reg_relu.add(Dense(10, activation='relu')) # Hidden 3\n",
    "model_reg_relu.add(Dense(1)) # Output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/100\n",
      " - 4s - loss: 2.4100 - val_loss: 0.4742\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.47422, saving model to ./best_weights_relu_3l.hdf5\n",
      "Epoch 2/100\n",
      " - 2s - loss: 0.3261 - val_loss: 0.3868\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.47422 to 0.38682, saving model to ./best_weights_relu_3l.hdf5\n",
      "Epoch 3/100\n",
      " - 2s - loss: 0.2102 - val_loss: 0.3775\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.38682 to 0.37753, saving model to ./best_weights_relu_3l.hdf5\n",
      "Epoch 4/100\n",
      " - 2s - loss: 0.1610 - val_loss: 0.3803\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.37753\n",
      "Epoch 5/100\n",
      " - 2s - loss: 0.1255 - val_loss: 0.3798\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.37753\n",
      "Epoch 6/100\n",
      " - 2s - loss: 0.0991 - val_loss: 0.3914\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.37753\n",
      "Epoch 00006: early stopping\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/100\n",
      " - 4s - loss: 0.0865 - val_loss: 0.3938\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.37753\n",
      "Epoch 2/100\n",
      " - 2s - loss: 0.0709 - val_loss: 0.3922\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.37753\n",
      "Epoch 3/100\n",
      " - 2s - loss: 0.0547 - val_loss: 0.3966\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.37753\n",
      "Epoch 4/100\n",
      " - 2s - loss: 0.0431 - val_loss: 0.3857\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.37753\n",
      "Epoch 5/100\n",
      " - 2s - loss: 0.0340 - val_loss: 0.3903\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.37753\n",
      "Epoch 6/100\n",
      " - 2s - loss: 0.0292 - val_loss: 0.3804\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.37753\n",
      "Epoch 7/100\n",
      " - 2s - loss: 0.0263 - val_loss: 0.3863\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.37753\n",
      "Epoch 8/100\n",
      " - 2s - loss: 0.0241 - val_loss: 0.3932\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.37753\n",
      "Epoch 9/100\n",
      " - 2s - loss: 0.0219 - val_loss: 0.3919\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.37753\n",
      "Epoch 00009: early stopping\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/100\n",
      " - 5s - loss: 0.0230 - val_loss: 0.3851\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.37753\n",
      "Epoch 2/100\n",
      " - 2s - loss: 0.0228 - val_loss: 0.3819\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.37753\n",
      "Epoch 3/100\n",
      " - 2s - loss: 0.0214 - val_loss: 0.3802\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.37753\n",
      "Epoch 4/100\n",
      " - 2s - loss: 0.0181 - val_loss: 0.3773\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.37753 to 0.37734, saving model to ./best_weights_relu_3l.hdf5\n",
      "Epoch 5/100\n",
      " - 2s - loss: 0.0168 - val_loss: 0.3829\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.37734\n",
      "Epoch 6/100\n",
      " - 2s - loss: 0.0151 - val_loss: 0.3790\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.37734\n",
      "Epoch 7/100\n",
      " - 2s - loss: 0.0133 - val_loss: 0.3746\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.37734 to 0.37459, saving model to ./best_weights_relu_3l.hdf5\n",
      "Epoch 8/100\n",
      " - 2s - loss: 0.0121 - val_loss: 0.3725\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.37459 to 0.37249, saving model to ./best_weights_relu_3l.hdf5\n",
      "Epoch 9/100\n",
      " - 2s - loss: 0.0119 - val_loss: 0.3685\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.37249 to 0.36847, saving model to ./best_weights_relu_3l.hdf5\n",
      "Epoch 10/100\n",
      " - 2s - loss: 0.0118 - val_loss: 0.3713\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.36847\n",
      "Epoch 11/100\n",
      " - 2s - loss: 0.0111 - val_loss: 0.3713\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.36847\n",
      "Epoch 12/100\n",
      " - 2s - loss: 0.0100 - val_loss: 0.3682\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.36847 to 0.36819, saving model to ./best_weights_relu_3l.hdf5\n",
      "Epoch 00012: early stopping\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/100\n",
      " - 5s - loss: 0.0109 - val_loss: 0.3691\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.36819\n",
      "Epoch 2/100\n",
      " - 2s - loss: 0.0130 - val_loss: 0.3713\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.36819\n",
      "Epoch 3/100\n",
      " - 2s - loss: 0.0129 - val_loss: 0.3716\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.36819\n",
      "Epoch 4/100\n",
      " - 2s - loss: 0.0113 - val_loss: 0.3670\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.36819 to 0.36702, saving model to ./best_weights_relu_3l.hdf5\n",
      "Epoch 5/100\n",
      " - 2s - loss: 0.0101 - val_loss: 0.3653\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.36702 to 0.36532, saving model to ./best_weights_relu_3l.hdf5\n",
      "Epoch 6/100\n",
      " - 2s - loss: 0.0093 - val_loss: 0.3658\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.36532\n",
      "Epoch 7/100\n",
      " - 2s - loss: 0.0089 - val_loss: 0.3651\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.36532 to 0.36512, saving model to ./best_weights_relu_3l.hdf5\n",
      "Epoch 8/100\n",
      " - 2s - loss: 0.0084 - val_loss: 0.3620\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.36512 to 0.36198, saving model to ./best_weights_relu_3l.hdf5\n",
      "Epoch 9/100\n",
      " - 2s - loss: 0.0090 - val_loss: 0.3653\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.36198\n",
      "Epoch 10/100\n",
      " - 2s - loss: 0.0085 - val_loss: 0.3605\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.36198 to 0.36054, saving model to ./best_weights_relu_3l.hdf5\n",
      "Epoch 11/100\n",
      " - 2s - loss: 0.0072 - val_loss: 0.3558\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.36054 to 0.35579, saving model to ./best_weights_relu_3l.hdf5\n",
      "Epoch 12/100\n",
      " - 2s - loss: 0.0065 - val_loss: 0.3603\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.35579\n",
      "Epoch 13/100\n",
      " - 2s - loss: 0.0065 - val_loss: 0.3638\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.35579\n",
      "Epoch 14/100\n",
      " - 2s - loss: 0.0071 - val_loss: 0.3631\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.35579\n",
      "Epoch 00014: early stopping\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/100\n",
      " - 6s - loss: 0.0078 - val_loss: 0.3567\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.35579\n",
      "Epoch 2/100\n",
      " - 2s - loss: 0.0077 - val_loss: 0.3605\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.35579\n",
      "Epoch 3/100\n",
      " - 2s - loss: 0.0076 - val_loss: 0.3518\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.35579 to 0.35183, saving model to ./best_weights_relu_3l.hdf5\n",
      "Epoch 4/100\n",
      " - 2s - loss: 0.0076 - val_loss: 0.3603\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.35183\n",
      "Epoch 5/100\n",
      " - 2s - loss: 0.0074 - val_loss: 0.3555\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.35183\n",
      "Epoch 6/100\n",
      " - 2s - loss: 0.0068 - val_loss: 0.3564\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.35183\n",
      "Epoch 00006: early stopping\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/100\n",
      " - 5s - loss: 0.0068 - val_loss: 0.3559\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.35183\n",
      "Epoch 2/100\n",
      " - 2s - loss: 0.0060 - val_loss: 0.3513\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.35183 to 0.35129, saving model to ./best_weights_relu_3l.hdf5\n",
      "Epoch 3/100\n",
      " - 2s - loss: 0.0064 - val_loss: 0.3561\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.35129\n",
      "Epoch 4/100\n",
      " - 2s - loss: 0.0065 - val_loss: 0.3525\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.35129\n",
      "Epoch 5/100\n",
      " - 2s - loss: 0.0062 - val_loss: 0.3538\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.35129\n",
      "Epoch 00005: early stopping\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/100\n",
      " - 5s - loss: 0.0061 - val_loss: 0.3532\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.35129\n",
      "Epoch 2/100\n",
      " - 2s - loss: 0.0053 - val_loss: 0.3511\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.35129 to 0.35113, saving model to ./best_weights_relu_3l.hdf5\n",
      "Epoch 3/100\n",
      " - 2s - loss: 0.0058 - val_loss: 0.3524\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.35113\n",
      "Epoch 4/100\n",
      " - 2s - loss: 0.0059 - val_loss: 0.3499\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.35113 to 0.34993, saving model to ./best_weights_relu_3l.hdf5\n",
      "Epoch 5/100\n",
      " - 2s - loss: 0.0058 - val_loss: 0.3547\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.34993\n",
      "Epoch 6/100\n",
      " - 2s - loss: 0.0058 - val_loss: 0.3523\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.34993\n",
      "Epoch 7/100\n",
      " - 2s - loss: 0.0056 - val_loss: 0.3492\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.34993 to 0.34919, saving model to ./best_weights_relu_3l.hdf5\n",
      "Epoch 00007: early stopping\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/100\n",
      " - 5s - loss: 0.0051 - val_loss: 0.3483\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.34919 to 0.34829, saving model to ./best_weights_relu_3l.hdf5\n",
      "Epoch 2/100\n",
      " - 2s - loss: 0.0043 - val_loss: 0.3517\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.34829\n",
      "Epoch 3/100\n",
      " - 2s - loss: 0.0048 - val_loss: 0.3477\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.34829 to 0.34767, saving model to ./best_weights_relu_3l.hdf5\n",
      "Epoch 4/100\n",
      " - 2s - loss: 0.0055 - val_loss: 0.3491\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.34767\n",
      "Epoch 00004: early stopping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/100\n",
      " - 5s - loss: 0.0052 - val_loss: 0.3493\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.34767\n",
      "Epoch 2/100\n",
      " - 2s - loss: 0.0043 - val_loss: 0.3466\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.34767 to 0.34662, saving model to ./best_weights_relu_3l.hdf5\n",
      "Epoch 3/100\n",
      " - 2s - loss: 0.0043 - val_loss: 0.3453\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.34662 to 0.34526, saving model to ./best_weights_relu_3l.hdf5\n",
      "Epoch 4/100\n",
      " - 2s - loss: 0.0047 - val_loss: 0.3451\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.34526 to 0.34511, saving model to ./best_weights_relu_3l.hdf5\n",
      "Epoch 5/100\n",
      " - 2s - loss: 0.0048 - val_loss: 0.3470\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.34511\n",
      "Epoch 6/100\n",
      " - 2s - loss: 0.0046 - val_loss: 0.3449\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.34511 to 0.34490, saving model to ./best_weights_relu_3l.hdf5\n",
      "Epoch 00006: early stopping\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/100\n",
      " - 5s - loss: 0.0044 - val_loss: 0.3481\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.34490\n",
      "Epoch 2/100\n",
      " - 2s - loss: 0.0040 - val_loss: 0.3483\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.34490\n",
      "Epoch 3/100\n",
      " - 2s - loss: 0.0043 - val_loss: 0.3434\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.34490 to 0.34340, saving model to ./best_weights_relu_3l.hdf5\n",
      "Epoch 4/100\n",
      " - 2s - loss: 0.0047 - val_loss: 0.3465\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.34340\n",
      "Epoch 5/100\n",
      " - 2s - loss: 0.0042 - val_loss: 0.3462\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.34340\n",
      "Epoch 6/100\n",
      " - 2s - loss: 0.0041 - val_loss: 0.3473\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.34340\n",
      "Epoch 00006: early stopping\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    model_reg_relu.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    model_reg_relu.fit(x_train_reg,y_train_reg,validation_data=(x_test_reg,y_test_reg),callbacks=[monitor,checkpointer_relu_3l],verbose=2,epochs=100) \n",
    "\n",
    "model_reg_relu.load_weights('./best_weights_relu_3l.hdf5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict stars\n",
    "pred_reg_hl_3 = model_reg_relu.predict(x_test_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Business ID: diaiQrxYFU1V5qxrFnW9fg, Actual Stars: 4.0, predicted Stars: [3.7569768]\n",
      "2. Business ID: TDTASGFy_aGp6vy0i23mDA, Actual Stars: 4.0, predicted Stars: [3.7612178]\n",
      "3. Business ID: VuKJ2s_JP8weQ54NfsXJXQ, Actual Stars: 3.0, predicted Stars: [4.3311234]\n",
      "4. Business ID: aGiBg2WKOpXS5-1DRnBiAQ, Actual Stars: 4.5, predicted Stars: [4.388369]\n",
      "5. Business ID: ZMmgFw2P4LWsFXNn1ZGc1g, Actual Stars: 3.0, predicted Stars: [3.1347184]\n",
      "6. Business ID: sEKFq5u8P_s0-2mAZnx0JQ, Actual Stars: 4.0, predicted Stars: [3.725758]\n",
      "7. Business ID: rYziPPEILDXJ_F5uKR--YQ, Actual Stars: 3.5, predicted Stars: [3.4403157]\n",
      "8. Business ID: Swm_uMOWNcJDZz5lXWyzKA, Actual Stars: 5.0, predicted Stars: [4.2882605]\n",
      "9. Business ID: 6nGnVP7M4qQRiclXxeqXSQ, Actual Stars: 4.5, predicted Stars: [4.796396]\n",
      "10. Business ID: Tc24GX9-ZPr4_SHU0nJZZA, Actual Stars: 2.5, predicted Stars: [2.1163046]\n"
     ]
    }
   ],
   "source": [
    "#Display 10 business\n",
    "\n",
    "for i in range(10):\n",
    "    print(\"{}. Business ID: {}, Actual Stars: {}, predicted Stars: {}\".format(i+1,merge_df['business_id'][2000+i],y_test_reg[i],pred_reg_hl_3[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final score (RMSE): 0.5860057473182678\n",
      "R2 score: 0.68\n"
     ]
    }
   ],
   "source": [
    "# Measure RMSE error.  RMSE is common for regression.\n",
    "score_3l = np.sqrt(mean_squared_error(y_test_reg,pred_reg_hl_3))\n",
    "print(\"Final score (RMSE): {}\".format(score_3l))\n",
    "print('R2 score: %.2f' % r2_score(y_test_reg, pred_reg_hl_3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up checkpointer\n",
    "checkpointer_relu_4l = ModelCheckpoint(filepath=\"./best_weights_relu_4l.hdf5\", verbose=2, save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensorflow model for Regression with sigmoid and forward approach with 4 hidden layers\n",
    "model_reg_relu = Sequential()\n",
    "\n",
    "model_reg_relu.add(Dense(80, input_dim=x_train_reg.shape[1], activation='relu'))  \n",
    "model_reg_relu.add(Dense(60, activation='relu')) # Hidden 2\n",
    "model_reg_relu.add(Dense(20, activation='relu')) # Hidden 3\n",
    "model_reg_relu.add(Dense(10, activation='relu')) # Hidden 4\n",
    "model_reg_relu.add(Dense(1)) # Output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/100\n",
      " - 5s - loss: 1.7569 - val_loss: 0.4450\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.44499, saving model to ./best_weights_relu_4l.hdf5\n",
      "Epoch 2/100\n",
      " - 2s - loss: 0.2967 - val_loss: 0.3966\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.44499 to 0.39661, saving model to ./best_weights_relu_4l.hdf5\n",
      "Epoch 3/100\n",
      " - 2s - loss: 0.1988 - val_loss: 0.3840\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.39661 to 0.38403, saving model to ./best_weights_relu_4l.hdf5\n",
      "Epoch 4/100\n",
      " - 2s - loss: 0.1512 - val_loss: 0.3790\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.38403 to 0.37897, saving model to ./best_weights_relu_4l.hdf5\n",
      "Epoch 5/100\n",
      " - 2s - loss: 0.1196 - val_loss: 0.3841\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.37897\n",
      "Epoch 6/100\n",
      " - 2s - loss: 0.0952 - val_loss: 0.3924\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.37897\n",
      "Epoch 7/100\n",
      " - 2s - loss: 0.0712 - val_loss: 0.3832\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.37897\n",
      "Epoch 00007: early stopping\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/100\n",
      " - 5s - loss: 0.0666 - val_loss: 0.3998\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.37897\n",
      "Epoch 2/100\n",
      " - 2s - loss: 0.0523 - val_loss: 0.3893\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.37897\n",
      "Epoch 3/100\n",
      " - 2s - loss: 0.0436 - val_loss: 0.3967\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.37897\n",
      "Epoch 4/100\n",
      " - 2s - loss: 0.0354 - val_loss: 0.3900\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.37897\n",
      "Epoch 5/100\n",
      " - 2s - loss: 0.0296 - val_loss: 0.3827\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.37897\n",
      "Epoch 6/100\n",
      " - 2s - loss: 0.0285 - val_loss: 0.3878\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.37897\n",
      "Epoch 7/100\n",
      " - 2s - loss: 0.0245 - val_loss: 0.3827\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.37897\n",
      "Epoch 8/100\n",
      " - 2s - loss: 0.0206 - val_loss: 0.3791\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.37897\n",
      "Epoch 9/100\n",
      " - 2s - loss: 0.0186 - val_loss: 0.3786\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.37897 to 0.37856, saving model to ./best_weights_relu_4l.hdf5\n",
      "Epoch 10/100\n",
      " - 2s - loss: 0.0173 - val_loss: 0.3828\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.37856\n",
      "Epoch 11/100\n",
      " - 2s - loss: 0.0160 - val_loss: 0.3820\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.37856\n",
      "Epoch 00011: early stopping\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/100\n",
      " - 5s - loss: 0.0170 - val_loss: 0.3789\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.37856\n",
      "Epoch 2/100\n",
      " - 2s - loss: 0.0206 - val_loss: 0.3782\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.37856 to 0.37818, saving model to ./best_weights_relu_4l.hdf5\n",
      "Epoch 3/100\n",
      " - 2s - loss: 0.0201 - val_loss: 0.3793\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.37818\n",
      "Epoch 4/100\n",
      " - 2s - loss: 0.0167 - val_loss: 0.3707\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.37818 to 0.37072, saving model to ./best_weights_relu_4l.hdf5\n",
      "Epoch 5/100\n",
      " - 2s - loss: 0.0148 - val_loss: 0.3720\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.37072\n",
      "Epoch 6/100\n",
      " - 2s - loss: 0.0127 - val_loss: 0.3691\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.37072 to 0.36914, saving model to ./best_weights_relu_4l.hdf5\n",
      "Epoch 7/100\n",
      " - 2s - loss: 0.0122 - val_loss: 0.3674\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.36914 to 0.36737, saving model to ./best_weights_relu_4l.hdf5\n",
      "Epoch 8/100\n",
      " - 2s - loss: 0.0122 - val_loss: 0.3713\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.36737\n",
      "Epoch 9/100\n",
      " - 2s - loss: 0.0113 - val_loss: 0.3757\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.36737\n",
      "Epoch 10/100\n",
      " - 2s - loss: 0.0108 - val_loss: 0.3703\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.36737\n",
      "Epoch 00010: early stopping\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/100\n",
      " - 5s - loss: 0.0112 - val_loss: 0.3647\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.36737 to 0.36467, saving model to ./best_weights_relu_4l.hdf5\n",
      "Epoch 2/100\n",
      " - 2s - loss: 0.0113 - val_loss: 0.3689\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.36467\n",
      "Epoch 3/100\n",
      " - 2s - loss: 0.0121 - val_loss: 0.3703\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.36467\n",
      "Epoch 4/100\n",
      " - 2s - loss: 0.0119 - val_loss: 0.3680\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.36467\n",
      "Epoch 00004: early stopping\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/100\n",
      " - 5s - loss: 0.0110 - val_loss: 0.3681\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.36467\n",
      "Epoch 2/100\n",
      " - 2s - loss: 0.0096 - val_loss: 0.3620\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.36467 to 0.36197, saving model to ./best_weights_relu_4l.hdf5\n",
      "Epoch 3/100\n",
      " - 2s - loss: 0.0096 - val_loss: 0.3620\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.36197 to 0.36196, saving model to ./best_weights_relu_4l.hdf5\n",
      "Epoch 4/100\n",
      " - 2s - loss: 0.0092 - val_loss: 0.3622\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.36196\n",
      "Epoch 5/100\n",
      " - 2s - loss: 0.0089 - val_loss: 0.3588\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.36196 to 0.35885, saving model to ./best_weights_relu_4l.hdf5\n",
      "Epoch 6/100\n",
      " - 2s - loss: 0.0086 - val_loss: 0.3652\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.35885\n",
      "Epoch 7/100\n",
      " - 2s - loss: 0.0084 - val_loss: 0.3610\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.35885\n",
      "Epoch 8/100\n",
      " - 2s - loss: 0.0076 - val_loss: 0.3629\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.35885\n",
      "Epoch 00008: early stopping\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/100\n",
      " - 5s - loss: 0.0075 - val_loss: 0.3607\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.35885\n",
      "Epoch 2/100\n",
      " - 2s - loss: 0.0077 - val_loss: 0.3627\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.35885\n",
      "Epoch 3/100\n",
      " - 2s - loss: 0.0092 - val_loss: 0.3575\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.35885 to 0.35753, saving model to ./best_weights_relu_4l.hdf5\n",
      "Epoch 4/100\n",
      " - 2s - loss: 0.0084 - val_loss: 0.3582\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.35753\n",
      "Epoch 5/100\n",
      " - 2s - loss: 0.0071 - val_loss: 0.3597\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.35753\n",
      "Epoch 6/100\n",
      " - 2s - loss: 0.0064 - val_loss: 0.3599\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.35753\n",
      "Epoch 00006: early stopping\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/100\n",
      " - 6s - loss: 0.0067 - val_loss: 0.3588\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.35753\n",
      "Epoch 2/100\n",
      " - 2s - loss: 0.0059 - val_loss: 0.3594\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.35753\n",
      "Epoch 3/100\n",
      " - 2s - loss: 0.0074 - val_loss: 0.3573\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.35753 to 0.35734, saving model to ./best_weights_relu_4l.hdf5\n",
      "Epoch 4/100\n",
      " - 2s - loss: 0.0077 - val_loss: 0.3582\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.35734\n",
      "Epoch 5/100\n",
      " - 2s - loss: 0.0064 - val_loss: 0.3579\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.35734\n",
      "Epoch 6/100\n",
      " - 2s - loss: 0.0057 - val_loss: 0.3574\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.35734\n",
      "Epoch 00006: early stopping\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/100\n",
      " - 6s - loss: 0.0060 - val_loss: 0.3578\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.35734\n",
      "Epoch 2/100\n",
      " - 2s - loss: 0.0059 - val_loss: 0.3535\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.35734 to 0.35350, saving model to ./best_weights_relu_4l.hdf5\n",
      "Epoch 3/100\n",
      " - 2s - loss: 0.0062 - val_loss: 0.3511\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.35350 to 0.35108, saving model to ./best_weights_relu_4l.hdf5\n",
      "Epoch 4/100\n",
      " - 2s - loss: 0.0061 - val_loss: 0.3569\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.35108\n",
      "Epoch 5/100\n",
      " - 2s - loss: 0.0065 - val_loss: 0.3551\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.35108\n",
      "Epoch 6/100\n",
      " - 2s - loss: 0.0065 - val_loss: 0.3547\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.35108\n",
      "Epoch 00006: early stopping\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/100\n",
      " - 6s - loss: 0.0059 - val_loss: 0.3527\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.35108\n",
      "Epoch 2/100\n",
      " - 2s - loss: 0.0046 - val_loss: 0.3553\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.35108\n",
      "Epoch 3/100\n",
      " - 2s - loss: 0.0050 - val_loss: 0.3539\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.35108\n",
      "Epoch 4/100\n",
      " - 2s - loss: 0.0053 - val_loss: 0.3505\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.35108 to 0.35052, saving model to ./best_weights_relu_4l.hdf5\n",
      "Epoch 5/100\n",
      " - 2s - loss: 0.0056 - val_loss: 0.3531\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.35052\n",
      "Epoch 6/100\n",
      " - 2s - loss: 0.0052 - val_loss: 0.3532\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.35052\n",
      "Epoch 7/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 2s - loss: 0.0046 - val_loss: 0.3526\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.35052\n",
      "Epoch 00007: early stopping\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/100\n",
      " - 6s - loss: 0.0047 - val_loss: 0.3503\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.35052 to 0.35025, saving model to ./best_weights_relu_4l.hdf5\n",
      "Epoch 2/100\n",
      " - 2s - loss: 0.0041 - val_loss: 0.3489\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.35025 to 0.34892, saving model to ./best_weights_relu_4l.hdf5\n",
      "Epoch 3/100\n",
      " - 2s - loss: 0.0047 - val_loss: 0.3503\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.34892\n",
      "Epoch 4/100\n",
      " - 2s - loss: 0.0046 - val_loss: 0.3475\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.34892 to 0.34753, saving model to ./best_weights_relu_4l.hdf5\n",
      "Epoch 5/100\n",
      " - 2s - loss: 0.0045 - val_loss: 0.3584\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.34753\n",
      "Epoch 6/100\n",
      " - 2s - loss: 0.0047 - val_loss: 0.3473\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.34753 to 0.34725, saving model to ./best_weights_relu_4l.hdf5\n",
      "Epoch 7/100\n",
      " - 2s - loss: 0.0048 - val_loss: 0.3485\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.34725\n",
      "Epoch 00007: early stopping\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    model_reg_relu.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    model_reg_relu.fit(x_train_reg,y_train_reg,validation_data=(x_test_reg,y_test_reg),callbacks=[monitor,checkpointer_relu_4l],verbose=2,epochs=100) \n",
    "\n",
    "model_reg_relu.load_weights('./best_weights_relu_4l.hdf5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict stars\n",
    "pred_reg_hl_4 = model_reg_relu.predict(x_test_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Business ID: diaiQrxYFU1V5qxrFnW9fg, Actual Stars: 4.0, predicted Stars: [3.6036444]\n",
      "2. Business ID: TDTASGFy_aGp6vy0i23mDA, Actual Stars: 4.0, predicted Stars: [4.3472104]\n",
      "3. Business ID: VuKJ2s_JP8weQ54NfsXJXQ, Actual Stars: 3.0, predicted Stars: [4.337791]\n",
      "4. Business ID: aGiBg2WKOpXS5-1DRnBiAQ, Actual Stars: 4.5, predicted Stars: [4.4997587]\n",
      "5. Business ID: ZMmgFw2P4LWsFXNn1ZGc1g, Actual Stars: 3.0, predicted Stars: [3.232984]\n",
      "6. Business ID: sEKFq5u8P_s0-2mAZnx0JQ, Actual Stars: 4.0, predicted Stars: [3.5531738]\n",
      "7. Business ID: rYziPPEILDXJ_F5uKR--YQ, Actual Stars: 3.5, predicted Stars: [3.5049272]\n",
      "8. Business ID: Swm_uMOWNcJDZz5lXWyzKA, Actual Stars: 5.0, predicted Stars: [4.572664]\n",
      "9. Business ID: 6nGnVP7M4qQRiclXxeqXSQ, Actual Stars: 4.5, predicted Stars: [4.2858105]\n",
      "10. Business ID: Tc24GX9-ZPr4_SHU0nJZZA, Actual Stars: 2.5, predicted Stars: [2.1794796]\n"
     ]
    }
   ],
   "source": [
    "#Display 10 business\n",
    "\n",
    "for i in range(10):\n",
    "    print(\"{}. Business ID: {}, Actual Stars: {}, predicted Stars: {}\".format(i+1,merge_df['business_id'][2000+i],y_test_reg[i],pred_reg_hl_4[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final score (RMSE): 0.5892806649208069\n",
      "R2 score: 0.68\n"
     ]
    }
   ],
   "source": [
    "# Measure RMSE error.  RMSE is common for regression.\n",
    "score_4l = np.sqrt(mean_squared_error(y_test_reg,pred_reg_hl_4))\n",
    "print(\"Final score (RMSE): {}\".format(score_4l))\n",
    "print('R2 score: %.2f' % r2_score(y_test_reg, pred_reg_hl_4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up checkpointer\n",
    "checkpointer_relu_5l = ModelCheckpoint(filepath=\"./best_weights_relu_5l.hdf5\", verbose=2, save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensorflow model for Regression with sigmoid and forward approach with 5 hidden layers\n",
    "model_reg_relu = Sequential()\n",
    "\n",
    "model_reg_relu.add(Dense(80, input_dim=x_train_reg.shape[1], activation='relu'))  \n",
    "model_reg_relu.add(Dense(60, activation='relu')) # Hidden 2\n",
    "model_reg_relu.add(Dense(40, activation='relu')) # Hidden 2\n",
    "model_reg_relu.add(Dense(20, activation='relu')) # Hidden 2\n",
    "model_reg_relu.add(Dense(10, activation='relu')) # Hidden 2\n",
    "model_reg_relu.add(Dense(1)) # Output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/100\n",
      " - 6s - loss: 2.1387 - val_loss: 0.4359\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.43586, saving model to ./best_weights_relu_5l.hdf5\n",
      "Epoch 2/100\n",
      " - 2s - loss: 0.2866 - val_loss: 0.3779\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.43586 to 0.37790, saving model to ./best_weights_relu_5l.hdf5\n",
      "Epoch 3/100\n",
      " - 2s - loss: 0.1925 - val_loss: 0.3705\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.37790 to 0.37049, saving model to ./best_weights_relu_5l.hdf5\n",
      "Epoch 4/100\n",
      " - 2s - loss: 0.1450 - val_loss: 0.3774\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.37049\n",
      "Epoch 5/100\n",
      " - 2s - loss: 0.1101 - val_loss: 0.3802\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.37049\n",
      "Epoch 6/100\n",
      " - 2s - loss: 0.0852 - val_loss: 0.3891\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.37049\n",
      "Epoch 00006: early stopping\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/100\n",
      " - 6s - loss: 0.0798 - val_loss: 0.3892\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.37049\n",
      "Epoch 2/100\n",
      " - 2s - loss: 0.0657 - val_loss: 0.3881\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.37049\n",
      "Epoch 3/100\n",
      " - 2s - loss: 0.0477 - val_loss: 0.3806\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.37049\n",
      "Epoch 4/100\n",
      " - 2s - loss: 0.0369 - val_loss: 0.3916\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.37049\n",
      "Epoch 5/100\n",
      " - 2s - loss: 0.0322 - val_loss: 0.3812\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.37049\n",
      "Epoch 6/100\n",
      " - 2s - loss: 0.0263 - val_loss: 0.3773\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.37049\n",
      "Epoch 7/100\n",
      " - 2s - loss: 0.0237 - val_loss: 0.3772\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.37049\n",
      "Epoch 8/100\n",
      " - 2s - loss: 0.0208 - val_loss: 0.3700\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.37049 to 0.37004, saving model to ./best_weights_relu_5l.hdf5\n",
      "Epoch 9/100\n",
      " - 2s - loss: 0.0189 - val_loss: 0.3727\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.37004\n",
      "Epoch 10/100\n",
      " - 2s - loss: 0.0173 - val_loss: 0.3729\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.37004\n",
      "Epoch 11/100\n",
      " - 2s - loss: 0.0165 - val_loss: 0.3761\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.37004\n",
      "Epoch 00011: early stopping\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/100\n",
      " - 7s - loss: 0.0181 - val_loss: 0.3722\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.37004\n",
      "Epoch 2/100\n",
      " - 2s - loss: 0.0200 - val_loss: 0.3703\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.37004\n",
      "Epoch 3/100\n",
      " - 2s - loss: 0.0200 - val_loss: 0.3606\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.37004 to 0.36065, saving model to ./best_weights_relu_5l.hdf5\n",
      "Epoch 4/100\n",
      " - 2s - loss: 0.0184 - val_loss: 0.3608\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.36065\n",
      "Epoch 5/100\n",
      " - 2s - loss: 0.0149 - val_loss: 0.3609\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.36065\n",
      "Epoch 6/100\n",
      " - 2s - loss: 0.0124 - val_loss: 0.3562\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.36065 to 0.35624, saving model to ./best_weights_relu_5l.hdf5\n",
      "Epoch 7/100\n",
      " - 2s - loss: 0.0105 - val_loss: 0.3574\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.35624\n",
      "Epoch 8/100\n",
      " - 2s - loss: 0.0098 - val_loss: 0.3536\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.35624 to 0.35361, saving model to ./best_weights_relu_5l.hdf5\n",
      "Epoch 9/100\n",
      " - 2s - loss: 0.0109 - val_loss: 0.3545\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.35361\n",
      "Epoch 10/100\n",
      " - 2s - loss: 0.0120 - val_loss: 0.3564\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.35361\n",
      "Epoch 11/100\n",
      " - 2s - loss: 0.0117 - val_loss: 0.3528\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.35361 to 0.35275, saving model to ./best_weights_relu_5l.hdf5\n",
      "Epoch 00011: early stopping\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/100\n",
      " - 6s - loss: 0.0116 - val_loss: 0.3517\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.35275 to 0.35173, saving model to ./best_weights_relu_5l.hdf5\n",
      "Epoch 2/100\n",
      " - 2s - loss: 0.0113 - val_loss: 0.3480\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.35173 to 0.34804, saving model to ./best_weights_relu_5l.hdf5\n",
      "Epoch 3/100\n",
      " - 2s - loss: 0.0113 - val_loss: 0.3541\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.34804\n",
      "Epoch 4/100\n",
      " - 2s - loss: 0.0110 - val_loss: 0.3448\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.34804 to 0.34485, saving model to ./best_weights_relu_5l.hdf5\n",
      "Epoch 5/100\n",
      " - 2s - loss: 0.0094 - val_loss: 0.3477\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.34485\n",
      "Epoch 6/100\n",
      " - 2s - loss: 0.0091 - val_loss: 0.3481\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.34485\n",
      "Epoch 7/100\n",
      " - 2s - loss: 0.0089 - val_loss: 0.3440\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.34485 to 0.34396, saving model to ./best_weights_relu_5l.hdf5\n",
      "Epoch 00007: early stopping\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/100\n",
      " - 6s - loss: 0.0089 - val_loss: 0.3428\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.34396 to 0.34283, saving model to ./best_weights_relu_5l.hdf5\n",
      "Epoch 2/100\n",
      " - 2s - loss: 0.0078 - val_loss: 0.3488\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.34283\n",
      "Epoch 3/100\n",
      " - 2s - loss: 0.0081 - val_loss: 0.3481\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.34283\n",
      "Epoch 4/100\n",
      " - 2s - loss: 0.0098 - val_loss: 0.3415\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.34283 to 0.34154, saving model to ./best_weights_relu_5l.hdf5\n",
      "Epoch 5/100\n",
      " - 2s - loss: 0.0091 - val_loss: 0.3437\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.34154\n",
      "Epoch 6/100\n",
      " - 2s - loss: 0.0074 - val_loss: 0.3392\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.34154 to 0.33918, saving model to ./best_weights_relu_5l.hdf5\n",
      "Epoch 7/100\n",
      " - 2s - loss: 0.0063 - val_loss: 0.3447\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.33918\n",
      "Epoch 8/100\n",
      " - 2s - loss: 0.0062 - val_loss: 0.3396\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.33918\n",
      "Epoch 9/100\n",
      " - 2s - loss: 0.0059 - val_loss: 0.3360\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.33918 to 0.33597, saving model to ./best_weights_relu_5l.hdf5\n",
      "Epoch 10/100\n",
      " - 2s - loss: 0.0062 - val_loss: 0.3423\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.33597\n",
      "Epoch 11/100\n",
      " - 2s - loss: 0.0064 - val_loss: 0.3376\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.33597\n",
      "Epoch 12/100\n",
      " - 2s - loss: 0.0066 - val_loss: 0.3399\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.33597\n",
      "Epoch 00012: early stopping\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/100\n",
      " - 6s - loss: 0.0063 - val_loss: 0.3359\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.33597 to 0.33589, saving model to ./best_weights_relu_5l.hdf5\n",
      "Epoch 2/100\n",
      " - 2s - loss: 0.0053 - val_loss: 0.3393\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.33589\n",
      "Epoch 3/100\n",
      " - 2s - loss: 0.0060 - val_loss: 0.3369\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.33589\n",
      "Epoch 4/100\n",
      " - 2s - loss: 0.0072 - val_loss: 0.3412\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.33589\n",
      "Epoch 00004: early stopping\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/100\n",
      " - 6s - loss: 0.0067 - val_loss: 0.3342\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.33589 to 0.33422, saving model to ./best_weights_relu_5l.hdf5\n",
      "Epoch 2/100\n",
      " - 2s - loss: 0.0050 - val_loss: 0.3402\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.33422\n",
      "Epoch 3/100\n",
      " - 2s - loss: 0.0051 - val_loss: 0.3368\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.33422\n",
      "Epoch 4/100\n",
      " - 2s - loss: 0.0050 - val_loss: 0.3348\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.33422\n",
      "Epoch 00004: early stopping\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/100\n",
      " - 6s - loss: 0.0054 - val_loss: 0.3357\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.33422\n",
      "Epoch 2/100\n",
      " - 2s - loss: 0.0043 - val_loss: 0.3351\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.33422\n",
      "Epoch 3/100\n",
      " - 2s - loss: 0.0050 - val_loss: 0.3365\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.33422\n",
      "Epoch 4/100\n",
      " - 2s - loss: 0.0056 - val_loss: 0.3386\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.33422\n",
      "Epoch 00004: early stopping\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/100\n",
      " - 6s - loss: 0.0051 - val_loss: 0.3325\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.33422 to 0.33248, saving model to ./best_weights_relu_5l.hdf5\n",
      "Epoch 2/100\n",
      " - 2s - loss: 0.0041 - val_loss: 0.3329\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.33248\n",
      "Epoch 3/100\n",
      " - 2s - loss: 0.0046 - val_loss: 0.3350\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.33248\n",
      "Epoch 4/100\n",
      " - 2s - loss: 0.0050 - val_loss: 0.3313\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.33248 to 0.33129, saving model to ./best_weights_relu_5l.hdf5\n",
      "Epoch 5/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 2s - loss: 0.0048 - val_loss: 0.3343\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.33129\n",
      "Epoch 6/100\n",
      " - 2s - loss: 0.0043 - val_loss: 0.3376\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.33129\n",
      "Epoch 7/100\n",
      " - 2s - loss: 0.0039 - val_loss: 0.3325\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.33129\n",
      "Epoch 00007: early stopping\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/100\n",
      " - 6s - loss: 0.0040 - val_loss: 0.3319\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.33129\n",
      "Epoch 2/100\n",
      " - 2s - loss: 0.0036 - val_loss: 0.3311\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.33129 to 0.33112, saving model to ./best_weights_relu_5l.hdf5\n",
      "Epoch 3/100\n",
      " - 2s - loss: 0.0044 - val_loss: 0.3321\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.33112\n",
      "Epoch 4/100\n",
      " - 2s - loss: 0.0047 - val_loss: 0.3359\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.33112\n",
      "Epoch 00004: early stopping\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    model_reg_relu.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    model_reg_relu.fit(x_train_reg,y_train_reg,validation_data=(x_test_reg,y_test_reg),callbacks=[monitor,checkpointer_relu_5l],verbose=2,epochs=100) \n",
    "\n",
    "model_reg_relu.load_weights('./best_weights_relu_5l.hdf5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict stars\n",
    "pred_reg_hl_5 = model_reg_relu.predict(x_test_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Business ID: diaiQrxYFU1V5qxrFnW9fg, Actual Stars: 4.0, predicted Stars: [3.6036444]\n",
      "2. Business ID: TDTASGFy_aGp6vy0i23mDA, Actual Stars: 4.0, predicted Stars: [4.3472104]\n",
      "3. Business ID: VuKJ2s_JP8weQ54NfsXJXQ, Actual Stars: 3.0, predicted Stars: [4.337791]\n",
      "4. Business ID: aGiBg2WKOpXS5-1DRnBiAQ, Actual Stars: 4.5, predicted Stars: [4.4997587]\n",
      "5. Business ID: ZMmgFw2P4LWsFXNn1ZGc1g, Actual Stars: 3.0, predicted Stars: [3.232984]\n",
      "6. Business ID: sEKFq5u8P_s0-2mAZnx0JQ, Actual Stars: 4.0, predicted Stars: [3.5531738]\n",
      "7. Business ID: rYziPPEILDXJ_F5uKR--YQ, Actual Stars: 3.5, predicted Stars: [3.5049272]\n",
      "8. Business ID: Swm_uMOWNcJDZz5lXWyzKA, Actual Stars: 5.0, predicted Stars: [4.572664]\n",
      "9. Business ID: 6nGnVP7M4qQRiclXxeqXSQ, Actual Stars: 4.5, predicted Stars: [4.2858105]\n",
      "10. Business ID: Tc24GX9-ZPr4_SHU0nJZZA, Actual Stars: 2.5, predicted Stars: [2.1794796]\n"
     ]
    }
   ],
   "source": [
    "#Display 10 business\n",
    "\n",
    "for i in range(10):\n",
    "    print(\"{}. Business ID: {}, Actual Stars: {}, predicted Stars: {}\".format(i+1,merge_df['business_id'][2000+i],y_test_reg[i],pred_reg_hl_4[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final score (RMSE): 0.5754275918006897\n",
      "R2 score: 0.69\n"
     ]
    }
   ],
   "source": [
    "# Measure RMSE error.  RMSE is common for regression.\n",
    "score_5l = np.sqrt(mean_squared_error(y_test_reg,pred_reg_hl_5))\n",
    "print(\"Final score (RMSE): {}\".format(score_5l))\n",
    "print('R2 score: %.2f' % r2_score(y_test_reg, pred_reg_hl_5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEVCAYAAAAb/KWvAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAFOZJREFUeJzt3X+QXWd93/H3x1KE+eE2DF6Da8mRICKJYkgxG5NMBoxtkso2kZKCqU2T4ClYQ1o1tJAE0TTu1CEUQ4s7adXEhpCkSYwA8yOLo+BpSNwCKaDF4ZfsCmuMGy9OQQZD+GWE7G//OEfmer3S3tXd9d199v2a2dE95zy7+51H5/nsuc/5cVNVSJLactK4C5AkLT7DXZIaZLhLUoMMd0lqkOEuSQ0y3CWpQYa7JDXIcJekBhnuktSgteP6xaeeempt3LhxXL9eklakj3/84/dU1cR87cYW7hs3bmR6enpcv16SVqQk/3eYdk7LSFKDhgr3JFuTHEhyMMmuY7R5UZJbk+xPcv3ililJWoh5p2WSrAF2Az8JzAD7kkxV1a0DbTYDrwF+oqruTXLaUhUsSZrfMEfu5wAHq+qOqjoM7AG2z2pzBbC7qu4FqKovLm6ZkqSFGCbczwDuGlie6dcNeirw1CQfTvKRJFsXq0BJ0sINc7VM5lg3+xM+1gKbgecC64EPJjmrqr7ykB+U7AB2AJx55pkLLlaSNJxhjtxngA0Dy+uBu+do8ydV9Z2q+hxwgC7sH6KqrquqyaqanJiY9zJNSdIJGibc9wGbk2xKsg64FJia1ea9wHkASU6lm6a5YzELlSQNb95pmao6kmQncBOwBnhrVe1PchUwXVVT/bafSnIrcD/wK1X1paUsXBqXjbv+dNwljN2dr7943CVoHkPdoVpVe4G9s9ZdOfC6gFf2X0tutQ8uB5ak+Yzt8QMaH/84+sdR7fPxA5LUIMNdkhrktIykR5xTg0s/NeiRuyQ1yHCXpAYZ7pLUIMNdkhpkuEtSgwx3SWqQ4S5JDTLcJalBhrskNchwl6QGGe6S1CDDXZIaZLhLUoMMd0lqkOEuSQ0y3CWpQYa7JDXIcJekBhnuktQgw12SGmS4S1KDDHdJatBQ4Z5ka5IDSQ4m2TXH9suTHEryif7rZYtfqiRpWGvna5BkDbAb+ElgBtiXZKqqbp3V9O1VtXMJapQkLdAwR+7nAAer6o6qOgzsAbYvbVmSpFEME+5nAHcNLM/062Z7QZJPJbkhyYZFqU6SdEKGCffMsa5mLb8P2FhVTwf+HPiDOX9QsiPJdJLpQ4cOLaxSSdLQhgn3GWDwSHw9cPdgg6r6UlV9u198M/DMuX5QVV1XVZNVNTkxMXEi9UqShjBMuO8DNifZlGQdcCkwNdggyekDi9uA2xavREnSQs17tUxVHUmyE7gJWAO8tar2J7kKmK6qKeCXkmwDjgBfBi5fwpolSfOYN9wBqmovsHfWuisHXr8GeM3iliZJOlHeoSpJDTLcJalBhrskNchwl6QGGe6S1CDDXZIaZLhLUoMMd0lqkOEuSQ0y3CWpQYa7JDXIcJekBhnuktQgw12SGmS4S1KDDHdJapDhLkkNMtwlqUGGuyQ1yHCXpAYZ7pLUIMNdkhpkuEtSgwx3SWqQ4S5JDTLcJalBhrskNWiocE+yNcmBJAeT7DpOuxcmqSSTi1eiJGmh5g33JGuA3cCFwBbgsiRb5mh3CvBLwEcXu0hJ0sIMc+R+DnCwqu6oqsPAHmD7HO1+A3gDcN8i1idJOgHDhPsZwF0DyzP9ugcleQawoapuPN4PSrIjyXSS6UOHDi24WEnScIYJ98yxrh7cmJwEXAO8ar4fVFXXVdVkVU1OTEwMX6UkaUGGCfcZYMPA8nrg7oHlU4CzgJuT3An8GDDlSVVJGp9hwn0fsDnJpiTrgEuBqaMbq+qrVXVqVW2sqo3AR4BtVTW9JBVLkuY1b7hX1RFgJ3ATcBvwjqran+SqJNuWukBJ0sKtHaZRVe0F9s5ad+Ux2j539LIkSaPwDlVJapDhLkkNMtwlqUGGuyQ1yHCXpAYZ7pLUIMNdkhpkuEtSgwx3SWqQ4S5JDTLcJalBhrskNchwl6QGGe6S1CDDXZIaZLhLUoMMd0lqkOEuSQ0y3CWpQYa7JDXIcJekBhnuktQgw12SGmS4S1KDDHdJapDhLkkNGirck2xNciDJwSS75tj+8iSfTvKJJB9KsmXxS5UkDWvecE+yBtgNXAhsAS6bI7yvr6qnVdU/BN4AvGnRK5UkDW2YI/dzgINVdUdVHQb2ANsHG1TV3w0sPhaoxStRkrRQa4docwZw18DyDPCs2Y2S/AvglcA64Py5flCSHcAOgDPPPHOhtUqShjTMkXvmWPewI/Oq2l1VTwFeDfzbuX5QVV1XVZNVNTkxMbGwSiVJQxsm3GeADQPL64G7j9N+D/AzoxQlSRrNMOG+D9icZFOSdcClwNRggySbBxYvBm5fvBIlSQs175x7VR1JshO4CVgDvLWq9ie5CpiuqilgZ5LnAd8B7gVespRFS5KOb5gTqlTVXmDvrHVXDrx+xSLXJUkagXeoSlKDDHdJapDhLkkNMtwlqUGGuyQ1yHCXpAYZ7pLUIMNdkhpkuEtSgwx3SWqQ4S5JDTLcJalBhrskNchwl6QGGe6S1CDDXZIaZLhLUoMMd0lqkOEuSQ0y3CWpQYa7JDXIcJekBhnuktQgw12SGmS4S1KDDHdJatBQ4Z5ka5IDSQ4m2TXH9lcmuTXJp5J8IMn3LX6pkqRhzRvuSdYAu4ELgS3AZUm2zGr218BkVT0duAF4w2IXKkka3jBH7ucAB6vqjqo6DOwBtg82qKq/rKpv9osfAdYvbpmSpIUYJtzPAO4aWJ7p1x3LS4E/m2tDkh1JppNMHzp0aPgqJUkLMky4Z451NWfD5OeASeCNc22vquuqarKqJicmJoavUpK0IGuHaDMDbBhYXg/cPbtRkucBvwacW1XfXpzyJEknYpgj933A5iSbkqwDLgWmBhskeQZwLbCtqr64+GVKkhZi3nCvqiPATuAm4DbgHVW1P8lVSbb1zd4IPA54Z5JPJJk6xo+TJD0ChpmWoar2Antnrbty4PXzFrkuSdIIvENVkhpkuEtSgwx3SWqQ4S5JDTLcJalBhrskNchwl6QGGe6S1CDDXZIaZLhLUoMMd0lqkOEuSQ0y3CWpQYa7JDXIcJekBhnuktQgw12SGmS4S1KDDHdJapDhLkkNMtwlqUGGuyQ1yHCXpAYZ7pLUIMNdkhpkuEtSg4YK9yRbkxxIcjDJrjm2PyfJLUmOJHnh4pcpSVqIecM9yRpgN3AhsAW4LMmWWc3+BrgcuH6xC5QkLdzaIdqcAxysqjsAkuwBtgO3Hm1QVXf22x5YgholSQs0zLTMGcBdA8sz/boFS7IjyXSS6UOHDp3Ij5AkDWGYcM8c6+pEfllVXVdVk1U1OTExcSI/QpI0hGHCfQbYMLC8Hrh7acqRJC2GYcJ9H7A5yaYk64BLgamlLUuSNIp5w72qjgA7gZuA24B3VNX+JFcl2QaQ5EeTzACXANcm2b+URUuSjm+Yq2Woqr3A3lnrrhx4vY9uukaStAx4h6okNchwl6QGGe6S1CDDXZIaZLhLUoMMd0lqkOEuSQ0y3CWpQYa7JDXIcJekBhnuktQgw12SGmS4S1KDDHdJapDhLkkNMtwlqUGGuyQ1yHCXpAYZ7pLUIMNdkhpkuEtSgwx3SWqQ4S5JDTLcJalBhrskNchwl6QGDRXuSbYmOZDkYJJdc2x/VJK399s/mmTjYhcqSRrevOGeZA2wG7gQ2AJclmTLrGYvBe6tqu8HrgGuXuxCJUnDG+bI/RzgYFXdUVWHgT3A9llttgN/0L++AbggSRavTEnSQgwT7mcAdw0sz/Tr5mxTVUeArwJPWIwCJUkLt3aINnMdgdcJtCHJDmBHv/j1JAeG+P3L0anAPeP65Vn5k1723+jsw9Gs5P77vmEaDRPuM8CGgeX1wN3HaDOTZC3w94Evz/5BVXUdcN0whS1nSaaranLcdaxU9t/o7MPRrIb+G2ZaZh+wOcmmJOuAS4GpWW2mgJf0r18I/EVVPezIXZL0yJj3yL2qjiTZCdwErAHeWlX7k1wFTFfVFPC7wB8mOUh3xH7pUhYtSTq+YaZlqKq9wN5Z664ceH0fcMnilrasrfippTGz/0ZnH46m+f6LsyeS1B4fPyBJDTLcJalBhvsS8Q7d0dh/o7H/RtNC/xnuSyBJqqqS/HSS1yWxnxdgoP8uSvKr465npbH/RtPK+F2RRS93/Y7xfODfA/+zqh4Yd00rSd9/24HfBPaPu56Vxv4bTSvj13BfAklOBs4DfhX4WJILk/y3JD/e38Gr40jyOGAb8PPAzUnOTXJ1ktPHXNqKYP+NppXx66WQiyzJBcCzgH8AfG//74eBs4AvVNXLx1jespfkXGAzXTh9he5RFrcDzwY+WVU7jvPtq579N5qmxm9V+bVIX8BTgPcBpwMnA/8Y+OF+21nA/wJOH3edy/ULeDJwI91DnR4P/Evg7H7bD/d9+/hx17lcv+y/kfuvqfG7Yt5iLHdJzgReR3fX77equ2v33f22i4A3Aruq6m/HV+XylWQz3V2Dd9B98Mv9wH/pt22jmz/+taq6d3xVLl/232haHL/OuY9g8HKpqvob4L10fXp+P+9JkkcDk8C/qqr3jaXQZWpW/91O9/yiHwJ+5Oi2JI8BLgJeXVVTLVyitljsv9G0Pn6dcz9BA5dLbaX7z7+f7iMGXwBcQLej3FxVf5dkbXUfYqLeQP9dQDdlcA/wdrqTWM8ErgI+U1UPJPmeqvrOGMtdduy/0ayG8euR+wnqd4wLgdfSzcVdAvynqvpj4GPAP6U7AlhDt+NoQN9/zwdeD3wReAXwmqr6D8BtwBuAp/XNV9zAWmr232hWw/g13E/AwE0NzwMuozt59U26eTmq6neA/wF8rqruL98ePUSSk/o+/On+6zDwAPDfAarq1+k+R2Btv2z/DbD/RrNaxq/TMicgyeaquj3JNcATgdOAl1fVwSQvBL6nqt423iqXrySbqupzSd5C9xGNTwau6PvvZ4CvVNXNYy1yGbP/RrNaxq9H7guQzqOAqX6u7o+Ai4F39DvGT9C9zVsxZ9Qfaf2Jqj1JzgbeRdd/b+7779l00wmHx1njcmb/nbjVNn69FHIB+rdn307yOuAHq+r9SX4e+K0k5wBnA7/sUdOxVdXXk7wbeEpVvTPJvwFe2w+sZwP/uqr+arxVLl/234lbbePXaZkhJZkEbumvPngG8Bbgn/R/8Z9Md9LlUVX12aNn4sda8DKT5Kyq+kz/+mLgPwIXVNXdSZ5G9y7y/qr6jP33cPbfaFbj+DXc55HkpH6HeBvw9+huRb6e7trhjXQ3hniZ2TH01xKvpTvZNwH8NvAhurv/jlTVm8dY3rJn/41mNY9fw/0YBq6DfWJVfaFfdzbwY8Av0j1tb1NVPWucdS5XA/33vVX1lX7di+hO/v0C3bzmt6rq+eOsc7my/0bj+DXc5zTw1/4iYBfwQeDzwO9V1bf6t8FPpTv5ckN/6Zl6A/13Md1AuoXuaPMvqupI/xb5ucDlwO6q+u2xFbsM2X+jcfx2PKE6IMm6qjrc7xjnAlcDLwJeRXfX2qYkv1FVnwY+neQ2ugEmIMma/rrgB5KcT3eDzaV0V3D8OLA5ye9V1XSSvwY+C/zAGEteVuy/0Th+H8pLIXtJTgVeleTo27Qz6O5S2wA8g+4Gh6cCv57klL7Ns4B/lOSxj3S9y02SJwGXJ9nYrzob+DlgPfAk4E+BrcBLkjymugdbbQZ+NsnJ/dzyqmX/jcbx+3CG+3c9ke5Rnxcm2VJV19MdGb0EeHFVvQu4j26wre+/5xBwSVV9YxwFLzPfT/cM8QuTPB54E90TCn8RuKiq/jPds8Un6W4aAfgS8LKquq+FqxNGZP+NxvE7i9Myvaran+6OvxcDlyV5T1Xdku5RoOcnWUd3m/Irquq2/ntuHGPJy0pVfSjdp9S8jO6g4b3Al+lOAJ6XZJruFu9rqurO/nt+fzzVLj/232gcvw/nCVW+e2a9f/0jdPOc99Nddvak/t/DdAPrXWMrdBk6Oh0w0H/nAf8M+CjwZuA5dG+J7wdeW1XvOfp9Hm3af6M6evJ0YNnx21u14T7HTjE74C+je5retXSXnZ1SVfc6qDqZ9RjUWf13HvBS4H/TXZ99EvC4qvq8/ddJckpVfa1/PXtftP/mkWSiqg4dY5vjl1U6557kB4B/l+TKJE/uB9eD/+FV9Um6Gx0eB/xz4NHVf4LNatkxjifJDwJv6vvvh+DBR6gePQr9S7o7AM+nOwr9RlV9/mi7MZW9bCTZBLw/yQ6A/uqOwQ+OsP+Oo9//Ppxkcq4TyY7fzqqbc+93jHcDvwOcS/cBuK8Avj3Yrqo+leT36W4U+dojXedy1Yf5HwO/Szdl8BS6k1ZHA/7o5Xw3p3u06pdqBX7QwRI7je7k6POTPLaqrhl417O2qo7Yf3Prx+9bgKuranqO7SdV1QOO31U2LZPkZLonwf1VVb2pP4H158DbquragXar5q3bQqR7ot6fADdW1X9N8gS6t7030D0//O7+JpGHTDPou/ojzQm6z+v8I7qrYd4P/CFwclV9fYzlLWv9eP0A8NWq2pbugzR+AVhHt//d1u9/jl9WWbjDg7cg30m3g9yf5NV0f91/a7yVrQxJTq2qe/qBdgtwO91VHd8EPlTdkwodXPNIspvukb1fo7tT8ugz2W/2j+Ox9fPp19MdvT8H+ALwhP7fD1TVe9z/Oqtxzv2TVfXl6m4Cge6zJ0+HbsdJ8qPjK235q6p7+n+PAK+rqhdU1RV0/Xhuv23VD6xjGZgj/irdddefo7vL9Bt0N9lgsB9bP5/+YuCX6d4pvryqLqH7qMHz+zbuf6zCcD8a6gODbB3dM56fDryTVdgnCzVw4nTPwOqPAY9N8ujVfrfk8QwEz410J/v2AdcBVwAX9SdbdRx9wD8T+JWB1fuAx3i37netuhOqRw0Mss8CO4CfAl5VVR8dX1Urw+wjoyQX0F2LvauqvjWeqlac/0M3/l5fVdf2N9lccazL+/RQVfX/jr7u97/X031A+H3jq2p5WXVz7rP10zAfBH62qv5s3PWsJP0Jrc10V85cXVVTzncOb/Bady1cf4S+Hng73R9J978Bhnt3YnBDdR847I6xQH3An1ZVf2v/nRj7bTRJTquqL9qPD7Xqw32QO4ekVhjuktQgrwyRpAYZ7pLUIMNdkhpkuEtSgwx3SWqQ4S5JDfr/DKeY9O6z7GEAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plotting RMSE score for different number of layers for relu model\n",
    "\n",
    "score_list_layers=[score_2l,score_3l,score_4l,score_5l]\n",
    "names =['2 layers','3 layers','4 layers','5 layers']\n",
    "tick_marks = np.arange(len(names))\n",
    "plt.bar(range(len(score_list_layers)), score_list_layers)\n",
    "plt.xticks(tick_marks, names, rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow Model for Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train test data for linear regression\n",
    "\n",
    "x_train_lr, x_test_lr, y_train_lr, y_test_lr = train_test_split(x_matrix_minmax, merge_df['encoded_stars'] , test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "#implementing Nearest Neighbor\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=1)\n",
    "\n",
    "knn.fit(x_train_lr, y_train_lr) \n",
    "\n",
    "y_pred_knn = knn.predict(x_test_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "business id - hTF1Qo6PRFnDgg1rh9a9BQ actual stars label - 6 predicted - 8\n",
      "business id - bAPjkuNJ67j2F4C5HQQHhQ actual stars label - 6 predicted - 4\n",
      "business id - dFcs3q8ynbFEaAnbyGSLjQ actual stars label - 6 predicted - 6\n",
      "business id - jrhc4s5XMR8S8kpGdU08og actual stars label - 7 predicted - 6\n",
      "business id - e7207sqC-pSn6GIf31ikhQ actual stars label - 6 predicted - 6\n",
      "business id - CF9TxeEdP5QxihYFAl4sUg actual stars label - 6 predicted - 6\n",
      "business id - zZPCAFK85NtitSNVP_wfYg actual stars label - 5 predicted - 7\n",
      "business id - 42U4Vlzr7nmQa1Bk8J4flw actual stars label - 8 predicted - 7\n",
      "business id - TTrYd662CZFRPaiwl-sUqA actual stars label - 2 predicted - 4\n",
      "business id - -lBIxCbHxuN3YO_sUkWeUQ actual stars label - 3 predicted - 6\n"
     ]
    }
   ],
   "source": [
    "# list  the business with the stars and prediction\n",
    "\n",
    "for i in range(0,10):\n",
    "    idx=y_test_lr.index[i]\n",
    "    print(\"business id - %s actual stars label - %d predicted - %d\" \n",
    "          %(merge_df['business_id'][idx], y_test_lr[idx], y_pred_knn[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score: 0.295\n",
      "Precision score: 0.2960480875614052\n",
      "Recall score: 0.295\n",
      "F1 score: 0.2918900582503685\n"
     ]
    }
   ],
   "source": [
    "# Metrics\n",
    "\n",
    "score_knn_acc = metrics.accuracy_score(y_test_lr, y_pred_knn)\n",
    "print(\"Accuracy score: {}\".format(score_knn_acc))\n",
    "\n",
    "score_knn_precision = metrics.precision_score(y_test_lr, y_pred_knn, average= \"weighted\")\n",
    "print(\"Precision score: {}\".format(score_knn_precision))\n",
    "\n",
    "score_knn_recall = metrics.recall_score(y_test_lr, y_pred_knn, average= \"weighted\")\n",
    "print(\"Recall score: {}\".format(score_knn_recall))\n",
    "\n",
    "score_knn_f1 = metrics.f1_score(y_test_lr, y_pred_knn, average= \"weighted\")\n",
    "print(\"F1 score: {}\".format(score_knn_f1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnb_model = MultinomialNB()\n",
    "\n",
    "mnb_model.fit(x_train_lr, y_train_lr)\n",
    "\n",
    "y_pred_mnb = mnb_model.predict(x_test_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "business id - hTF1Qo6PRFnDgg1rh9a9BQ actual stars label - 6 predicted - 8\n",
      "business id - bAPjkuNJ67j2F4C5HQQHhQ actual stars label - 6 predicted - 6\n",
      "business id - dFcs3q8ynbFEaAnbyGSLjQ actual stars label - 6 predicted - 6\n",
      "business id - jrhc4s5XMR8S8kpGdU08og actual stars label - 7 predicted - 7\n",
      "business id - e7207sqC-pSn6GIf31ikhQ actual stars label - 6 predicted - 6\n",
      "business id - CF9TxeEdP5QxihYFAl4sUg actual stars label - 6 predicted - 8\n",
      "business id - zZPCAFK85NtitSNVP_wfYg actual stars label - 5 predicted - 8\n",
      "business id - 42U4Vlzr7nmQa1Bk8J4flw actual stars label - 8 predicted - 8\n",
      "business id - TTrYd662CZFRPaiwl-sUqA actual stars label - 2 predicted - 3\n",
      "business id - -lBIxCbHxuN3YO_sUkWeUQ actual stars label - 3 predicted - 0\n"
     ]
    }
   ],
   "source": [
    "# list  the business with the stars and prediction\n",
    "\n",
    "for i in range(0,10):\n",
    "    idx=y_test_lr.index[i]\n",
    "    print(\"business id - %s actual stars label - %d predicted - %d\" \n",
    "          %(merge_df['business_id'][idx], y_test_lr[idx], y_pred_mnb[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score: 0.3325\n",
      "Precision score: 0.29977451126707483\n",
      "Recall score: 0.3325\n",
      "F1 score: 0.2831433707336793\n"
     ]
    }
   ],
   "source": [
    "# Metrics\n",
    "\n",
    "score_mnb_acc = metrics.accuracy_score(y_test_lr, y_pred_mnb)\n",
    "print(\"Accuracy score: {}\".format(score_mnb_acc))\n",
    "\n",
    "score_mnb_precision = metrics.precision_score(y_test_lr, y_pred_mnb, average= \"weighted\")\n",
    "print(\"Precision score: {}\".format(score_mnb_precision))\n",
    "\n",
    "score_mnb_recall = metrics.recall_score(y_test_lr, y_pred_mnb, average= \"weighted\")\n",
    "print(\"Recall score: {}\".format(score_mnb_recall))\n",
    "\n",
    "score_mnb_f1 = metrics.f1_score(y_test_lr, y_pred_mnb, average= \"weighted\")\n",
    "print(\"F1 score: {}\".format(score_mnb_f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-hot cooding of postal codes \n",
    "\n",
    "hotcoded_stars_df = pd.get_dummies(merge_df['encoded_stars'], sparse = 'true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_stars_encoded = hotcoded_stars_df.values.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train test data for \n",
    "\n",
    "x_train_lr, x_test_lr, y_train_lr, y_test_lr = train_test_split(x_matrix_minmax, y_stars_encoded, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      " - 4s - loss: 1.7835\n",
      "Epoch 2/100\n",
      " - 1s - loss: 1.3468\n",
      "Epoch 3/100\n",
      " - 1s - loss: 1.1881\n",
      "Epoch 4/100\n",
      " - 1s - loss: 1.1043\n",
      "Epoch 5/100\n",
      " - 1s - loss: 1.0541\n",
      "Epoch 6/100\n",
      " - 1s - loss: 1.0025\n",
      "Epoch 7/100\n",
      " - 1s - loss: 0.9625\n",
      "Epoch 8/100\n",
      " - 1s - loss: 0.9229\n",
      "Epoch 9/100\n",
      " - 1s - loss: 0.8861\n",
      "Epoch 10/100\n",
      " - 1s - loss: 0.8479\n",
      "Epoch 11/100\n",
      " - 1s - loss: 0.8162\n",
      "Epoch 12/100\n",
      " - 1s - loss: 0.7821\n",
      "Epoch 13/100\n",
      " - 1s - loss: 0.7457\n",
      "Epoch 14/100\n",
      " - 1s - loss: 0.7148\n",
      "Epoch 15/100\n",
      " - 1s - loss: 0.6881\n",
      "Epoch 16/100\n",
      " - 1s - loss: 0.6525\n",
      "Epoch 17/100\n",
      " - 1s - loss: 0.6201\n",
      "Epoch 18/100\n",
      " - 1s - loss: 0.5911\n",
      "Epoch 19/100\n",
      " - 1s - loss: 0.5632\n",
      "Epoch 20/100\n",
      " - 1s - loss: 0.5292\n",
      "Epoch 21/100\n",
      " - 1s - loss: 0.4980\n",
      "Epoch 22/100\n",
      " - 1s - loss: 0.4710\n",
      "Epoch 23/100\n",
      " - 1s - loss: 0.4426\n",
      "Epoch 24/100\n",
      " - 1s - loss: 0.4150\n",
      "Epoch 25/100\n",
      " - 1s - loss: 0.3874\n",
      "Epoch 26/100\n",
      " - 1s - loss: 0.3640\n",
      "Epoch 27/100\n",
      " - 1s - loss: 0.3371\n",
      "Epoch 28/100\n",
      " - 1s - loss: 0.3153\n",
      "Epoch 29/100\n",
      " - 1s - loss: 0.2926\n",
      "Epoch 30/100\n",
      " - 1s - loss: 0.2683\n",
      "Epoch 31/100\n",
      " - 1s - loss: 0.2510\n",
      "Epoch 32/100\n",
      " - 1s - loss: 0.2275\n",
      "Epoch 33/100\n",
      " - 1s - loss: 0.2124\n",
      "Epoch 34/100\n",
      " - 1s - loss: 0.1908\n",
      "Epoch 35/100\n",
      " - 1s - loss: 0.1773\n",
      "Epoch 36/100\n",
      " - 1s - loss: 0.1640\n",
      "Epoch 37/100\n",
      " - 1s - loss: 0.1461\n",
      "Epoch 38/100\n",
      " - 1s - loss: 0.1320\n",
      "Epoch 39/100\n",
      " - 1s - loss: 0.1207\n",
      "Epoch 40/100\n",
      " - 1s - loss: 0.1104\n",
      "Epoch 41/100\n",
      " - 1s - loss: 0.0972\n",
      "Epoch 42/100\n",
      " - 1s - loss: 0.0911\n",
      "Epoch 43/100\n",
      " - 1s - loss: 0.0811\n",
      "Epoch 44/100\n",
      " - 1s - loss: 0.0725\n",
      "Epoch 45/100\n",
      " - 1s - loss: 0.0638\n",
      "Epoch 46/100\n",
      " - 1s - loss: 0.0572\n",
      "Epoch 47/100\n",
      " - 1s - loss: 0.0519\n",
      "Epoch 48/100\n",
      " - 1s - loss: 0.0467\n",
      "Epoch 49/100\n",
      " - 1s - loss: 0.0429\n",
      "Epoch 50/100\n",
      " - 1s - loss: 0.0397\n",
      "Epoch 51/100\n",
      " - 1s - loss: 0.0342\n",
      "Epoch 52/100\n",
      " - 1s - loss: 0.0297\n",
      "Epoch 53/100\n",
      " - 1s - loss: 0.0272\n",
      "Epoch 54/100\n",
      " - 1s - loss: 0.0297\n",
      "Epoch 55/100\n",
      " - 1s - loss: 0.0440\n",
      "Epoch 56/100\n",
      " - 1s - loss: 0.0448\n",
      "Epoch 57/100\n",
      " - 1s - loss: 0.0263\n",
      "Epoch 58/100\n",
      " - 1s - loss: 0.0175\n",
      "Epoch 59/100\n",
      " - 1s - loss: 0.0149\n",
      "Epoch 60/100\n",
      " - 1s - loss: 0.0130\n",
      "Epoch 61/100\n",
      " - 1s - loss: 0.0123\n",
      "Epoch 62/100\n",
      " - 1s - loss: 0.0109\n",
      "Epoch 63/100\n",
      " - 1s - loss: 0.0103\n",
      "Epoch 64/100\n",
      " - 1s - loss: 0.0101\n",
      "Epoch 65/100\n",
      " - 1s - loss: 0.0378\n",
      "Epoch 66/100\n",
      " - 1s - loss: 0.0645\n",
      "Epoch 67/100\n",
      " - 1s - loss: 0.0187\n",
      "Epoch 68/100\n",
      " - 1s - loss: 0.0103\n",
      "Epoch 69/100\n",
      " - 1s - loss: 0.0085\n",
      "Epoch 70/100\n",
      " - 1s - loss: 0.0074\n",
      "Epoch 71/100\n",
      " - 1s - loss: 0.0069\n",
      "Epoch 72/100\n",
      " - 1s - loss: 0.0066\n",
      "Epoch 73/100\n",
      " - 1s - loss: 0.0060\n",
      "Epoch 74/100\n",
      " - 1s - loss: 0.0056\n",
      "Epoch 75/100\n",
      " - 1s - loss: 0.0058\n",
      "Epoch 76/100\n",
      " - 1s - loss: 0.0051\n",
      "Epoch 77/100\n",
      " - 1s - loss: 0.0047\n",
      "Epoch 78/100\n",
      " - 1s - loss: 0.0047\n",
      "Epoch 79/100\n",
      " - 1s - loss: 0.0049\n",
      "Epoch 80/100\n",
      " - 1s - loss: 0.1065\n",
      "Epoch 81/100\n",
      " - 1s - loss: 0.0275\n",
      "Epoch 82/100\n",
      " - 1s - loss: 0.0080\n",
      "Epoch 83/100\n",
      " - 1s - loss: 0.0051\n",
      "Epoch 84/100\n",
      " - 1s - loss: 0.0047\n",
      "Epoch 85/100\n",
      " - 1s - loss: 0.0044\n",
      "Epoch 86/100\n",
      " - 1s - loss: 0.0041\n",
      "Epoch 87/100\n",
      " - 1s - loss: 0.0037\n",
      "Epoch 88/100\n",
      " - 1s - loss: 0.0038\n",
      "Epoch 89/100\n",
      " - 1s - loss: 0.0033\n",
      "Epoch 90/100\n",
      " - 1s - loss: 0.0032\n",
      "Epoch 91/100\n",
      " - 1s - loss: 0.0030\n",
      "Epoch 92/100\n",
      " - 1s - loss: 0.0028\n",
      "Epoch 93/100\n",
      " - 1s - loss: 0.0033\n",
      "Epoch 94/100\n",
      " - 1s - loss: 0.0031\n",
      "Epoch 95/100\n",
      " - 1s - loss: 0.0024\n",
      "Epoch 96/100\n",
      " - 1s - loss: 0.0022\n",
      "Epoch 97/100\n",
      " - 1s - loss: 0.0020\n",
      "Epoch 98/100\n",
      " - 1s - loss: 0.0018\n",
      "Epoch 99/100\n",
      " - 1s - loss: 0.0018\n",
      "Epoch 100/100\n",
      " - 1s - loss: 0.0016\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1b13078c18>"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Tensorflow classification\n",
    "\n",
    "model_class = Sequential()\n",
    "model_class.add(Dense(50, input_dim=x_train_lr.shape[1], activation='relu')) # Hidden 1\n",
    "model_class.add(Dense(25, activation='relu')) # Hidden 2\n",
    "model_class.add(Dense(y_train_lr.shape[1], activation='softmax')) # Output\n",
    "\n",
    "model_class.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "model_class.fit(x_train_lr,y_train_lr,verbose=2,epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (2000, 9)\n",
      "[[0.0000000e+00 9.0550011e-32 1.3659213e-34 ... 9.9981183e-01\n",
      "  1.0377554e-08 5.8346041e-18]\n",
      " [0.0000000e+00 6.4917041e-37 0.0000000e+00 ... 7.1663570e-01\n",
      "  4.5088825e-08 1.3659771e-15]\n",
      " [0.0000000e+00 0.0000000e+00 0.0000000e+00 ... 1.3362568e-07\n",
      "  1.4631912e-02 9.8536795e-01]\n",
      " ...\n",
      " [1.4083814e-15 2.9510001e-02 1.7830956e-01 ... 3.6779843e-13\n",
      "  1.8252636e-18 2.5144965e-23]\n",
      " [3.3521966e-10 2.0965272e-06 7.9351793e-07 ... 3.9346814e-05\n",
      "  4.6281608e-07 2.1384891e-10]\n",
      " [0.0000000e+00 6.0629617e-29 2.2351458e-32 ... 6.8042896e-06\n",
      "  7.2141000e-15 3.6652270e-23]]\n"
     ]
    }
   ],
   "source": [
    "pred_class = model_class.predict(x_test_lr)\n",
    "print(\"Shape: {}\".format(pred_class.shape))\n",
    "print(pred_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_stars = np.argmax(pred_class,axis=1)\n",
    "\n",
    "true_stars = np.argmax(y_test_lr,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Business ID: diaiQrxYFU1V5qxrFnW9fg, Actual Stars: 6, predicted Stars: 6\n",
      "2. Business ID: TDTASGFy_aGp6vy0i23mDA, Actual Stars: 6, predicted Stars: 6\n",
      "3. Business ID: VuKJ2s_JP8weQ54NfsXJXQ, Actual Stars: 6, predicted Stars: 8\n",
      "4. Business ID: aGiBg2WKOpXS5-1DRnBiAQ, Actual Stars: 7, predicted Stars: 7\n",
      "5. Business ID: ZMmgFw2P4LWsFXNn1ZGc1g, Actual Stars: 6, predicted Stars: 6\n",
      "6. Business ID: sEKFq5u8P_s0-2mAZnx0JQ, Actual Stars: 6, predicted Stars: 6\n",
      "7. Business ID: rYziPPEILDXJ_F5uKR--YQ, Actual Stars: 5, predicted Stars: 5\n",
      "8. Business ID: Swm_uMOWNcJDZz5lXWyzKA, Actual Stars: 8, predicted Stars: 7\n",
      "9. Business ID: 6nGnVP7M4qQRiclXxeqXSQ, Actual Stars: 2, predicted Stars: 2\n",
      "10. Business ID: Tc24GX9-ZPr4_SHU0nJZZA, Actual Stars: 3, predicted Stars: 3\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(\"{}. Business ID: {}, Actual Stars: {}, predicted Stars: {}\".format(i+1,merge_df['business_id'][2000+i],true_stars[i],predict_stars[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score: 0.4235\n",
      "Precision score: 0.42908601811601144\n",
      "Recall score: 0.4235\n",
      "F1 score: 0.4256234887594561\n"
     ]
    }
   ],
   "source": [
    "# Metrics\n",
    "\n",
    "score_tf_acc = metrics.accuracy_score(true_stars, predict_stars)\n",
    "print(\"Accuracy score: {}\".format(score_tf_acc))\n",
    "\n",
    "score_tf_precision = metrics.precision_score(true_stars, predict_stars, average= \"weighted\")\n",
    "print(\"Precision score: {}\".format(score_tf_precision))\n",
    "\n",
    "score_tf_recall = metrics.recall_score(true_stars, predict_stars, average= \"weighted\")\n",
    "print(\"Recall score: {}\".format(score_tf_recall))\n",
    "\n",
    "score_tf_f1 = metrics.f1_score(true_stars, predict_stars, average= \"weighted\")\n",
    "print(\"F1 score: {}\".format(score_tf_f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification with Stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tensorflow classification with stopping\n",
    "\n",
    "model_class = Sequential()\n",
    "model_class.add(Dense(70, input_dim=x_train_lr.shape[1], activation='relu')) # Hidden 1\n",
    "model_class.add(Dense(30, activation='relu')) # Hidden 2\n",
    "model_class.add(Dense(y_train_lr.shape[1], activation='softmax')) # Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up checkpointer\n",
    "checkpointer_classification = ModelCheckpoint(filepath=\"./best_weights_classification.hdf5\", verbose=2, save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/100\n",
      " - 5s - loss: 1.7665 - val_loss: 1.4384\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.43845, saving model to ./best_weights_classification.hdf5\n",
      "Epoch 2/100\n",
      " - 1s - loss: 1.3082 - val_loss: 1.2876\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.43845 to 1.28761, saving model to ./best_weights_classification.hdf5\n",
      "Epoch 3/100\n",
      " - 1s - loss: 1.1688 - val_loss: 1.2556\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.28761 to 1.25556, saving model to ./best_weights_classification.hdf5\n",
      "Epoch 4/100\n",
      " - 1s - loss: 1.0884 - val_loss: 1.2558\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.25556\n",
      "Epoch 5/100\n",
      " - 1s - loss: 1.0276 - val_loss: 1.2612\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.25556\n",
      "Epoch 6/100\n",
      " - 1s - loss: 0.9716 - val_loss: 1.2772\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.25556\n",
      "Epoch 00006: early stopping\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/100\n",
      " - 5s - loss: 0.9137 - val_loss: 1.3044\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1.25556\n",
      "Epoch 2/100\n",
      " - 1s - loss: 0.8664 - val_loss: 1.3250\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.25556\n",
      "Epoch 3/100\n",
      " - 1s - loss: 0.8235 - val_loss: 1.3668\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.25556\n",
      "Epoch 4/100\n",
      " - 1s - loss: 0.7827 - val_loss: 1.4003\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.25556\n",
      "Epoch 00004: early stopping\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/100\n",
      " - 5s - loss: 0.7437 - val_loss: 1.4326\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1.25556\n",
      "Epoch 2/100\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    model_class.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "    model_class.fit(x_train_lr,y_train_lr,validation_data=(x_test_lr,y_test_lr),callbacks=[monitor,checkpointer_classification],verbose=2,epochs=100) \n",
    "\n",
    "model_class.load_weights('./best_weights_classification.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_class_stopping = model_class.predict(x_test_lr)\n",
    "print(\"Shape: {}\".format(pred_class_stopping.shape))\n",
    "print(pred_class_stopping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_stars = np.argmax(pred_class,axis=1)\n",
    "\n",
    "true_stars = np.argmax(y_test_lr,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    print(\"{}. Business ID: {}, Actual Stars: {}, predicted Stars: {}\".format(i+1,merge_df['business_id'][2000+i],true_stars[i],predict_stars[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics\n",
    "\n",
    "score_tf_acc_stopping = metrics.accuracy_score(true_stars, predict_stars)\n",
    "print(\"Accuracy score: {}\".format(score_tf_acc_stopping))\n",
    "\n",
    "score_tf_precision_stopping = metrics.precision_score(true_stars, predict_stars, average= \"weighted\")\n",
    "print(\"Precision score: {}\".format(score_tf_precision_stopping))\n",
    "\n",
    "score_tf_recall_stopping = metrics.recall_score(true_stars, predict_stars, average= \"weighted\")\n",
    "print(\"Recall score: {}\".format(score_tf_recall_stopping))\n",
    "\n",
    "score_tf_f1_stopping = metrics.f1_score(true_stars, predict_stars, average= \"weighted\")\n",
    "print(\"F1 score: {}\".format(score_tf_f1_stopping))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification with postal and categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train test data for linear regression\n",
    "\n",
    "x_train_tf, x_test_tf, y_train_tf, y_test_tf = train_test_split(x_matrix_final, y_stars_encoded, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tensorflow classification\n",
    "\n",
    "model_class = Sequential()\n",
    "model_class.add(Dense(50, input_dim=x_train_tf.shape[1], activation='relu')) # Hidden 1\n",
    "model_class.add(Dense(25, activation='relu')) # Hidden 2\n",
    "model_class.add(Dense(y_train_lr.shape[1], activation='softmax')) # Output\n",
    "\n",
    "model_class.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "model_class.fit(x_train_tf,y_train_tf,verbose=2,epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_class = model_class.predict(x_test_tf)\n",
    "print(\"Shape: {}\".format(pred_class.shape))\n",
    "print(pred_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_stars = np.argmax(pred_class,axis=1)\n",
    "\n",
    "true_stars = np.argmax(y_test_tf,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    print(\"{}. Business ID: {}, Actual Stars: {}, predicted Stars: {}\".format(i+1,merge_df['business_id'][2000+i],true_stars[i],predict_stars[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics\n",
    "\n",
    "score_tf_acc_postal = metrics.accuracy_score(true_stars, predict_stars)\n",
    "print(\"Accuracy score: {}\".format(score_tf_acc_postal))\n",
    "\n",
    "score_tf_precision_postal = metrics.precision_score(true_stars, predict_stars, average= \"weighted\")\n",
    "print(\"Precision score: {}\".format(score_tf_precision_postal))\n",
    "\n",
    "score_tf_recall_postal = metrics.recall_score(true_stars, predict_stars, average= \"weighted\")\n",
    "print(\"Recall score: {}\".format(score_tf_recall_postal))\n",
    "\n",
    "score_tf_f1_postal = metrics.f1_score(true_stars, predict_stars, average= \"weighted\")\n",
    "print(\"F1 score: {}\".format(score_tf_f1_postal))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting RMSE score for different number of layers for relu model\n",
    "\n",
    "score_list_Classification=[score_knn_acc,score_mnb_acc,score_tf_acc,score_tf_acc_stopping,score_tf_acc_postal]\n",
    "names =['KNN','MNB','TF Classification','TF Classification Stopping','TF postal + Categories']\n",
    "tick_marks = np.arange(len(names))\n",
    "plt.bar(range(len(score_list_Classification)), score_list_Classification)\n",
    "plt.xticks(tick_marks, names, rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additional Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib.pyplot import figure, show\n",
    "from sklearn import metrics\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras import regularizers\n",
    "from keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train test data\n",
    "x_train_reg, x_test_reg, y_train_reg, y_test_reg = train_test_split(x_matrix_minmax, y_stars_regression , test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#USe linear regression for regularization\n",
    "model_regularization = Sequential()\n",
    "model_regularization.add(Dense(50, input_dim=x_train_reg.shape[1], activation='relu'))\n",
    "model_regularization.add(Dense(25, activation='relu'))\n",
    "model_regularization.add(Dense(10, \n",
    "                kernel_regularizer=regularizers.l1(0.01),\n",
    "                activity_regularizer=regularizers.l2(0.01), activation='relu'))\n",
    "model_regularization.add(Dense(1)) \n",
    "model_regularization.compile(loss='mean_squared_error', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=5, verbose=2, mode='auto')\n",
    "\n",
    "model_regularization.fit(x_train_reg,y_train_reg,validation_data=(x_test_reg,y_test_reg),callbacks=[monitor],verbose=2,epochs=100)\n",
    "\n",
    "pred_regularization = model_regularization.predict(x_test_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measure RMSE error.  RMSE is common for regression.\n",
    "score_regularization = np.sqrt(metrics.mean_squared_error(pred_regularization,y_test_reg))\n",
    "print(\"Final score (RMSE): {}\".format(score_regularization))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropout\n",
    "\n",
    "model_dropout = Sequential()\n",
    "model_dropout.add(Dense(50, input_dim=x_train_reg.shape[1]))\n",
    "model_dropout.add(Dropout(0.1))\n",
    "\n",
    "model_dropout.add(Dense(25, activation='relu'))\n",
    "model_dropout.add(Dense(10, activation='relu'))\n",
    "model_dropout.add(Dense(1))\n",
    "\n",
    "model_dropout.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\n",
    "monitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=5, verbose=2, mode='auto')\n",
    "model_dropout.fit(x_train_reg,y_train_reg,validation_data=(x_test_reg,y_test_reg),callbacks=[monitor],verbose=0,epochs=1000)\n",
    "\n",
    "pred_dropout = model_dropout.predict(x_test_reg)\n",
    "\n",
    "# Measure RMSE error.  RMSE is common for regression.\n",
    "score_dropout = np.sqrt(metrics.mean_squared_error(pred_dropout,y_test_reg))\n",
    "print(\"Final score (RMSE): {}\".format(score_dropout))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dropout.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking to see if regularization helps with over fitting when using postal code and categories and one hot coded values\n",
    "#train test data\n",
    "x_train_reg_ad, x_test_reg_ad, y_train_reg_ad, y_test_reg_ad = train_test_split(x_matrix_final, y_stars_regression , test_size=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#USe linear regression for regularization\n",
    "model_regularization_ad = Sequential()\n",
    "model_regularization_ad.add(Dense(50, input_dim=x_train_reg_ad.shape[1], activation='relu'))\n",
    "model_regularization_ad.add(Dense(25, activation='relu'))\n",
    "model_regularization_ad.add(Dense(10, \n",
    "                kernel_regularizer=regularizers.l1(0.01),\n",
    "                activity_regularizer=regularizers.l2(0.01), activation='relu'))\n",
    "model_regularization_ad.add(Dense(1)) \n",
    "model_regularization_ad.compile(loss='mean_squared_error', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=5, verbose=1, mode='auto')\n",
    "\n",
    "model_regularization_ad.fit(x_train_reg_ad,y_train_reg_ad,validation_data=(x_test_reg_ad,y_test_reg_ad),callbacks=[monitor],verbose=2,epochs=100)\n",
    "\n",
    "pred_regularization_ad = model_regularization_ad.predict(x_test_reg_ad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measure RMSE error.  RMSE is common for regression.\n",
    "score_regularization_ad = np.sqrt(metrics.mean_squared_error(pred_regularization_ad,y_test_reg_ad))\n",
    "print(\"Final score (RMSE): {}\".format(score_regularization_ad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropout\n",
    "\n",
    "model_dropout_ad = Sequential()\n",
    "model_dropout_ad.add(Dense(50, input_dim=x_train_reg_ad.shape[1]))\n",
    "model_dropout_ad.add(Dropout(0.1))\n",
    "\n",
    "model_dropout_ad.add(Dense(25, activation='relu'))\n",
    "model_dropout_ad.add(Dense(10, activation='relu'))\n",
    "model_dropout_ad.add(Dense(1))\n",
    "\n",
    "model_dropout_ad.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\n",
    "monitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=5, verbose=2, mode='auto')\n",
    "model_dropout_ad.fit(x_train_reg_ad,y_train_reg_ad,validation_data=(x_test_reg_ad,y_test_reg_ad),callbacks=[monitor],verbose=2,epochs=1000)\n",
    "\n",
    "pred_dropout_ad = model_dropout_ad.predict(x_test_reg_ad)\n",
    "\n",
    "# Measure RMSE error.  RMSE is common for regression.\n",
    "score_dropout_ad = np.sqrt(metrics.mean_squared_error(pred_dropout_ad,y_test_reg_ad))\n",
    "print(\"Final score (RMSE): {}\".format(score_dropout_ad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dropout_ad.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM\n",
    "\n",
    "svm_model = SVC(kernel=\"linear\")\n",
    "\n",
    "svm_model.fit(x_train_lr, y_train_lr)\n",
    "\n",
    "y_pred_svm = svm_model.predict(x_test_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "business id - EmezZdxbvjydG5FkN6Mecw actual stars label - 2 predicted - 6\n",
      "business id - cmaPrML-0zCJOs8_1VYmaw actual stars label - 7 predicted - 6\n",
      "business id - E6U8zl527AsspbTf5nZCdw actual stars label - 4 predicted - 6\n",
      "business id - zXRf_6Bs1yX9an_QKpzbHQ actual stars label - 2 predicted - 6\n",
      "business id - vllzSssD2HXGlzGUcITxhw actual stars label - 6 predicted - 6\n",
      "business id - AcGRSWCpb7YB95MTsHlGEw actual stars label - 2 predicted - 6\n",
      "business id - zfEcOCrgUKe8xYOdqNVmmA actual stars label - 5 predicted - 6\n",
      "business id - z-q6Wu-L-iDCftYVfoElPw actual stars label - 8 predicted - 6\n",
      "business id - K7c5wAhxd6CqtmBmY47c7g actual stars label - 5 predicted - 6\n",
      "business id - b30HREePgMGPZMPaExTZSA actual stars label - 6 predicted - 6\n"
     ]
    }
   ],
   "source": [
    "# list  the business with the stars and prediction\n",
    "\n",
    "for i in range(0,10):\n",
    "    idx=y_test_lr.index[i]\n",
    "    print(\"business id - %s actual stars label - %d predicted - %d\" \n",
    "          %(merge_df['business_id'][idx], y_test_lr[idx], y_pred_svm[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final score (RMSE): 2.14247062073706\n",
      "R2 score: -0.16\n"
     ]
    }
   ],
   "source": [
    "# Metrics\n",
    "\n",
    "score_svm_acc = metrics.accuracy_score(y_test_lr, y_pred_svm)\n",
    "print(\"Accuracy score: {}\".format(score_svm_acc))\n",
    "\n",
    "score_svm_precision = metrics.precision_score(y_test_lr, y_pred_svm, average= \"weighted\")\n",
    "print(\"Precision score: {}\".format(score_svm_precision))\n",
    "\n",
    "score_svm_recall = metrics.recall_score(y_test_lr, y_pred_svm, average= \"weighted\")\n",
    "print(\"Recall score: {}\".format(score_svm_recall))\n",
    "\n",
    "score_svm_f1 = metrics.f1_score(y_test_lr, y_pred_svm, average= \"weighted\")\n",
    "print(\"F1 score: {}\".format(score_svm_f1))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
