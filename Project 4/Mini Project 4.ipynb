{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sci\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras import regularizers\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.layers import Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import io\n",
    "import requests\n",
    "import tensorflow as tf\n",
    "from scipy import sparse\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sklearn.feature_extraction.text as sk_text\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.metrics import accuracy_score, precision_score, f1_score\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import collections\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "import shutil\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Useful Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to normalize columns\n",
    "def normalize_numeric_minmax(df, name):\n",
    "        df[name] = ((df[name] - df[name].min()) / (df[name].max() - df[name].min())).astype(np.float32)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert a Pandas dataframe to the x,y inputs that TensorFlow needs\n",
    "import collections\n",
    "def to_xy(df, target):\n",
    "    result = []\n",
    "    for x in df.columns:\n",
    "        if x != target:\n",
    "            result.append(x)\n",
    "    # find out the type of the target column. \n",
    "    target_type = df[target].dtypes\n",
    "    target_type = target_type[0] if isinstance(target_type, collections.Sequence) else target_type\n",
    "    # Encode to int for classification, float otherwise. TensorFlow likes 32 bits.\n",
    "    if target_type in (np.int64, np.int32):\n",
    "        # Classification\n",
    "        dummies = pd.get_dummies(df[target])\n",
    "        return df[result].values.astype(np.float32), dummies.values.astype(np.float32)\n",
    "    else:\n",
    "        # Regression\n",
    "        return df[result].values.astype(np.float32), df[target].values.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regression chart.\n",
    "def chart_regression(pred,y,sort=True):\n",
    "    t = pd.DataFrame({'pred' : pred, 'y' : y.flatten()})\n",
    "    if sort:\n",
    "        t.sort_values(by=['y'],inplace=True)\n",
    "    a = plt.plot(t['y'].tolist(),label='expected')\n",
    "    b = plt.plot(t['pred'].tolist(),label='prediction')\n",
    "    plt.ylabel('output')\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj_Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2000/3/27</td>\n",
       "      <td>3.812500</td>\n",
       "      <td>4.156250</td>\n",
       "      <td>3.812500</td>\n",
       "      <td>4.125000</td>\n",
       "      <td>4.125000</td>\n",
       "      <td>3675600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2000/3/28</td>\n",
       "      <td>4.125000</td>\n",
       "      <td>4.125000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.015625</td>\n",
       "      <td>4.015625</td>\n",
       "      <td>1077600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2000/3/29</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.031250</td>\n",
       "      <td>3.953125</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>437200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2000/3/30</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.843750</td>\n",
       "      <td>3.843750</td>\n",
       "      <td>3.843750</td>\n",
       "      <td>1883600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2000/3/31</td>\n",
       "      <td>3.734375</td>\n",
       "      <td>3.734375</td>\n",
       "      <td>3.390625</td>\n",
       "      <td>3.390625</td>\n",
       "      <td>3.390625</td>\n",
       "      <td>7931600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2000/4/3</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>3.703125</td>\n",
       "      <td>3.437500</td>\n",
       "      <td>3.437500</td>\n",
       "      <td>3.437500</td>\n",
       "      <td>11486800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2000/4/4</td>\n",
       "      <td>3.531250</td>\n",
       "      <td>3.578125</td>\n",
       "      <td>3.093750</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>13136800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2000/4/5</td>\n",
       "      <td>3.468750</td>\n",
       "      <td>3.562500</td>\n",
       "      <td>3.453125</td>\n",
       "      <td>3.484375</td>\n",
       "      <td>3.484375</td>\n",
       "      <td>6349600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2000/4/6</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>3.593750</td>\n",
       "      <td>3.468750</td>\n",
       "      <td>3.578125</td>\n",
       "      <td>3.578125</td>\n",
       "      <td>7181200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2000/4/7</td>\n",
       "      <td>3.593750</td>\n",
       "      <td>3.812500</td>\n",
       "      <td>3.593750</td>\n",
       "      <td>3.609375</td>\n",
       "      <td>3.609375</td>\n",
       "      <td>13904800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2000/4/10</td>\n",
       "      <td>3.687500</td>\n",
       "      <td>3.750000</td>\n",
       "      <td>3.625000</td>\n",
       "      <td>3.640625</td>\n",
       "      <td>3.640625</td>\n",
       "      <td>5280800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2000/4/11</td>\n",
       "      <td>3.578125</td>\n",
       "      <td>3.656250</td>\n",
       "      <td>3.562500</td>\n",
       "      <td>3.578125</td>\n",
       "      <td>3.578125</td>\n",
       "      <td>6590000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2000/4/12</td>\n",
       "      <td>3.546875</td>\n",
       "      <td>3.640625</td>\n",
       "      <td>3.531250</td>\n",
       "      <td>3.578125</td>\n",
       "      <td>3.578125</td>\n",
       "      <td>8546400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2000/4/13</td>\n",
       "      <td>3.578125</td>\n",
       "      <td>3.656250</td>\n",
       "      <td>3.578125</td>\n",
       "      <td>3.613275</td>\n",
       "      <td>3.613275</td>\n",
       "      <td>6874400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2000/4/14</td>\n",
       "      <td>3.609375</td>\n",
       "      <td>3.625000</td>\n",
       "      <td>3.531250</td>\n",
       "      <td>3.609375</td>\n",
       "      <td>3.609375</td>\n",
       "      <td>2626000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2000/4/17</td>\n",
       "      <td>3.578125</td>\n",
       "      <td>3.609375</td>\n",
       "      <td>3.515625</td>\n",
       "      <td>3.562500</td>\n",
       "      <td>3.562500</td>\n",
       "      <td>2992000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2000/4/18</td>\n",
       "      <td>3.609375</td>\n",
       "      <td>3.625000</td>\n",
       "      <td>3.484375</td>\n",
       "      <td>3.515625</td>\n",
       "      <td>3.515625</td>\n",
       "      <td>2896000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2000/4/19</td>\n",
       "      <td>3.562500</td>\n",
       "      <td>3.656250</td>\n",
       "      <td>3.562500</td>\n",
       "      <td>3.593750</td>\n",
       "      <td>3.593750</td>\n",
       "      <td>4662400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2000/4/20</td>\n",
       "      <td>3.578125</td>\n",
       "      <td>3.687500</td>\n",
       "      <td>3.562500</td>\n",
       "      <td>3.656250</td>\n",
       "      <td>3.656250</td>\n",
       "      <td>4558800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2000/4/24</td>\n",
       "      <td>3.593750</td>\n",
       "      <td>3.656250</td>\n",
       "      <td>3.531250</td>\n",
       "      <td>3.562500</td>\n",
       "      <td>3.562500</td>\n",
       "      <td>3815200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2000/4/25</td>\n",
       "      <td>3.578125</td>\n",
       "      <td>3.781250</td>\n",
       "      <td>3.468750</td>\n",
       "      <td>3.765625</td>\n",
       "      <td>3.765625</td>\n",
       "      <td>4635600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2000/4/26</td>\n",
       "      <td>3.781250</td>\n",
       "      <td>4.125000</td>\n",
       "      <td>3.750000</td>\n",
       "      <td>3.828125</td>\n",
       "      <td>3.828125</td>\n",
       "      <td>5321200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2000/4/27</td>\n",
       "      <td>3.765625</td>\n",
       "      <td>3.765625</td>\n",
       "      <td>3.640625</td>\n",
       "      <td>3.703125</td>\n",
       "      <td>3.703125</td>\n",
       "      <td>4085200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2000/4/28</td>\n",
       "      <td>3.687500</td>\n",
       "      <td>3.750000</td>\n",
       "      <td>3.625000</td>\n",
       "      <td>3.750000</td>\n",
       "      <td>3.750000</td>\n",
       "      <td>3539200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2000/5/1</td>\n",
       "      <td>3.750000</td>\n",
       "      <td>3.750000</td>\n",
       "      <td>3.609375</td>\n",
       "      <td>3.687500</td>\n",
       "      <td>3.687500</td>\n",
       "      <td>4310000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2000/5/2</td>\n",
       "      <td>3.656250</td>\n",
       "      <td>3.687500</td>\n",
       "      <td>3.609375</td>\n",
       "      <td>3.640625</td>\n",
       "      <td>3.640625</td>\n",
       "      <td>2684400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2000/5/3</td>\n",
       "      <td>3.546875</td>\n",
       "      <td>3.640625</td>\n",
       "      <td>3.515625</td>\n",
       "      <td>3.562500</td>\n",
       "      <td>3.562500</td>\n",
       "      <td>4164400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2000/5/4</td>\n",
       "      <td>3.593750</td>\n",
       "      <td>3.687500</td>\n",
       "      <td>3.562500</td>\n",
       "      <td>3.687500</td>\n",
       "      <td>3.687500</td>\n",
       "      <td>3610400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2000/5/5</td>\n",
       "      <td>3.718750</td>\n",
       "      <td>3.843750</td>\n",
       "      <td>3.718750</td>\n",
       "      <td>3.781250</td>\n",
       "      <td>3.781250</td>\n",
       "      <td>1710800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2000/5/8</td>\n",
       "      <td>3.843750</td>\n",
       "      <td>3.906250</td>\n",
       "      <td>3.796875</td>\n",
       "      <td>3.812500</td>\n",
       "      <td>3.812500</td>\n",
       "      <td>1435600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4362</th>\n",
       "      <td>2017/7/28</td>\n",
       "      <td>115.949997</td>\n",
       "      <td>116.190002</td>\n",
       "      <td>113.360001</td>\n",
       "      <td>115.510002</td>\n",
       "      <td>115.510002</td>\n",
       "      <td>2185100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4363</th>\n",
       "      <td>2017/7/31</td>\n",
       "      <td>115.779999</td>\n",
       "      <td>115.870003</td>\n",
       "      <td>114.080002</td>\n",
       "      <td>115.180000</td>\n",
       "      <td>115.180000</td>\n",
       "      <td>1467900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4364</th>\n",
       "      <td>2017/8/1</td>\n",
       "      <td>115.870003</td>\n",
       "      <td>116.220001</td>\n",
       "      <td>115.110001</td>\n",
       "      <td>115.269997</td>\n",
       "      <td>115.269997</td>\n",
       "      <td>1297200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4365</th>\n",
       "      <td>2017/8/2</td>\n",
       "      <td>115.260002</td>\n",
       "      <td>115.480003</td>\n",
       "      <td>112.870003</td>\n",
       "      <td>114.239998</td>\n",
       "      <td>114.239998</td>\n",
       "      <td>1249500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4366</th>\n",
       "      <td>2017/8/3</td>\n",
       "      <td>114.250000</td>\n",
       "      <td>116.800003</td>\n",
       "      <td>113.760002</td>\n",
       "      <td>116.449997</td>\n",
       "      <td>116.449997</td>\n",
       "      <td>1469600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4367</th>\n",
       "      <td>2017/8/4</td>\n",
       "      <td>116.820000</td>\n",
       "      <td>117.000000</td>\n",
       "      <td>115.500000</td>\n",
       "      <td>116.190002</td>\n",
       "      <td>116.190002</td>\n",
       "      <td>962800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4368</th>\n",
       "      <td>2017/8/7</td>\n",
       "      <td>116.199997</td>\n",
       "      <td>116.739998</td>\n",
       "      <td>115.879997</td>\n",
       "      <td>116.480003</td>\n",
       "      <td>116.480003</td>\n",
       "      <td>723200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4369</th>\n",
       "      <td>2017/8/8</td>\n",
       "      <td>116.480003</td>\n",
       "      <td>117.290001</td>\n",
       "      <td>115.800003</td>\n",
       "      <td>116.160004</td>\n",
       "      <td>116.160004</td>\n",
       "      <td>976900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4370</th>\n",
       "      <td>2017/8/9</td>\n",
       "      <td>115.750000</td>\n",
       "      <td>116.500000</td>\n",
       "      <td>115.230003</td>\n",
       "      <td>116.260002</td>\n",
       "      <td>116.260002</td>\n",
       "      <td>980400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4371</th>\n",
       "      <td>2017/8/10</td>\n",
       "      <td>115.739998</td>\n",
       "      <td>115.919998</td>\n",
       "      <td>113.570000</td>\n",
       "      <td>114.129997</td>\n",
       "      <td>114.129997</td>\n",
       "      <td>982100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4372</th>\n",
       "      <td>2017/8/11</td>\n",
       "      <td>113.949997</td>\n",
       "      <td>115.660004</td>\n",
       "      <td>113.540001</td>\n",
       "      <td>114.500000</td>\n",
       "      <td>114.500000</td>\n",
       "      <td>673800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4373</th>\n",
       "      <td>2017/8/14</td>\n",
       "      <td>115.040001</td>\n",
       "      <td>115.940002</td>\n",
       "      <td>114.959999</td>\n",
       "      <td>115.639999</td>\n",
       "      <td>115.639999</td>\n",
       "      <td>1294500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4374</th>\n",
       "      <td>2017/8/15</td>\n",
       "      <td>115.779999</td>\n",
       "      <td>115.779999</td>\n",
       "      <td>114.690002</td>\n",
       "      <td>114.830002</td>\n",
       "      <td>114.830002</td>\n",
       "      <td>898400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4375</th>\n",
       "      <td>2017/8/16</td>\n",
       "      <td>115.220001</td>\n",
       "      <td>115.930000</td>\n",
       "      <td>115.000000</td>\n",
       "      <td>115.570000</td>\n",
       "      <td>115.570000</td>\n",
       "      <td>943600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4376</th>\n",
       "      <td>2017/8/17</td>\n",
       "      <td>115.489998</td>\n",
       "      <td>115.790001</td>\n",
       "      <td>113.360001</td>\n",
       "      <td>113.570000</td>\n",
       "      <td>113.570000</td>\n",
       "      <td>981500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4377</th>\n",
       "      <td>2017/8/18</td>\n",
       "      <td>113.370003</td>\n",
       "      <td>113.610001</td>\n",
       "      <td>112.339996</td>\n",
       "      <td>113.110001</td>\n",
       "      <td>113.110001</td>\n",
       "      <td>1134600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4378</th>\n",
       "      <td>2017/8/21</td>\n",
       "      <td>113.150002</td>\n",
       "      <td>114.989998</td>\n",
       "      <td>112.550003</td>\n",
       "      <td>114.220001</td>\n",
       "      <td>114.220001</td>\n",
       "      <td>1089000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4379</th>\n",
       "      <td>2017/8/22</td>\n",
       "      <td>114.050003</td>\n",
       "      <td>115.089996</td>\n",
       "      <td>113.800003</td>\n",
       "      <td>114.220001</td>\n",
       "      <td>114.220001</td>\n",
       "      <td>1433200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4380</th>\n",
       "      <td>2017/8/23</td>\n",
       "      <td>114.000000</td>\n",
       "      <td>114.000000</td>\n",
       "      <td>112.470001</td>\n",
       "      <td>112.709999</td>\n",
       "      <td>112.709999</td>\n",
       "      <td>2074000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4381</th>\n",
       "      <td>2017/8/24</td>\n",
       "      <td>112.769997</td>\n",
       "      <td>113.459999</td>\n",
       "      <td>112.070000</td>\n",
       "      <td>112.750000</td>\n",
       "      <td>112.750000</td>\n",
       "      <td>1269400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4382</th>\n",
       "      <td>2017/8/25</td>\n",
       "      <td>112.949997</td>\n",
       "      <td>113.400002</td>\n",
       "      <td>111.720001</td>\n",
       "      <td>111.760002</td>\n",
       "      <td>111.760002</td>\n",
       "      <td>1206400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4383</th>\n",
       "      <td>2017/8/28</td>\n",
       "      <td>112.559998</td>\n",
       "      <td>113.449997</td>\n",
       "      <td>112.470001</td>\n",
       "      <td>113.110001</td>\n",
       "      <td>113.110001</td>\n",
       "      <td>1050200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4384</th>\n",
       "      <td>2017/8/29</td>\n",
       "      <td>112.320000</td>\n",
       "      <td>113.250000</td>\n",
       "      <td>111.739998</td>\n",
       "      <td>111.830002</td>\n",
       "      <td>111.830002</td>\n",
       "      <td>1366000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4385</th>\n",
       "      <td>2017/8/30</td>\n",
       "      <td>112.019997</td>\n",
       "      <td>112.989998</td>\n",
       "      <td>111.529999</td>\n",
       "      <td>112.690002</td>\n",
       "      <td>112.690002</td>\n",
       "      <td>929200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4386</th>\n",
       "      <td>2017/8/31</td>\n",
       "      <td>112.820000</td>\n",
       "      <td>113.790001</td>\n",
       "      <td>112.440002</td>\n",
       "      <td>113.660004</td>\n",
       "      <td>113.660004</td>\n",
       "      <td>1091400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4387</th>\n",
       "      <td>2017/9/1</td>\n",
       "      <td>113.790001</td>\n",
       "      <td>114.099998</td>\n",
       "      <td>112.790001</td>\n",
       "      <td>113.309998</td>\n",
       "      <td>113.309998</td>\n",
       "      <td>950000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4388</th>\n",
       "      <td>2017/9/5</td>\n",
       "      <td>112.519997</td>\n",
       "      <td>113.529999</td>\n",
       "      <td>111.160004</td>\n",
       "      <td>111.870003</td>\n",
       "      <td>111.870003</td>\n",
       "      <td>1805200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4389</th>\n",
       "      <td>2017/9/6</td>\n",
       "      <td>112.029999</td>\n",
       "      <td>112.489998</td>\n",
       "      <td>110.250000</td>\n",
       "      <td>112.230003</td>\n",
       "      <td>112.230003</td>\n",
       "      <td>2136700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4390</th>\n",
       "      <td>2017/9/7</td>\n",
       "      <td>112.459999</td>\n",
       "      <td>112.900002</td>\n",
       "      <td>112.000000</td>\n",
       "      <td>112.339996</td>\n",
       "      <td>112.339996</td>\n",
       "      <td>1251600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4391</th>\n",
       "      <td>2017/9/8</td>\n",
       "      <td>112.300003</td>\n",
       "      <td>114.790001</td>\n",
       "      <td>112.010002</td>\n",
       "      <td>113.190002</td>\n",
       "      <td>113.190002</td>\n",
       "      <td>1611700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4392 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Date        Open        High         Low       Close   Adj_Close  \\\n",
       "0     2000/3/27    3.812500    4.156250    3.812500    4.125000    4.125000   \n",
       "1     2000/3/28    4.125000    4.125000    4.000000    4.015625    4.015625   \n",
       "2     2000/3/29    4.000000    4.031250    3.953125    4.000000    4.000000   \n",
       "3     2000/3/30    4.000000    4.000000    3.843750    3.843750    3.843750   \n",
       "4     2000/3/31    3.734375    3.734375    3.390625    3.390625    3.390625   \n",
       "5      2000/4/3    3.500000    3.703125    3.437500    3.437500    3.437500   \n",
       "6      2000/4/4    3.531250    3.578125    3.093750    3.500000    3.500000   \n",
       "7      2000/4/5    3.468750    3.562500    3.453125    3.484375    3.484375   \n",
       "8      2000/4/6    3.500000    3.593750    3.468750    3.578125    3.578125   \n",
       "9      2000/4/7    3.593750    3.812500    3.593750    3.609375    3.609375   \n",
       "10    2000/4/10    3.687500    3.750000    3.625000    3.640625    3.640625   \n",
       "11    2000/4/11    3.578125    3.656250    3.562500    3.578125    3.578125   \n",
       "12    2000/4/12    3.546875    3.640625    3.531250    3.578125    3.578125   \n",
       "13    2000/4/13    3.578125    3.656250    3.578125    3.613275    3.613275   \n",
       "14    2000/4/14    3.609375    3.625000    3.531250    3.609375    3.609375   \n",
       "15    2000/4/17    3.578125    3.609375    3.515625    3.562500    3.562500   \n",
       "16    2000/4/18    3.609375    3.625000    3.484375    3.515625    3.515625   \n",
       "17    2000/4/19    3.562500    3.656250    3.562500    3.593750    3.593750   \n",
       "18    2000/4/20    3.578125    3.687500    3.562500    3.656250    3.656250   \n",
       "19    2000/4/24    3.593750    3.656250    3.531250    3.562500    3.562500   \n",
       "20    2000/4/25    3.578125    3.781250    3.468750    3.765625    3.765625   \n",
       "21    2000/4/26    3.781250    4.125000    3.750000    3.828125    3.828125   \n",
       "22    2000/4/27    3.765625    3.765625    3.640625    3.703125    3.703125   \n",
       "23    2000/4/28    3.687500    3.750000    3.625000    3.750000    3.750000   \n",
       "24     2000/5/1    3.750000    3.750000    3.609375    3.687500    3.687500   \n",
       "25     2000/5/2    3.656250    3.687500    3.609375    3.640625    3.640625   \n",
       "26     2000/5/3    3.546875    3.640625    3.515625    3.562500    3.562500   \n",
       "27     2000/5/4    3.593750    3.687500    3.562500    3.687500    3.687500   \n",
       "28     2000/5/5    3.718750    3.843750    3.718750    3.781250    3.781250   \n",
       "29     2000/5/8    3.843750    3.906250    3.796875    3.812500    3.812500   \n",
       "...         ...         ...         ...         ...         ...         ...   \n",
       "4362  2017/7/28  115.949997  116.190002  113.360001  115.510002  115.510002   \n",
       "4363  2017/7/31  115.779999  115.870003  114.080002  115.180000  115.180000   \n",
       "4364   2017/8/1  115.870003  116.220001  115.110001  115.269997  115.269997   \n",
       "4365   2017/8/2  115.260002  115.480003  112.870003  114.239998  114.239998   \n",
       "4366   2017/8/3  114.250000  116.800003  113.760002  116.449997  116.449997   \n",
       "4367   2017/8/4  116.820000  117.000000  115.500000  116.190002  116.190002   \n",
       "4368   2017/8/7  116.199997  116.739998  115.879997  116.480003  116.480003   \n",
       "4369   2017/8/8  116.480003  117.290001  115.800003  116.160004  116.160004   \n",
       "4370   2017/8/9  115.750000  116.500000  115.230003  116.260002  116.260002   \n",
       "4371  2017/8/10  115.739998  115.919998  113.570000  114.129997  114.129997   \n",
       "4372  2017/8/11  113.949997  115.660004  113.540001  114.500000  114.500000   \n",
       "4373  2017/8/14  115.040001  115.940002  114.959999  115.639999  115.639999   \n",
       "4374  2017/8/15  115.779999  115.779999  114.690002  114.830002  114.830002   \n",
       "4375  2017/8/16  115.220001  115.930000  115.000000  115.570000  115.570000   \n",
       "4376  2017/8/17  115.489998  115.790001  113.360001  113.570000  113.570000   \n",
       "4377  2017/8/18  113.370003  113.610001  112.339996  113.110001  113.110001   \n",
       "4378  2017/8/21  113.150002  114.989998  112.550003  114.220001  114.220001   \n",
       "4379  2017/8/22  114.050003  115.089996  113.800003  114.220001  114.220001   \n",
       "4380  2017/8/23  114.000000  114.000000  112.470001  112.709999  112.709999   \n",
       "4381  2017/8/24  112.769997  113.459999  112.070000  112.750000  112.750000   \n",
       "4382  2017/8/25  112.949997  113.400002  111.720001  111.760002  111.760002   \n",
       "4383  2017/8/28  112.559998  113.449997  112.470001  113.110001  113.110001   \n",
       "4384  2017/8/29  112.320000  113.250000  111.739998  111.830002  111.830002   \n",
       "4385  2017/8/30  112.019997  112.989998  111.529999  112.690002  112.690002   \n",
       "4386  2017/8/31  112.820000  113.790001  112.440002  113.660004  113.660004   \n",
       "4387   2017/9/1  113.790001  114.099998  112.790001  113.309998  113.309998   \n",
       "4388   2017/9/5  112.519997  113.529999  111.160004  111.870003  111.870003   \n",
       "4389   2017/9/6  112.029999  112.489998  110.250000  112.230003  112.230003   \n",
       "4390   2017/9/7  112.459999  112.900002  112.000000  112.339996  112.339996   \n",
       "4391   2017/9/8  112.300003  114.790001  112.010002  113.190002  113.190002   \n",
       "\n",
       "        Volume  \n",
       "0      3675600  \n",
       "1      1077600  \n",
       "2       437200  \n",
       "3      1883600  \n",
       "4      7931600  \n",
       "5     11486800  \n",
       "6     13136800  \n",
       "7      6349600  \n",
       "8      7181200  \n",
       "9     13904800  \n",
       "10     5280800  \n",
       "11     6590000  \n",
       "12     8546400  \n",
       "13     6874400  \n",
       "14     2626000  \n",
       "15     2992000  \n",
       "16     2896000  \n",
       "17     4662400  \n",
       "18     4558800  \n",
       "19     3815200  \n",
       "20     4635600  \n",
       "21     5321200  \n",
       "22     4085200  \n",
       "23     3539200  \n",
       "24     4310000  \n",
       "25     2684400  \n",
       "26     4164400  \n",
       "27     3610400  \n",
       "28     1710800  \n",
       "29     1435600  \n",
       "...        ...  \n",
       "4362   2185100  \n",
       "4363   1467900  \n",
       "4364   1297200  \n",
       "4365   1249500  \n",
       "4366   1469600  \n",
       "4367    962800  \n",
       "4368    723200  \n",
       "4369    976900  \n",
       "4370    980400  \n",
       "4371    982100  \n",
       "4372    673800  \n",
       "4373   1294500  \n",
       "4374    898400  \n",
       "4375    943600  \n",
       "4376    981500  \n",
       "4377   1134600  \n",
       "4378   1089000  \n",
       "4379   1433200  \n",
       "4380   2074000  \n",
       "4381   1269400  \n",
       "4382   1206400  \n",
       "4383   1050200  \n",
       "4384   1366000  \n",
       "4385    929200  \n",
       "4386   1091400  \n",
       "4387    950000  \n",
       "4388   1805200  \n",
       "4389   2136700  \n",
       "4390   1251600  \n",
       "4391   1611700  \n",
       "\n",
       "[4392 rows x 7 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Read Stock_Price_MAX.csv file and load data into stock_df dataframe \n",
    "stock_df= pd.read_csv('Stock_Price_MAX.csv')\n",
    "stock_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.812500</td>\n",
       "      <td>4.156250</td>\n",
       "      <td>3.812500</td>\n",
       "      <td>4.125000</td>\n",
       "      <td>3675600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.125000</td>\n",
       "      <td>4.125000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.015625</td>\n",
       "      <td>1077600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.031250</td>\n",
       "      <td>3.953125</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>437200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.843750</td>\n",
       "      <td>3.843750</td>\n",
       "      <td>1883600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.734375</td>\n",
       "      <td>3.734375</td>\n",
       "      <td>3.390625</td>\n",
       "      <td>3.390625</td>\n",
       "      <td>7931600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3.500000</td>\n",
       "      <td>3.703125</td>\n",
       "      <td>3.437500</td>\n",
       "      <td>3.437500</td>\n",
       "      <td>11486800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3.531250</td>\n",
       "      <td>3.578125</td>\n",
       "      <td>3.093750</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>13136800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3.468750</td>\n",
       "      <td>3.562500</td>\n",
       "      <td>3.453125</td>\n",
       "      <td>3.484375</td>\n",
       "      <td>6349600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3.500000</td>\n",
       "      <td>3.593750</td>\n",
       "      <td>3.468750</td>\n",
       "      <td>3.578125</td>\n",
       "      <td>7181200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3.593750</td>\n",
       "      <td>3.812500</td>\n",
       "      <td>3.593750</td>\n",
       "      <td>3.609375</td>\n",
       "      <td>13904800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>3.687500</td>\n",
       "      <td>3.750000</td>\n",
       "      <td>3.625000</td>\n",
       "      <td>3.640625</td>\n",
       "      <td>5280800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>3.578125</td>\n",
       "      <td>3.656250</td>\n",
       "      <td>3.562500</td>\n",
       "      <td>3.578125</td>\n",
       "      <td>6590000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>3.546875</td>\n",
       "      <td>3.640625</td>\n",
       "      <td>3.531250</td>\n",
       "      <td>3.578125</td>\n",
       "      <td>8546400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>3.578125</td>\n",
       "      <td>3.656250</td>\n",
       "      <td>3.578125</td>\n",
       "      <td>3.613275</td>\n",
       "      <td>6874400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>3.609375</td>\n",
       "      <td>3.625000</td>\n",
       "      <td>3.531250</td>\n",
       "      <td>3.609375</td>\n",
       "      <td>2626000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>3.578125</td>\n",
       "      <td>3.609375</td>\n",
       "      <td>3.515625</td>\n",
       "      <td>3.562500</td>\n",
       "      <td>2992000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>3.609375</td>\n",
       "      <td>3.625000</td>\n",
       "      <td>3.484375</td>\n",
       "      <td>3.515625</td>\n",
       "      <td>2896000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>3.562500</td>\n",
       "      <td>3.656250</td>\n",
       "      <td>3.562500</td>\n",
       "      <td>3.593750</td>\n",
       "      <td>4662400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>3.578125</td>\n",
       "      <td>3.687500</td>\n",
       "      <td>3.562500</td>\n",
       "      <td>3.656250</td>\n",
       "      <td>4558800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>3.593750</td>\n",
       "      <td>3.656250</td>\n",
       "      <td>3.531250</td>\n",
       "      <td>3.562500</td>\n",
       "      <td>3815200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>3.578125</td>\n",
       "      <td>3.781250</td>\n",
       "      <td>3.468750</td>\n",
       "      <td>3.765625</td>\n",
       "      <td>4635600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>3.781250</td>\n",
       "      <td>4.125000</td>\n",
       "      <td>3.750000</td>\n",
       "      <td>3.828125</td>\n",
       "      <td>5321200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>3.765625</td>\n",
       "      <td>3.765625</td>\n",
       "      <td>3.640625</td>\n",
       "      <td>3.703125</td>\n",
       "      <td>4085200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>3.687500</td>\n",
       "      <td>3.750000</td>\n",
       "      <td>3.625000</td>\n",
       "      <td>3.750000</td>\n",
       "      <td>3539200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>3.750000</td>\n",
       "      <td>3.750000</td>\n",
       "      <td>3.609375</td>\n",
       "      <td>3.687500</td>\n",
       "      <td>4310000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>3.656250</td>\n",
       "      <td>3.687500</td>\n",
       "      <td>3.609375</td>\n",
       "      <td>3.640625</td>\n",
       "      <td>2684400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>3.546875</td>\n",
       "      <td>3.640625</td>\n",
       "      <td>3.515625</td>\n",
       "      <td>3.562500</td>\n",
       "      <td>4164400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>3.593750</td>\n",
       "      <td>3.687500</td>\n",
       "      <td>3.562500</td>\n",
       "      <td>3.687500</td>\n",
       "      <td>3610400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>3.718750</td>\n",
       "      <td>3.843750</td>\n",
       "      <td>3.718750</td>\n",
       "      <td>3.781250</td>\n",
       "      <td>1710800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>3.843750</td>\n",
       "      <td>3.906250</td>\n",
       "      <td>3.796875</td>\n",
       "      <td>3.812500</td>\n",
       "      <td>1435600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4362</th>\n",
       "      <td>115.949997</td>\n",
       "      <td>116.190002</td>\n",
       "      <td>113.360001</td>\n",
       "      <td>115.510002</td>\n",
       "      <td>2185100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4363</th>\n",
       "      <td>115.779999</td>\n",
       "      <td>115.870003</td>\n",
       "      <td>114.080002</td>\n",
       "      <td>115.180000</td>\n",
       "      <td>1467900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4364</th>\n",
       "      <td>115.870003</td>\n",
       "      <td>116.220001</td>\n",
       "      <td>115.110001</td>\n",
       "      <td>115.269997</td>\n",
       "      <td>1297200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4365</th>\n",
       "      <td>115.260002</td>\n",
       "      <td>115.480003</td>\n",
       "      <td>112.870003</td>\n",
       "      <td>114.239998</td>\n",
       "      <td>1249500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4366</th>\n",
       "      <td>114.250000</td>\n",
       "      <td>116.800003</td>\n",
       "      <td>113.760002</td>\n",
       "      <td>116.449997</td>\n",
       "      <td>1469600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4367</th>\n",
       "      <td>116.820000</td>\n",
       "      <td>117.000000</td>\n",
       "      <td>115.500000</td>\n",
       "      <td>116.190002</td>\n",
       "      <td>962800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4368</th>\n",
       "      <td>116.199997</td>\n",
       "      <td>116.739998</td>\n",
       "      <td>115.879997</td>\n",
       "      <td>116.480003</td>\n",
       "      <td>723200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4369</th>\n",
       "      <td>116.480003</td>\n",
       "      <td>117.290001</td>\n",
       "      <td>115.800003</td>\n",
       "      <td>116.160004</td>\n",
       "      <td>976900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4370</th>\n",
       "      <td>115.750000</td>\n",
       "      <td>116.500000</td>\n",
       "      <td>115.230003</td>\n",
       "      <td>116.260002</td>\n",
       "      <td>980400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4371</th>\n",
       "      <td>115.739998</td>\n",
       "      <td>115.919998</td>\n",
       "      <td>113.570000</td>\n",
       "      <td>114.129997</td>\n",
       "      <td>982100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4372</th>\n",
       "      <td>113.949997</td>\n",
       "      <td>115.660004</td>\n",
       "      <td>113.540001</td>\n",
       "      <td>114.500000</td>\n",
       "      <td>673800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4373</th>\n",
       "      <td>115.040001</td>\n",
       "      <td>115.940002</td>\n",
       "      <td>114.959999</td>\n",
       "      <td>115.639999</td>\n",
       "      <td>1294500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4374</th>\n",
       "      <td>115.779999</td>\n",
       "      <td>115.779999</td>\n",
       "      <td>114.690002</td>\n",
       "      <td>114.830002</td>\n",
       "      <td>898400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4375</th>\n",
       "      <td>115.220001</td>\n",
       "      <td>115.930000</td>\n",
       "      <td>115.000000</td>\n",
       "      <td>115.570000</td>\n",
       "      <td>943600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4376</th>\n",
       "      <td>115.489998</td>\n",
       "      <td>115.790001</td>\n",
       "      <td>113.360001</td>\n",
       "      <td>113.570000</td>\n",
       "      <td>981500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4377</th>\n",
       "      <td>113.370003</td>\n",
       "      <td>113.610001</td>\n",
       "      <td>112.339996</td>\n",
       "      <td>113.110001</td>\n",
       "      <td>1134600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4378</th>\n",
       "      <td>113.150002</td>\n",
       "      <td>114.989998</td>\n",
       "      <td>112.550003</td>\n",
       "      <td>114.220001</td>\n",
       "      <td>1089000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4379</th>\n",
       "      <td>114.050003</td>\n",
       "      <td>115.089996</td>\n",
       "      <td>113.800003</td>\n",
       "      <td>114.220001</td>\n",
       "      <td>1433200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4380</th>\n",
       "      <td>114.000000</td>\n",
       "      <td>114.000000</td>\n",
       "      <td>112.470001</td>\n",
       "      <td>112.709999</td>\n",
       "      <td>2074000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4381</th>\n",
       "      <td>112.769997</td>\n",
       "      <td>113.459999</td>\n",
       "      <td>112.070000</td>\n",
       "      <td>112.750000</td>\n",
       "      <td>1269400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4382</th>\n",
       "      <td>112.949997</td>\n",
       "      <td>113.400002</td>\n",
       "      <td>111.720001</td>\n",
       "      <td>111.760002</td>\n",
       "      <td>1206400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4383</th>\n",
       "      <td>112.559998</td>\n",
       "      <td>113.449997</td>\n",
       "      <td>112.470001</td>\n",
       "      <td>113.110001</td>\n",
       "      <td>1050200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4384</th>\n",
       "      <td>112.320000</td>\n",
       "      <td>113.250000</td>\n",
       "      <td>111.739998</td>\n",
       "      <td>111.830002</td>\n",
       "      <td>1366000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4385</th>\n",
       "      <td>112.019997</td>\n",
       "      <td>112.989998</td>\n",
       "      <td>111.529999</td>\n",
       "      <td>112.690002</td>\n",
       "      <td>929200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4386</th>\n",
       "      <td>112.820000</td>\n",
       "      <td>113.790001</td>\n",
       "      <td>112.440002</td>\n",
       "      <td>113.660004</td>\n",
       "      <td>1091400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4387</th>\n",
       "      <td>113.790001</td>\n",
       "      <td>114.099998</td>\n",
       "      <td>112.790001</td>\n",
       "      <td>113.309998</td>\n",
       "      <td>950000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4388</th>\n",
       "      <td>112.519997</td>\n",
       "      <td>113.529999</td>\n",
       "      <td>111.160004</td>\n",
       "      <td>111.870003</td>\n",
       "      <td>1805200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4389</th>\n",
       "      <td>112.029999</td>\n",
       "      <td>112.489998</td>\n",
       "      <td>110.250000</td>\n",
       "      <td>112.230003</td>\n",
       "      <td>2136700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4390</th>\n",
       "      <td>112.459999</td>\n",
       "      <td>112.900002</td>\n",
       "      <td>112.000000</td>\n",
       "      <td>112.339996</td>\n",
       "      <td>1251600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4391</th>\n",
       "      <td>112.300003</td>\n",
       "      <td>114.790001</td>\n",
       "      <td>112.010002</td>\n",
       "      <td>113.190002</td>\n",
       "      <td>1611700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4392 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Open        High         Low       Close    Volume\n",
       "0       3.812500    4.156250    3.812500    4.125000   3675600\n",
       "1       4.125000    4.125000    4.000000    4.015625   1077600\n",
       "2       4.000000    4.031250    3.953125    4.000000    437200\n",
       "3       4.000000    4.000000    3.843750    3.843750   1883600\n",
       "4       3.734375    3.734375    3.390625    3.390625   7931600\n",
       "5       3.500000    3.703125    3.437500    3.437500  11486800\n",
       "6       3.531250    3.578125    3.093750    3.500000  13136800\n",
       "7       3.468750    3.562500    3.453125    3.484375   6349600\n",
       "8       3.500000    3.593750    3.468750    3.578125   7181200\n",
       "9       3.593750    3.812500    3.593750    3.609375  13904800\n",
       "10      3.687500    3.750000    3.625000    3.640625   5280800\n",
       "11      3.578125    3.656250    3.562500    3.578125   6590000\n",
       "12      3.546875    3.640625    3.531250    3.578125   8546400\n",
       "13      3.578125    3.656250    3.578125    3.613275   6874400\n",
       "14      3.609375    3.625000    3.531250    3.609375   2626000\n",
       "15      3.578125    3.609375    3.515625    3.562500   2992000\n",
       "16      3.609375    3.625000    3.484375    3.515625   2896000\n",
       "17      3.562500    3.656250    3.562500    3.593750   4662400\n",
       "18      3.578125    3.687500    3.562500    3.656250   4558800\n",
       "19      3.593750    3.656250    3.531250    3.562500   3815200\n",
       "20      3.578125    3.781250    3.468750    3.765625   4635600\n",
       "21      3.781250    4.125000    3.750000    3.828125   5321200\n",
       "22      3.765625    3.765625    3.640625    3.703125   4085200\n",
       "23      3.687500    3.750000    3.625000    3.750000   3539200\n",
       "24      3.750000    3.750000    3.609375    3.687500   4310000\n",
       "25      3.656250    3.687500    3.609375    3.640625   2684400\n",
       "26      3.546875    3.640625    3.515625    3.562500   4164400\n",
       "27      3.593750    3.687500    3.562500    3.687500   3610400\n",
       "28      3.718750    3.843750    3.718750    3.781250   1710800\n",
       "29      3.843750    3.906250    3.796875    3.812500   1435600\n",
       "...          ...         ...         ...         ...       ...\n",
       "4362  115.949997  116.190002  113.360001  115.510002   2185100\n",
       "4363  115.779999  115.870003  114.080002  115.180000   1467900\n",
       "4364  115.870003  116.220001  115.110001  115.269997   1297200\n",
       "4365  115.260002  115.480003  112.870003  114.239998   1249500\n",
       "4366  114.250000  116.800003  113.760002  116.449997   1469600\n",
       "4367  116.820000  117.000000  115.500000  116.190002    962800\n",
       "4368  116.199997  116.739998  115.879997  116.480003    723200\n",
       "4369  116.480003  117.290001  115.800003  116.160004    976900\n",
       "4370  115.750000  116.500000  115.230003  116.260002    980400\n",
       "4371  115.739998  115.919998  113.570000  114.129997    982100\n",
       "4372  113.949997  115.660004  113.540001  114.500000    673800\n",
       "4373  115.040001  115.940002  114.959999  115.639999   1294500\n",
       "4374  115.779999  115.779999  114.690002  114.830002    898400\n",
       "4375  115.220001  115.930000  115.000000  115.570000    943600\n",
       "4376  115.489998  115.790001  113.360001  113.570000    981500\n",
       "4377  113.370003  113.610001  112.339996  113.110001   1134600\n",
       "4378  113.150002  114.989998  112.550003  114.220001   1089000\n",
       "4379  114.050003  115.089996  113.800003  114.220001   1433200\n",
       "4380  114.000000  114.000000  112.470001  112.709999   2074000\n",
       "4381  112.769997  113.459999  112.070000  112.750000   1269400\n",
       "4382  112.949997  113.400002  111.720001  111.760002   1206400\n",
       "4383  112.559998  113.449997  112.470001  113.110001   1050200\n",
       "4384  112.320000  113.250000  111.739998  111.830002   1366000\n",
       "4385  112.019997  112.989998  111.529999  112.690002    929200\n",
       "4386  112.820000  113.790001  112.440002  113.660004   1091400\n",
       "4387  113.790001  114.099998  112.790001  113.309998    950000\n",
       "4388  112.519997  113.529999  111.160004  111.870003   1805200\n",
       "4389  112.029999  112.489998  110.250000  112.230003   2136700\n",
       "4390  112.459999  112.900002  112.000000  112.339996   1251600\n",
       "4391  112.300003  114.790001  112.010002  113.190002   1611700\n",
       "\n",
       "[4392 rows x 5 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stock_df = stock_df.drop(['Date', 'Adj_Close'], axis = 1)\n",
    "stock_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.004378</td>\n",
       "      <td>0.006469</td>\n",
       "      <td>0.006934</td>\n",
       "      <td>0.007408</td>\n",
       "      <td>0.075401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.007031</td>\n",
       "      <td>0.006205</td>\n",
       "      <td>0.008535</td>\n",
       "      <td>0.006482</td>\n",
       "      <td>0.019194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.005970</td>\n",
       "      <td>0.005413</td>\n",
       "      <td>0.008135</td>\n",
       "      <td>0.006350</td>\n",
       "      <td>0.005339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.005970</td>\n",
       "      <td>0.005149</td>\n",
       "      <td>0.007201</td>\n",
       "      <td>0.005027</td>\n",
       "      <td>0.036632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.003714</td>\n",
       "      <td>0.002904</td>\n",
       "      <td>0.003334</td>\n",
       "      <td>0.001191</td>\n",
       "      <td>0.167478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.001725</td>\n",
       "      <td>0.002640</td>\n",
       "      <td>0.003734</td>\n",
       "      <td>0.001588</td>\n",
       "      <td>0.244393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.001990</td>\n",
       "      <td>0.001584</td>\n",
       "      <td>0.000800</td>\n",
       "      <td>0.002117</td>\n",
       "      <td>0.280091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.001459</td>\n",
       "      <td>0.001452</td>\n",
       "      <td>0.003867</td>\n",
       "      <td>0.001984</td>\n",
       "      <td>0.133252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.001725</td>\n",
       "      <td>0.001716</td>\n",
       "      <td>0.004001</td>\n",
       "      <td>0.002778</td>\n",
       "      <td>0.151243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.002521</td>\n",
       "      <td>0.003564</td>\n",
       "      <td>0.005067</td>\n",
       "      <td>0.003043</td>\n",
       "      <td>0.296706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.003316</td>\n",
       "      <td>0.003036</td>\n",
       "      <td>0.005334</td>\n",
       "      <td>0.003307</td>\n",
       "      <td>0.110129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.002388</td>\n",
       "      <td>0.002244</td>\n",
       "      <td>0.004801</td>\n",
       "      <td>0.002778</td>\n",
       "      <td>0.138453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.002123</td>\n",
       "      <td>0.002112</td>\n",
       "      <td>0.004534</td>\n",
       "      <td>0.002778</td>\n",
       "      <td>0.180779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.002388</td>\n",
       "      <td>0.002244</td>\n",
       "      <td>0.004934</td>\n",
       "      <td>0.003076</td>\n",
       "      <td>0.144606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.002653</td>\n",
       "      <td>0.001980</td>\n",
       "      <td>0.004534</td>\n",
       "      <td>0.003043</td>\n",
       "      <td>0.052693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.002388</td>\n",
       "      <td>0.001848</td>\n",
       "      <td>0.004401</td>\n",
       "      <td>0.002646</td>\n",
       "      <td>0.060612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.002653</td>\n",
       "      <td>0.001980</td>\n",
       "      <td>0.004134</td>\n",
       "      <td>0.002249</td>\n",
       "      <td>0.058535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.002255</td>\n",
       "      <td>0.002244</td>\n",
       "      <td>0.004801</td>\n",
       "      <td>0.002910</td>\n",
       "      <td>0.096750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.002388</td>\n",
       "      <td>0.002508</td>\n",
       "      <td>0.004801</td>\n",
       "      <td>0.003440</td>\n",
       "      <td>0.094509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.002521</td>\n",
       "      <td>0.002244</td>\n",
       "      <td>0.004534</td>\n",
       "      <td>0.002646</td>\n",
       "      <td>0.078421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.002388</td>\n",
       "      <td>0.003300</td>\n",
       "      <td>0.004001</td>\n",
       "      <td>0.004366</td>\n",
       "      <td>0.096170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.004112</td>\n",
       "      <td>0.006205</td>\n",
       "      <td>0.006401</td>\n",
       "      <td>0.004895</td>\n",
       "      <td>0.111003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.003980</td>\n",
       "      <td>0.003168</td>\n",
       "      <td>0.005467</td>\n",
       "      <td>0.003836</td>\n",
       "      <td>0.084263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.003316</td>\n",
       "      <td>0.003036</td>\n",
       "      <td>0.005334</td>\n",
       "      <td>0.004233</td>\n",
       "      <td>0.072450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.003847</td>\n",
       "      <td>0.003036</td>\n",
       "      <td>0.005201</td>\n",
       "      <td>0.003704</td>\n",
       "      <td>0.089126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.003051</td>\n",
       "      <td>0.002508</td>\n",
       "      <td>0.005201</td>\n",
       "      <td>0.003307</td>\n",
       "      <td>0.053957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.002123</td>\n",
       "      <td>0.002112</td>\n",
       "      <td>0.004401</td>\n",
       "      <td>0.002646</td>\n",
       "      <td>0.085976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.002521</td>\n",
       "      <td>0.002508</td>\n",
       "      <td>0.004801</td>\n",
       "      <td>0.003704</td>\n",
       "      <td>0.073990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.003582</td>\n",
       "      <td>0.003828</td>\n",
       "      <td>0.006134</td>\n",
       "      <td>0.004498</td>\n",
       "      <td>0.032893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.004643</td>\n",
       "      <td>0.004356</td>\n",
       "      <td>0.006801</td>\n",
       "      <td>0.004763</td>\n",
       "      <td>0.026939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4362</th>\n",
       "      <td>0.956445</td>\n",
       "      <td>0.953024</td>\n",
       "      <td>0.941879</td>\n",
       "      <td>0.950470</td>\n",
       "      <td>0.043155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4363</th>\n",
       "      <td>0.955002</td>\n",
       "      <td>0.950321</td>\n",
       "      <td>0.948024</td>\n",
       "      <td>0.947676</td>\n",
       "      <td>0.027638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4364</th>\n",
       "      <td>0.955766</td>\n",
       "      <td>0.953278</td>\n",
       "      <td>0.956815</td>\n",
       "      <td>0.948438</td>\n",
       "      <td>0.023945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4365</th>\n",
       "      <td>0.950587</td>\n",
       "      <td>0.947026</td>\n",
       "      <td>0.937697</td>\n",
       "      <td>0.939717</td>\n",
       "      <td>0.022913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4366</th>\n",
       "      <td>0.942012</td>\n",
       "      <td>0.958178</td>\n",
       "      <td>0.945293</td>\n",
       "      <td>0.958429</td>\n",
       "      <td>0.027675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4367</th>\n",
       "      <td>0.963832</td>\n",
       "      <td>0.959868</td>\n",
       "      <td>0.960143</td>\n",
       "      <td>0.956227</td>\n",
       "      <td>0.016711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4368</th>\n",
       "      <td>0.958568</td>\n",
       "      <td>0.957671</td>\n",
       "      <td>0.963387</td>\n",
       "      <td>0.958683</td>\n",
       "      <td>0.011527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4369</th>\n",
       "      <td>0.960945</td>\n",
       "      <td>0.962318</td>\n",
       "      <td>0.962704</td>\n",
       "      <td>0.955973</td>\n",
       "      <td>0.017016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4370</th>\n",
       "      <td>0.954747</td>\n",
       "      <td>0.955644</td>\n",
       "      <td>0.957839</td>\n",
       "      <td>0.956820</td>\n",
       "      <td>0.017091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4371</th>\n",
       "      <td>0.954662</td>\n",
       "      <td>0.950743</td>\n",
       "      <td>0.943672</td>\n",
       "      <td>0.938786</td>\n",
       "      <td>0.017128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4372</th>\n",
       "      <td>0.939465</td>\n",
       "      <td>0.948547</td>\n",
       "      <td>0.943416</td>\n",
       "      <td>0.941919</td>\n",
       "      <td>0.010458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4373</th>\n",
       "      <td>0.948719</td>\n",
       "      <td>0.950912</td>\n",
       "      <td>0.955535</td>\n",
       "      <td>0.951571</td>\n",
       "      <td>0.023887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4374</th>\n",
       "      <td>0.955002</td>\n",
       "      <td>0.949560</td>\n",
       "      <td>0.953230</td>\n",
       "      <td>0.944713</td>\n",
       "      <td>0.015317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4375</th>\n",
       "      <td>0.950248</td>\n",
       "      <td>0.950828</td>\n",
       "      <td>0.955876</td>\n",
       "      <td>0.950978</td>\n",
       "      <td>0.016295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4376</th>\n",
       "      <td>0.952540</td>\n",
       "      <td>0.949645</td>\n",
       "      <td>0.941879</td>\n",
       "      <td>0.934045</td>\n",
       "      <td>0.017115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4377</th>\n",
       "      <td>0.934541</td>\n",
       "      <td>0.931226</td>\n",
       "      <td>0.933174</td>\n",
       "      <td>0.930150</td>\n",
       "      <td>0.020427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4378</th>\n",
       "      <td>0.932673</td>\n",
       "      <td>0.942886</td>\n",
       "      <td>0.934966</td>\n",
       "      <td>0.939548</td>\n",
       "      <td>0.019441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4379</th>\n",
       "      <td>0.940314</td>\n",
       "      <td>0.943731</td>\n",
       "      <td>0.945635</td>\n",
       "      <td>0.939548</td>\n",
       "      <td>0.026888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4380</th>\n",
       "      <td>0.939889</td>\n",
       "      <td>0.934521</td>\n",
       "      <td>0.934284</td>\n",
       "      <td>0.926763</td>\n",
       "      <td>0.040751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4381</th>\n",
       "      <td>0.929447</td>\n",
       "      <td>0.929959</td>\n",
       "      <td>0.930870</td>\n",
       "      <td>0.927102</td>\n",
       "      <td>0.023344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4382</th>\n",
       "      <td>0.930975</td>\n",
       "      <td>0.929452</td>\n",
       "      <td>0.927883</td>\n",
       "      <td>0.918720</td>\n",
       "      <td>0.021981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4383</th>\n",
       "      <td>0.927664</td>\n",
       "      <td>0.929875</td>\n",
       "      <td>0.934284</td>\n",
       "      <td>0.930150</td>\n",
       "      <td>0.018601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4384</th>\n",
       "      <td>0.925626</td>\n",
       "      <td>0.928185</td>\n",
       "      <td>0.928053</td>\n",
       "      <td>0.919313</td>\n",
       "      <td>0.025434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4385</th>\n",
       "      <td>0.923079</td>\n",
       "      <td>0.925988</td>\n",
       "      <td>0.926261</td>\n",
       "      <td>0.926594</td>\n",
       "      <td>0.015984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4386</th>\n",
       "      <td>0.929871</td>\n",
       "      <td>0.932747</td>\n",
       "      <td>0.934027</td>\n",
       "      <td>0.934807</td>\n",
       "      <td>0.019493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4387</th>\n",
       "      <td>0.938107</td>\n",
       "      <td>0.935366</td>\n",
       "      <td>0.937015</td>\n",
       "      <td>0.931843</td>\n",
       "      <td>0.016434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4388</th>\n",
       "      <td>0.927324</td>\n",
       "      <td>0.930551</td>\n",
       "      <td>0.923103</td>\n",
       "      <td>0.919651</td>\n",
       "      <td>0.034936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4389</th>\n",
       "      <td>0.923164</td>\n",
       "      <td>0.921764</td>\n",
       "      <td>0.915337</td>\n",
       "      <td>0.922699</td>\n",
       "      <td>0.042107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4390</th>\n",
       "      <td>0.926815</td>\n",
       "      <td>0.925228</td>\n",
       "      <td>0.930272</td>\n",
       "      <td>0.923630</td>\n",
       "      <td>0.022959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4391</th>\n",
       "      <td>0.925456</td>\n",
       "      <td>0.941196</td>\n",
       "      <td>0.930358</td>\n",
       "      <td>0.930827</td>\n",
       "      <td>0.030749</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4392 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Open      High       Low     Close    Volume\n",
       "0     0.004378  0.006469  0.006934  0.007408  0.075401\n",
       "1     0.007031  0.006205  0.008535  0.006482  0.019194\n",
       "2     0.005970  0.005413  0.008135  0.006350  0.005339\n",
       "3     0.005970  0.005149  0.007201  0.005027  0.036632\n",
       "4     0.003714  0.002904  0.003334  0.001191  0.167478\n",
       "5     0.001725  0.002640  0.003734  0.001588  0.244393\n",
       "6     0.001990  0.001584  0.000800  0.002117  0.280091\n",
       "7     0.001459  0.001452  0.003867  0.001984  0.133252\n",
       "8     0.001725  0.001716  0.004001  0.002778  0.151243\n",
       "9     0.002521  0.003564  0.005067  0.003043  0.296706\n",
       "10    0.003316  0.003036  0.005334  0.003307  0.110129\n",
       "11    0.002388  0.002244  0.004801  0.002778  0.138453\n",
       "12    0.002123  0.002112  0.004534  0.002778  0.180779\n",
       "13    0.002388  0.002244  0.004934  0.003076  0.144606\n",
       "14    0.002653  0.001980  0.004534  0.003043  0.052693\n",
       "15    0.002388  0.001848  0.004401  0.002646  0.060612\n",
       "16    0.002653  0.001980  0.004134  0.002249  0.058535\n",
       "17    0.002255  0.002244  0.004801  0.002910  0.096750\n",
       "18    0.002388  0.002508  0.004801  0.003440  0.094509\n",
       "19    0.002521  0.002244  0.004534  0.002646  0.078421\n",
       "20    0.002388  0.003300  0.004001  0.004366  0.096170\n",
       "21    0.004112  0.006205  0.006401  0.004895  0.111003\n",
       "22    0.003980  0.003168  0.005467  0.003836  0.084263\n",
       "23    0.003316  0.003036  0.005334  0.004233  0.072450\n",
       "24    0.003847  0.003036  0.005201  0.003704  0.089126\n",
       "25    0.003051  0.002508  0.005201  0.003307  0.053957\n",
       "26    0.002123  0.002112  0.004401  0.002646  0.085976\n",
       "27    0.002521  0.002508  0.004801  0.003704  0.073990\n",
       "28    0.003582  0.003828  0.006134  0.004498  0.032893\n",
       "29    0.004643  0.004356  0.006801  0.004763  0.026939\n",
       "...        ...       ...       ...       ...       ...\n",
       "4362  0.956445  0.953024  0.941879  0.950470  0.043155\n",
       "4363  0.955002  0.950321  0.948024  0.947676  0.027638\n",
       "4364  0.955766  0.953278  0.956815  0.948438  0.023945\n",
       "4365  0.950587  0.947026  0.937697  0.939717  0.022913\n",
       "4366  0.942012  0.958178  0.945293  0.958429  0.027675\n",
       "4367  0.963832  0.959868  0.960143  0.956227  0.016711\n",
       "4368  0.958568  0.957671  0.963387  0.958683  0.011527\n",
       "4369  0.960945  0.962318  0.962704  0.955973  0.017016\n",
       "4370  0.954747  0.955644  0.957839  0.956820  0.017091\n",
       "4371  0.954662  0.950743  0.943672  0.938786  0.017128\n",
       "4372  0.939465  0.948547  0.943416  0.941919  0.010458\n",
       "4373  0.948719  0.950912  0.955535  0.951571  0.023887\n",
       "4374  0.955002  0.949560  0.953230  0.944713  0.015317\n",
       "4375  0.950248  0.950828  0.955876  0.950978  0.016295\n",
       "4376  0.952540  0.949645  0.941879  0.934045  0.017115\n",
       "4377  0.934541  0.931226  0.933174  0.930150  0.020427\n",
       "4378  0.932673  0.942886  0.934966  0.939548  0.019441\n",
       "4379  0.940314  0.943731  0.945635  0.939548  0.026888\n",
       "4380  0.939889  0.934521  0.934284  0.926763  0.040751\n",
       "4381  0.929447  0.929959  0.930870  0.927102  0.023344\n",
       "4382  0.930975  0.929452  0.927883  0.918720  0.021981\n",
       "4383  0.927664  0.929875  0.934284  0.930150  0.018601\n",
       "4384  0.925626  0.928185  0.928053  0.919313  0.025434\n",
       "4385  0.923079  0.925988  0.926261  0.926594  0.015984\n",
       "4386  0.929871  0.932747  0.934027  0.934807  0.019493\n",
       "4387  0.938107  0.935366  0.937015  0.931843  0.016434\n",
       "4388  0.927324  0.930551  0.923103  0.919651  0.034936\n",
       "4389  0.923164  0.921764  0.915337  0.922699  0.042107\n",
       "4390  0.926815  0.925228  0.930272  0.923630  0.022959\n",
       "4391  0.925456  0.941196  0.930358  0.930827  0.030749\n",
       "\n",
       "[4392 rows x 5 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Normalize the columns\n",
    "    \n",
    "normalize_numeric_minmax(stock_df,\"Open\")\n",
    "normalize_numeric_minmax(stock_df,\"High\") \n",
    "normalize_numeric_minmax(stock_df,\"Low\") \n",
    "normalize_numeric_minmax(stock_df,\"Volume\") \n",
    "normalize_numeric_minmax(stock_df,\"Close\") \n",
    "stock_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to xy to convert pandas to tensor flow\n",
    "x,y=to_xy(stock_df,\"Close\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fully Connected Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split for train and test\n",
    "x_train, x_test, y_train, y_test = train_test_split(x,y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3074, 4)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1318, 4)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3074,)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1318,)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## ReLU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### ReLU, adam, 2 layers, early stopping and Model checkpoint  - score_relu_2l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# set up checkpointer\n",
    "checkpointer_relu = ModelCheckpoint(filepath=\"./best_weights_relu_2l.hdf5\", verbose=1, save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Train on 3074 samples, validate on 1318 samples\n",
      "Epoch 1/100\n",
      " - 0s - loss: 0.0104 - val_loss: 9.5655e-05\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.00010, saving model to ./best_weights_relu_2l_nn.hdf5\n",
      "Epoch 2/100\n",
      " - 0s - loss: 7.5660e-05 - val_loss: 3.7997e-05\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.00010 to 0.00004, saving model to ./best_weights_relu_2l_nn.hdf5\n",
      "Epoch 3/100\n",
      " - 0s - loss: 3.1386e-05 - val_loss: 2.1262e-05\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.00004 to 0.00002, saving model to ./best_weights_relu_2l_nn.hdf5\n",
      "Epoch 4/100\n",
      " - 0s - loss: 2.0685e-05 - val_loss: 1.5795e-05\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.00002 to 0.00002, saving model to ./best_weights_relu_2l_nn.hdf5\n",
      "Epoch 5/100\n",
      " - 0s - loss: 1.7797e-05 - val_loss: 1.4677e-05\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.00002 to 0.00001, saving model to ./best_weights_relu_2l_nn.hdf5\n",
      "Epoch 6/100\n",
      " - 0s - loss: 1.7891e-05 - val_loss: 1.4893e-05\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.00001\n",
      "Epoch 00006: early stopping\n",
      "1\n",
      "Train on 3074 samples, validate on 1318 samples\n",
      "Epoch 1/100\n",
      " - 0s - loss: 0.0131 - val_loss: 2.5571e-05\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.00001\n",
      "Epoch 2/100\n",
      " - 0s - loss: 2.3038e-05 - val_loss: 1.7276e-05\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.00001\n",
      "Epoch 3/100\n",
      " - 0s - loss: 1.9504e-05 - val_loss: 1.5916e-05\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.00001\n",
      "Epoch 4/100\n",
      " - 0s - loss: 1.8442e-05 - val_loss: 1.4447e-05\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.00001 to 0.00001, saving model to ./best_weights_relu_2l_nn.hdf5\n",
      "Epoch 5/100\n",
      " - 0s - loss: 1.7868e-05 - val_loss: 1.4530e-05\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.00001\n",
      "Epoch 6/100\n",
      " - 0s - loss: 1.7505e-05 - val_loss: 1.4515e-05\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.00001\n",
      "Epoch 00006: early stopping\n",
      "2\n",
      "Train on 3074 samples, validate on 1318 samples\n",
      "Epoch 1/100\n",
      " - 0s - loss: 0.0210 - val_loss: 5.1225e-05\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.00001\n",
      "Epoch 2/100\n",
      " - 0s - loss: 2.0514e-05 - val_loss: 1.3345e-05\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.00001 to 0.00001, saving model to ./best_weights_relu_2l_nn.hdf5\n",
      "Epoch 3/100\n",
      " - 0s - loss: 1.2152e-05 - val_loss: 9.4232e-06\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.00001 to 0.00001, saving model to ./best_weights_relu_2l_nn.hdf5\n",
      "Epoch 4/100\n",
      " - 0s - loss: 1.0598e-05 - val_loss: 1.2901e-05\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.00001\n",
      "Epoch 5/100\n",
      " - 0s - loss: 1.0353e-05 - val_loss: 8.7574e-06\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.00001 to 0.00001, saving model to ./best_weights_relu_2l_nn.hdf5\n",
      "Epoch 6/100\n",
      " - 0s - loss: 9.9275e-06 - val_loss: 9.7413e-06\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.00001\n",
      "Epoch 00006: early stopping\n",
      "3\n",
      "Train on 3074 samples, validate on 1318 samples\n",
      "Epoch 1/100\n",
      " - 0s - loss: 0.0088 - val_loss: 3.3905e-05\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.00001\n",
      "Epoch 2/100\n",
      " - 0s - loss: 2.2908e-05 - val_loss: 1.6647e-05\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.00001\n",
      "Epoch 3/100\n",
      " - 0s - loss: 1.3962e-05 - val_loss: 1.4148e-05\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.00001\n",
      "Epoch 4/100\n",
      " - 0s - loss: 1.3322e-05 - val_loss: 1.0054e-05\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.00001\n",
      "Epoch 5/100\n",
      " - 0s - loss: 1.2940e-05 - val_loss: 9.9484e-06\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.00001\n",
      "Epoch 6/100\n",
      " - 0s - loss: 1.2706e-05 - val_loss: 1.0066e-05\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.00001\n",
      "Epoch 00006: early stopping\n",
      "4\n",
      "Train on 3074 samples, validate on 1318 samples\n",
      "Epoch 1/100\n",
      " - 0s - loss: 0.0139 - val_loss: 2.4433e-05\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.00001\n",
      "Epoch 2/100\n",
      " - 0s - loss: 2.1299e-05 - val_loss: 1.7810e-05\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.00001\n",
      "Epoch 3/100\n",
      " - 0s - loss: 1.9521e-05 - val_loss: 1.6800e-05\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.00001\n",
      "Epoch 4/100\n",
      " - 0s - loss: 1.9855e-05 - val_loss: 1.5972e-05\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.00001\n",
      "Epoch 5/100\n",
      " - 0s - loss: 1.8289e-05 - val_loss: 1.5890e-05\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.00001\n",
      "Epoch 6/100\n",
      " - 0s - loss: 1.7960e-05 - val_loss: 2.0965e-05\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.00001\n",
      "Epoch 00006: early stopping\n",
      "5\n",
      "Train on 3074 samples, validate on 1318 samples\n",
      "Epoch 1/100\n",
      " - 0s - loss: 0.0237 - val_loss: 6.1015e-05\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.00001\n",
      "Epoch 2/100\n",
      " - 0s - loss: 4.5125e-05 - val_loss: 2.2214e-05\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.00001\n",
      "Epoch 3/100\n",
      " - 0s - loss: 2.4087e-05 - val_loss: 1.3457e-05\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.00001\n",
      "Epoch 4/100\n",
      " - 0s - loss: 1.4812e-05 - val_loss: 1.1250e-05\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.00001\n",
      "Epoch 5/100\n",
      " - 0s - loss: 1.2965e-05 - val_loss: 9.7896e-06\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.00001\n",
      "Epoch 6/100\n",
      " - 0s - loss: 1.2122e-05 - val_loss: 1.1357e-05\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.00001\n",
      "Epoch 00006: early stopping\n",
      "6\n",
      "Train on 3074 samples, validate on 1318 samples\n",
      "Epoch 1/100\n",
      " - 0s - loss: 0.0177 - val_loss: 4.3532e-05\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.00001\n",
      "Epoch 2/100\n",
      " - 0s - loss: 2.9299e-05 - val_loss: 1.9460e-05\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.00001\n",
      "Epoch 3/100\n",
      " - 0s - loss: 2.1412e-05 - val_loss: 1.6622e-05\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.00001\n",
      "Epoch 4/100\n",
      " - 0s - loss: 1.9019e-05 - val_loss: 2.2898e-05\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.00001\n",
      "Epoch 5/100\n",
      " - 0s - loss: 1.8970e-05 - val_loss: 1.5301e-05\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.00001\n",
      "Epoch 6/100\n",
      " - 0s - loss: 1.7539e-05 - val_loss: 2.7791e-05\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.00001\n",
      "Epoch 00006: early stopping\n",
      "7\n",
      "Train on 3074 samples, validate on 1318 samples\n",
      "Epoch 1/100\n",
      " - 0s - loss: 0.0074 - val_loss: 2.5734e-05\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.00001\n",
      "Epoch 2/100\n",
      " - 0s - loss: 2.2280e-05 - val_loss: 1.5194e-05\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.00001\n",
      "Epoch 3/100\n",
      " - 0s - loss: 1.6385e-05 - val_loss: 1.4459e-05\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.00001\n",
      "Epoch 4/100\n",
      " - 0s - loss: 1.5838e-05 - val_loss: 1.3328e-05\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.00001\n",
      "Epoch 5/100\n",
      " - 0s - loss: 1.4800e-05 - val_loss: 1.2656e-05\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.00001\n",
      "Epoch 6/100\n",
      " - 0s - loss: 1.5887e-05 - val_loss: 1.4097e-05\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.00001\n",
      "Epoch 00006: early stopping\n",
      "8\n",
      "Train on 3074 samples, validate on 1318 samples\n",
      "Epoch 1/100\n",
      " - 0s - loss: 0.0252 - val_loss: 6.0339e-05\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.00001\n",
      "Epoch 2/100\n",
      " - 0s - loss: 3.9798e-05 - val_loss: 2.2953e-05\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.00001\n",
      "Epoch 3/100\n",
      " - 0s - loss: 2.1040e-05 - val_loss: 1.6666e-05\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.00001\n",
      "Epoch 4/100\n",
      " - 0s - loss: 1.7975e-05 - val_loss: 1.4276e-05\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.00001\n",
      "Epoch 5/100\n",
      " - 0s - loss: 1.6056e-05 - val_loss: 1.4211e-05\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.00001\n",
      "Epoch 6/100\n",
      " - 0s - loss: 1.5846e-05 - val_loss: 1.4501e-05\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.00001\n",
      "Epoch 00006: early stopping\n",
      "9\n",
      "Train on 3074 samples, validate on 1318 samples\n",
      "Epoch 1/100\n",
      " - 0s - loss: 0.0213 - val_loss: 4.7412e-05\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.00001\n",
      "Epoch 2/100\n",
      " - 0s - loss: 2.4557e-05 - val_loss: 1.7940e-05\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.00001\n",
      "Epoch 3/100\n",
      " - 0s - loss: 1.5567e-05 - val_loss: 1.4326e-05\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.00001\n",
      "Epoch 4/100\n",
      " - 0s - loss: 1.3920e-05 - val_loss: 1.2215e-05\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.00001\n",
      "Epoch 5/100\n",
      " - 0s - loss: 1.3844e-05 - val_loss: 1.1974e-05\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.00001\n",
      "Epoch 6/100\n",
      " - 0s - loss: 1.2957e-05 - val_loss: 1.1661e-05\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.00001\n",
      "Epoch 00006: early stopping\n",
      "Training finished...Loading the best model\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'model_reg_relu' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-98228b8f9927>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Training finished...Loading the best model'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m \u001b[0mmodel_reg_relu\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'./best_weights_relu_2l.hdf5'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'model_reg_relu' is not defined"
     ]
    }
   ],
   "source": [
    "# relu adam 2 layers\n",
    "for i in range(10):\n",
    "    print(i)\n",
    "    \n",
    "    # Build network\n",
    "    model_relu = Sequential()\n",
    "    model_relu.add(Dense(60, input_dim=x_train.shape[1], activation='relu')) \n",
    "    model_relu.add(Dense(30, activation='relu')) # Hidden 2\n",
    "    model_relu.add(Dense(1)) # Output\n",
    "    model_relu.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    monitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=5, verbose=1, mode='auto')\n",
    "    model_relu.fit(x_train,y_train,validation_data=(x_test,y_test),callbacks=[monitor,checkpointer_relu],verbose=2,epochs=100) \n",
    "    \n",
    "print('Training finished...Loading the best model') \n",
    "print()\n",
    "model_reg_relu.load_weights('./best_weights_relu_2l.hdf5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (1318, 1)\n"
     ]
    }
   ],
   "source": [
    "# Predict stars\n",
    "pred_relu_2l = model_relu.predict(x_test)\n",
    "print(\"Shape: {}\".format(pred_relu_2l.shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final score (RMSE): 0.00341482344083488\n",
      "R2 score: 1.00\n"
     ]
    }
   ],
   "source": [
    "# Measure RMSE error.  RMSE is common for regression.\n",
    "score_relu_2l = np.sqrt(mean_squared_error(y_test,pred_relu_2l))\n",
    "print(\"Final score (RMSE): {}\".format(score_relu_2l))\n",
    "print('R2 score: %.2f' % r2_score(y_test, pred_relu_2l))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### ReLU, adam, 3 layer, early stopping and Model checkpoint  - score_relu_3l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# set up checkpointer\n",
    "checkpointer_relu = ModelCheckpoint(filepath=\"./best_weights_relu_3l.hdf5\", verbose=1, save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Train on 3074 samples, validate on 1318 samples\n",
      "Epoch 1/100\n",
      " - 1s - loss: 0.0538 - val_loss: 2.0855e-04\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.00021, saving model to ./best_weights_relu_3l.hdf5\n",
      "Epoch 2/100\n",
      " - 0s - loss: 6.9398e-05 - val_loss: 2.4523e-05\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.00021 to 0.00002, saving model to ./best_weights_relu_3l.hdf5\n",
      "Epoch 3/100\n",
      " - 0s - loss: 2.0924e-05 - val_loss: 1.6575e-05\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.00002 to 0.00002, saving model to ./best_weights_relu_3l.hdf5\n",
      "Epoch 4/100\n",
      " - 0s - loss: 1.7167e-05 - val_loss: 1.9958e-05\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.00002\n",
      "Epoch 5/100\n",
      " - 0s - loss: 1.8122e-05 - val_loss: 1.4409e-05\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.00002 to 0.00001, saving model to ./best_weights_relu_3l.hdf5\n",
      "Epoch 6/100\n",
      " - 0s - loss: 1.6714e-05 - val_loss: 1.3740e-05\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.00001 to 0.00001, saving model to ./best_weights_relu_3l.hdf5\n",
      "Epoch 00006: early stopping\n",
      "1\n",
      "Train on 3074 samples, validate on 1318 samples\n",
      "Epoch 1/100\n",
      " - 1s - loss: 0.0655 - val_loss: 0.0039\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.00001\n",
      "Epoch 2/100\n",
      " - 0s - loss: 4.3279e-04 - val_loss: 7.2152e-05\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.00001\n",
      "Epoch 3/100\n",
      " - 0s - loss: 4.8040e-05 - val_loss: 3.8291e-05\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.00001\n",
      "Epoch 4/100\n",
      " - 0s - loss: 3.1101e-05 - val_loss: 2.6041e-05\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.00001\n",
      "Epoch 5/100\n",
      " - 0s - loss: 2.9708e-05 - val_loss: 2.6270e-05\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.00001\n",
      "Epoch 6/100\n",
      " - 0s - loss: 2.1698e-05 - val_loss: 5.5915e-05\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.00001\n",
      "Epoch 7/100\n",
      " - 0s - loss: 2.8674e-05 - val_loss: 1.6211e-05\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.00001\n",
      "Epoch 00007: early stopping\n",
      "2\n",
      "Train on 3074 samples, validate on 1318 samples\n",
      "Epoch 1/100\n",
      " - 1s - loss: 0.0358 - val_loss: 3.9662e-04\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.00001\n",
      "Epoch 2/100\n",
      " - 0s - loss: 1.2798e-04 - val_loss: 2.4743e-05\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.00001\n",
      "Epoch 3/100\n",
      " - 0s - loss: 2.1591e-05 - val_loss: 1.6728e-05\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.00001\n",
      "Epoch 4/100\n",
      " - 0s - loss: 2.2280e-05 - val_loss: 1.6422e-05\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.00001\n",
      "Epoch 5/100\n",
      " - 0s - loss: 2.8024e-05 - val_loss: 8.0260e-05\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.00001\n",
      "Epoch 6/100\n",
      " - 0s - loss: 2.6128e-05 - val_loss: 1.7577e-05\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.00001\n",
      "Epoch 00006: early stopping\n",
      "3\n",
      "Train on 3074 samples, validate on 1318 samples\n",
      "Epoch 1/100\n",
      " - 2s - loss: 0.0259 - val_loss: 2.9088e-04\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.00001\n",
      "Epoch 2/100\n",
      " - 0s - loss: 8.9002e-05 - val_loss: 1.9947e-05\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.00001\n",
      "Epoch 3/100\n",
      " - 0s - loss: 2.1234e-05 - val_loss: 2.0996e-05\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.00001\n",
      "Epoch 4/100\n",
      " - 0s - loss: 1.9577e-05 - val_loss: 1.3931e-05\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.00001\n",
      "Epoch 5/100\n",
      " - 0s - loss: 1.7658e-05 - val_loss: 1.4032e-05\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.00001\n",
      "Epoch 6/100\n",
      " - 0s - loss: 1.8563e-05 - val_loss: 1.4239e-05\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.00001\n",
      "Epoch 00006: early stopping\n",
      "4\n",
      "Train on 3074 samples, validate on 1318 samples\n",
      "Epoch 1/100\n",
      " - 2s - loss: 0.0188 - val_loss: 5.1489e-04\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.00001\n",
      "Epoch 2/100\n",
      " - 0s - loss: 2.1466e-04 - val_loss: 4.4445e-05\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.00001\n",
      "Epoch 3/100\n",
      " - 0s - loss: 2.5643e-05 - val_loss: 4.8058e-05\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.00001\n",
      "Epoch 4/100\n",
      " - 0s - loss: 2.3546e-05 - val_loss: 1.5577e-05\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.00001\n",
      "Epoch 5/100\n",
      " - 0s - loss: 1.8340e-05 - val_loss: 9.8518e-05\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.00001\n",
      "Epoch 6/100\n",
      " - 0s - loss: 4.4543e-05 - val_loss: 6.3385e-05\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.00001\n",
      "Epoch 00006: early stopping\n",
      "5\n",
      "Train on 3074 samples, validate on 1318 samples\n",
      "Epoch 1/100\n",
      " - 2s - loss: 0.0105 - val_loss: 8.1257e-05\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.00001\n",
      "Epoch 2/100\n",
      " - 0s - loss: 2.5599e-05 - val_loss: 2.3190e-05\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.00001\n",
      "Epoch 3/100\n",
      " - 0s - loss: 1.4140e-05 - val_loss: 1.2993e-05\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.00001 to 0.00001, saving model to ./best_weights_relu_3l.hdf5\n",
      "Epoch 4/100\n",
      " - 0s - loss: 2.0343e-05 - val_loss: 3.0095e-05\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.00001\n",
      "Epoch 5/100\n",
      " - 0s - loss: 1.5080e-05 - val_loss: 1.1970e-05\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.00001 to 0.00001, saving model to ./best_weights_relu_3l.hdf5\n",
      "Epoch 6/100\n",
      " - 0s - loss: 1.7656e-05 - val_loss: 1.7729e-05\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.00001\n",
      "Epoch 00006: early stopping\n",
      "6\n",
      "Train on 3074 samples, validate on 1318 samples\n",
      "Epoch 1/100\n",
      " - 2s - loss: 0.0058 - val_loss: 1.8892e-05\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.00001\n",
      "Epoch 2/100\n",
      " - 0s - loss: 1.5997e-05 - val_loss: 1.7650e-05\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.00001\n",
      "Epoch 3/100\n",
      " - 0s - loss: 1.6593e-05 - val_loss: 1.7103e-05\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.00001\n",
      "Epoch 4/100\n",
      " - 0s - loss: 1.5826e-05 - val_loss: 1.4202e-05\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.00001\n",
      "Epoch 5/100\n",
      " - 0s - loss: 2.2185e-05 - val_loss: 3.4909e-05\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.00001\n",
      "Epoch 6/100\n",
      " - 0s - loss: 2.8060e-05 - val_loss: 4.3272e-05\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.00001\n",
      "Epoch 00006: early stopping\n",
      "7\n",
      "Train on 3074 samples, validate on 1318 samples\n",
      "Epoch 1/100\n",
      " - 3s - loss: 0.0096 - val_loss: 4.9494e-05\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.00001\n",
      "Epoch 2/100\n",
      " - 0s - loss: 1.7102e-05 - val_loss: 3.6717e-05\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.00001\n",
      "Epoch 3/100\n",
      " - 0s - loss: 1.4523e-05 - val_loss: 1.1212e-05\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.00001 to 0.00001, saving model to ./best_weights_relu_3l.hdf5\n",
      "Epoch 4/100\n",
      " - 0s - loss: 1.6061e-05 - val_loss: 1.7181e-05\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.00001\n",
      "Epoch 5/100\n",
      " - 0s - loss: 1.7823e-05 - val_loss: 1.6094e-05\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.00001\n",
      "Epoch 6/100\n",
      " - 0s - loss: 2.1394e-05 - val_loss: 2.1162e-05\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.00001\n",
      "Epoch 00006: early stopping\n",
      "8\n",
      "Train on 3074 samples, validate on 1318 samples\n",
      "Epoch 1/100\n",
      " - 3s - loss: 0.0415 - val_loss: 0.0012\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.00001\n",
      "Epoch 2/100\n",
      " - 0s - loss: 3.4469e-04 - val_loss: 5.7576e-05\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.00001\n",
      "Epoch 3/100\n",
      " - 0s - loss: 1.8654e-04 - val_loss: 3.1744e-05\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.00001\n",
      "Epoch 4/100\n",
      " - 1s - loss: 3.0211e-05 - val_loss: 3.8364e-05\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.00001\n",
      "Epoch 5/100\n",
      " - 0s - loss: 1.0093e-04 - val_loss: 1.6035e-05\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.00001\n",
      "Epoch 6/100\n",
      " - 0s - loss: 2.5848e-05 - val_loss: 5.6635e-05\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.00001\n",
      "Epoch 7/100\n",
      " - 0s - loss: 7.2805e-05 - val_loss: 1.7225e-05\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.00001\n",
      "Epoch 00007: early stopping\n",
      "9\n",
      "Train on 3074 samples, validate on 1318 samples\n",
      "Epoch 1/100\n",
      " - 3s - loss: 0.0064 - val_loss: 5.9207e-05\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.00001\n",
      "Epoch 2/100\n",
      " - 0s - loss: 3.9255e-05 - val_loss: 2.3463e-05\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.00001\n",
      "Epoch 3/100\n",
      " - 0s - loss: 3.8086e-05 - val_loss: 2.0301e-05\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.00001\n",
      "Epoch 4/100\n",
      " - 0s - loss: 3.7835e-05 - val_loss: 1.6737e-05\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.00001\n",
      "Epoch 5/100\n",
      " - 0s - loss: 2.0324e-05 - val_loss: 4.4009e-05\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.00001\n",
      "Epoch 6/100\n",
      " - 0s - loss: 2.4640e-05 - val_loss: 2.0421e-05\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.00001\n",
      "Epoch 00006: early stopping\n",
      "Training finished...Loading the best model\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "You are trying to load a weight file containing 35 layers into a model with 0 layers.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-33-95645bd5b417>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Training finished...Loading the best model'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m \u001b[0mmodel_reg_relu\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'./best_weights_relu_3l.hdf5'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\network.py\u001b[0m in \u001b[0;36mload_weights\u001b[1;34m(self, filepath, by_name, skip_mismatch, reshape)\u001b[0m\n\u001b[0;32m   1159\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1160\u001b[0m                 saving.load_weights_from_hdf5_group(\n\u001b[1;32m-> 1161\u001b[1;33m                     f, self.layers, reshape=reshape)\n\u001b[0m\u001b[0;32m   1162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1163\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_updated_config\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\saving.py\u001b[0m in \u001b[0;36mload_weights_from_hdf5_group\u001b[1;34m(f, layers, reshape)\u001b[0m\n\u001b[0;32m    898\u001b[0m                          \u001b[1;34m'containing '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayer_names\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    899\u001b[0m                          \u001b[1;34m' layers into a model with '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 900\u001b[1;33m                          str(len(filtered_layers)) + ' layers.')\n\u001b[0m\u001b[0;32m    901\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    902\u001b[0m     \u001b[1;31m# We batch weight value assignments in a single backend call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: You are trying to load a weight file containing 35 layers into a model with 0 layers."
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(i)\n",
    "    \n",
    "    # Build network\n",
    "    model_reg_relu = Sequential()\n",
    "\n",
    "    model_relu.add(Dense(60, input_dim=x_train.shape[1], activation='relu'))  \n",
    "    model_relu.add(Dense(30, activation='relu')) # Hidden 2\n",
    "    model_relu.add(Dense(10, activation='relu')) # Hidden 3\n",
    "    model_relu.add(Dense(1)) # Output\n",
    "    model_relu.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    monitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=5, verbose=1, mode='auto')\n",
    "    model_relu.fit(x_train,y_train,validation_data=(x_test,y_test),callbacks=[monitor,checkpointer_relu],verbose=2,epochs=100) \n",
    "    \n",
    "print('Training finished...Loading the best model') \n",
    "print()\n",
    "model_reg_relu.load_weights('./best_weights_relu_3l.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Predict stars\n",
    "pred_hl_3 = model_relu.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final score (RMSE): 0.004518930334597826\n",
      "R2 score: 1.00\n"
     ]
    }
   ],
   "source": [
    "# Measure RMSE error.  RMSE is common for regression.\n",
    "score_3l = np.sqrt(mean_squared_error(y_test,pred_hl_3))\n",
    "print(\"Final score (RMSE): {}\".format(score_3l))\n",
    "print('R2 score: %.2f' % r2_score(y_test, pred_hl_3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### ReLU, adam, 4 layer, early stopping and Model checkpoint  - score_relu_4l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# set up checkpointer\n",
    "checkpointer_relu = ModelCheckpoint(filepath=\"./best_weights_relu_4l.hdf5\", verbose=1, save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Train on 3074 samples, validate on 1318 samples\n",
      "Epoch 1/100\n",
      " - 2s - loss: 0.0099 - val_loss: 1.8032e-05\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.00002, saving model to ./best_weights_relu_4l.hdf5\n",
      "Epoch 2/100\n",
      " - 0s - loss: 1.5572e-05 - val_loss: 1.4469e-05\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.00002 to 0.00001, saving model to ./best_weights_relu_4l.hdf5\n",
      "Epoch 3/100\n",
      " - 0s - loss: 1.4089e-05 - val_loss: 1.4323e-05\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.00001 to 0.00001, saving model to ./best_weights_relu_4l.hdf5\n",
      "Epoch 4/100\n",
      " - 0s - loss: 1.5336e-05 - val_loss: 2.0596e-05\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.00001\n",
      "Epoch 5/100\n",
      " - 0s - loss: 1.5336e-05 - val_loss: 2.8708e-05\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.00001\n",
      "Epoch 6/100\n",
      " - 0s - loss: 1.3487e-05 - val_loss: 1.0825e-05\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.00001 to 0.00001, saving model to ./best_weights_relu_4l.hdf5\n",
      "Epoch 00006: early stopping\n",
      "1\n",
      "Train on 3074 samples, validate on 1318 samples\n",
      "Epoch 1/100\n",
      " - 2s - loss: 0.0046 - val_loss: 2.2265e-05\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.00001\n",
      "Epoch 2/100\n",
      " - 0s - loss: 1.9163e-05 - val_loss: 1.7765e-05\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.00001\n",
      "Epoch 3/100\n",
      " - 0s - loss: 1.6033e-05 - val_loss: 1.3557e-05\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.00001\n",
      "Epoch 4/100\n",
      " - 0s - loss: 1.5811e-05 - val_loss: 1.4667e-05\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.00001\n",
      "Epoch 5/100\n",
      " - 0s - loss: 1.4347e-05 - val_loss: 1.2214e-05\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.00001\n",
      "Epoch 6/100\n",
      " - 0s - loss: 1.5405e-05 - val_loss: 1.5293e-05\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.00001\n",
      "Epoch 00006: early stopping\n",
      "2\n",
      "Train on 3074 samples, validate on 1318 samples\n",
      "Epoch 1/100\n",
      " - 2s - loss: 0.0069 - val_loss: 2.3907e-05\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.00001\n",
      "Epoch 2/100\n",
      " - 0s - loss: 1.9082e-05 - val_loss: 1.3323e-05\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.00001\n",
      "Epoch 3/100\n",
      " - 0s - loss: 1.3370e-05 - val_loss: 1.3787e-05\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.00001\n",
      "Epoch 4/100\n",
      " - 0s - loss: 1.3197e-05 - val_loss: 1.0805e-05\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.00001 to 0.00001, saving model to ./best_weights_relu_4l.hdf5\n",
      "Epoch 5/100\n",
      " - 0s - loss: 1.2046e-05 - val_loss: 1.0824e-05\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.00001\n",
      "Epoch 6/100\n",
      " - 0s - loss: 1.1518e-05 - val_loss: 1.0275e-05\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.00001 to 0.00001, saving model to ./best_weights_relu_4l.hdf5\n",
      "Epoch 00006: early stopping\n",
      "3\n",
      "Train on 3074 samples, validate on 1318 samples\n",
      "Epoch 1/100\n",
      " - 2s - loss: 0.0064 - val_loss: 2.5835e-05\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.00001\n",
      "Epoch 2/100\n",
      " - 0s - loss: 2.1780e-05 - val_loss: 1.4993e-05\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.00001\n",
      "Epoch 3/100\n",
      " - 0s - loss: 1.5287e-05 - val_loss: 1.3192e-05\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.00001\n",
      "Epoch 4/100\n",
      " - 0s - loss: 1.3447e-05 - val_loss: 1.1846e-05\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.00001\n",
      "Epoch 5/100\n",
      " - 0s - loss: 1.3741e-05 - val_loss: 1.5074e-05\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.00001\n",
      "Epoch 6/100\n",
      " - 0s - loss: 1.2696e-05 - val_loss: 1.0198e-05\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.00001 to 0.00001, saving model to ./best_weights_relu_4l.hdf5\n",
      "Epoch 00006: early stopping\n",
      "4\n",
      "Train on 3074 samples, validate on 1318 samples\n",
      "Epoch 1/100\n",
      " - 2s - loss: 0.0077 - val_loss: 2.8838e-05\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.00001\n",
      "Epoch 2/100\n",
      " - 0s - loss: 2.0400e-05 - val_loss: 1.5925e-05\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.00001\n",
      "Epoch 3/100\n",
      " - 0s - loss: 1.6095e-05 - val_loss: 1.6789e-05\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.00001\n",
      "Epoch 4/100\n",
      " - 0s - loss: 1.5106e-05 - val_loss: 1.3030e-05\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.00001\n",
      "Epoch 5/100\n",
      " - 0s - loss: 1.6038e-05 - val_loss: 1.3965e-05\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.00001\n",
      "Epoch 6/100\n",
      " - 0s - loss: 1.4740e-05 - val_loss: 1.2627e-05\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.00001\n",
      "Epoch 00006: early stopping\n",
      "5\n",
      "Train on 3074 samples, validate on 1318 samples\n",
      "Epoch 1/100\n",
      " - 2s - loss: 0.0085 - val_loss: 2.9864e-05\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.00001\n",
      "Epoch 2/100\n",
      " - 0s - loss: 3.2390e-05 - val_loss: 1.4236e-05\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.00001\n",
      "Epoch 3/100\n",
      " - 0s - loss: 1.5850e-05 - val_loss: 1.1900e-05\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.00001\n",
      "Epoch 4/100\n",
      " - 0s - loss: 1.3572e-05 - val_loss: 1.0710e-05\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.00001\n",
      "Epoch 5/100\n",
      " - 0s - loss: 1.4023e-05 - val_loss: 1.0612e-05\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.00001\n",
      "Epoch 6/100\n",
      " - 0s - loss: 1.2972e-05 - val_loss: 1.0602e-05\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.00001\n",
      "Epoch 00006: early stopping\n",
      "6\n",
      "Train on 3074 samples, validate on 1318 samples\n",
      "Epoch 1/100\n",
      " - 2s - loss: 0.0104 - val_loss: 3.3883e-05\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.00001\n",
      "Epoch 2/100\n",
      " - 0s - loss: 2.0750e-05 - val_loss: 1.4003e-05\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.00001\n",
      "Epoch 3/100\n",
      " - 0s - loss: 1.6681e-05 - val_loss: 1.4368e-05\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.00001\n",
      "Epoch 4/100\n",
      " - 0s - loss: 1.7793e-05 - val_loss: 1.7051e-05\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.00001\n",
      "Epoch 5/100\n",
      " - 0s - loss: 1.5513e-05 - val_loss: 2.1039e-05\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.00001\n",
      "Epoch 6/100\n",
      " - 0s - loss: 1.8895e-05 - val_loss: 1.4300e-05\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.00001\n",
      "Epoch 00006: early stopping\n",
      "7\n",
      "Train on 3074 samples, validate on 1318 samples\n",
      "Epoch 1/100\n",
      " - 2s - loss: 0.0314 - val_loss: 7.3536e-05\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.00001\n",
      "Epoch 2/100\n",
      " - 0s - loss: 2.9678e-05 - val_loss: 1.7537e-05\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.00001\n",
      "Epoch 3/100\n",
      " - 0s - loss: 1.3878e-05 - val_loss: 9.6962e-06\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.00001 to 0.00001, saving model to ./best_weights_relu_4l.hdf5\n",
      "Epoch 4/100\n",
      " - 0s - loss: 1.3746e-05 - val_loss: 1.2843e-05\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.00001\n",
      "Epoch 5/100\n",
      " - 0s - loss: 1.4407e-05 - val_loss: 1.0277e-05\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.00001\n",
      "Epoch 6/100\n",
      " - 0s - loss: 1.1822e-05 - val_loss: 1.4883e-05\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.00001\n",
      "Epoch 00006: early stopping\n",
      "8\n",
      "Train on 3074 samples, validate on 1318 samples\n",
      "Epoch 1/100\n",
      " - 2s - loss: 0.0151 - val_loss: 2.6022e-05\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.00001\n",
      "Epoch 2/100\n",
      " - 0s - loss: 2.1186e-05 - val_loss: 1.6232e-05\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.00001\n",
      "Epoch 3/100\n",
      " - 0s - loss: 1.9162e-05 - val_loss: 1.5351e-05\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.00001\n",
      "Epoch 4/100\n",
      " - 0s - loss: 1.7052e-05 - val_loss: 2.6951e-05\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.00001\n",
      "Epoch 5/100\n",
      " - 0s - loss: 1.7948e-05 - val_loss: 1.8084e-05\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.00001\n",
      "Epoch 6/100\n",
      " - 0s - loss: 1.8478e-05 - val_loss: 1.2884e-05\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.00001\n",
      "Epoch 00006: early stopping\n",
      "9\n",
      "Train on 3074 samples, validate on 1318 samples\n",
      "Epoch 1/100\n",
      " - 2s - loss: 0.0132 - val_loss: 2.3094e-05\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.00001\n",
      "Epoch 2/100\n",
      " - 0s - loss: 1.6836e-05 - val_loss: 1.1997e-05\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.00001\n",
      "Epoch 3/100\n",
      " - 0s - loss: 1.3372e-05 - val_loss: 1.1074e-05\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.00001\n",
      "Epoch 4/100\n",
      " - 0s - loss: 1.3576e-05 - val_loss: 1.0916e-05\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.00001\n",
      "Epoch 5/100\n",
      " - 0s - loss: 1.2998e-05 - val_loss: 1.1592e-05\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.00001\n",
      "Epoch 6/100\n",
      " - 0s - loss: 1.2721e-05 - val_loss: 5.0346e-05\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.00001\n",
      "Epoch 00006: early stopping\n",
      "Training finished...Loading the best model\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "You are trying to load a weight file containing 5 layers into a model with 0 layers.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-41-eabea5ec808c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Training finished...Loading the best model'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m \u001b[0mmodel_reg_relu\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'./best_weights_relu_4l.hdf5'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\network.py\u001b[0m in \u001b[0;36mload_weights\u001b[1;34m(self, filepath, by_name, skip_mismatch, reshape)\u001b[0m\n\u001b[0;32m   1159\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1160\u001b[0m                 saving.load_weights_from_hdf5_group(\n\u001b[1;32m-> 1161\u001b[1;33m                     f, self.layers, reshape=reshape)\n\u001b[0m\u001b[0;32m   1162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1163\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_updated_config\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\saving.py\u001b[0m in \u001b[0;36mload_weights_from_hdf5_group\u001b[1;34m(f, layers, reshape)\u001b[0m\n\u001b[0;32m    898\u001b[0m                          \u001b[1;34m'containing '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayer_names\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    899\u001b[0m                          \u001b[1;34m' layers into a model with '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 900\u001b[1;33m                          str(len(filtered_layers)) + ' layers.')\n\u001b[0m\u001b[0;32m    901\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    902\u001b[0m     \u001b[1;31m# We batch weight value assignments in a single backend call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: You are trying to load a weight file containing 5 layers into a model with 0 layers."
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(i)\n",
    "    \n",
    "    # Build network\n",
    "    model_relu = Sequential()\n",
    "\n",
    "    model_relu.add(Dense(80, input_dim=x_train.shape[1], activation='relu'))  \n",
    "    model_relu.add(Dense(60, activation='relu')) # Hidden 2\n",
    "    model_relu.add(Dense(20, activation='relu')) # Hidden 3\n",
    "    model_relu.add(Dense(10, activation='relu')) # Hidden 4\n",
    "    model_relu.add(Dense(1)) # Output\n",
    "    model_relu.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    monitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=5, verbose=1, mode='auto')\n",
    "    model_relu.fit(x_train,y_train,validation_data=(x_test,y_test),callbacks=[monitor,checkpointer_relu],verbose=2,epochs=100) \n",
    "    \n",
    "print('Training finished...Loading the best model') \n",
    "print()\n",
    "model_reg_relu.load_weights('./best_weights_relu_4l.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Predict stars\n",
    "pred_hl4 = model_relu.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final score (RMSE): 0.021769022569060326\n",
      "R2 score: 0.99\n"
     ]
    }
   ],
   "source": [
    "# Measure RMSE error.  RMSE is common for regression.\n",
    "score_4l = np.sqrt(mean_squared_error(y_test,pred_hl4))\n",
    "print(\"Final score (RMSE): {}\".format(score_4l))\n",
    "print('R2 score: %.2f' % r2_score(y_test, pred_hl4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### ReLU, adam, 4 layer and dropout - score_relu_4l_do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3074 samples, validate on 1318 samples\n",
      "Epoch 1/100\n",
      " - 4s - loss: 0.0069 - val_loss: 1.1118e-04\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.00002\n",
      "Epoch 2/100\n",
      " - 0s - loss: 4.9394e-04 - val_loss: 5.0593e-05\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.00002\n",
      "Epoch 3/100\n",
      " - 0s - loss: 3.9193e-04 - val_loss: 1.8051e-04\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.00002\n",
      "Epoch 4/100\n",
      " - 0s - loss: 3.1749e-04 - val_loss: 1.5043e-04\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.00002\n",
      "Epoch 5/100\n",
      " - 0s - loss: 2.5651e-04 - val_loss: 4.7390e-04\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.00002\n",
      "Epoch 6/100\n",
      " - 0s - loss: 1.9608e-04 - val_loss: 5.0287e-04\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.00002\n",
      "Epoch 00006: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1b39621bc18>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    # Build network\n",
    "    model_relu = Sequential()\n",
    "\n",
    "    model_relu.add(Dense(80, input_dim=x_train.shape[1]))  \n",
    "    model_relu.add(Dropout(0.1))\n",
    "    model_relu.add(Dense(60, activation='relu')) # Hidden 2\n",
    "    model_relu.add(Dense(20, activation='relu')) # Hidden 3\n",
    "    model_relu.add(Dense(10, activation='relu')) # Hidden 4\n",
    "    model_relu.add(Dense(1)) # Output\n",
    "    model_relu.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    monitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=5, verbose=1, mode='auto')\n",
    "    model_relu.fit(x_train,y_train,validation_data=(x_test,y_test),callbacks=[monitor,checkpointer_relu],verbose=2,epochs=100) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Predict stars\n",
    "pred_hl4_do = model_relu.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final score (RMSE): 0.022424694150686264\n",
      "R2 score: 0.99\n"
     ]
    }
   ],
   "source": [
    "# Measure RMSE error.  RMSE is common for regression.\n",
    "score_4l_do = np.sqrt(mean_squared_error(y_test,pred_hl4_do))\n",
    "print(\"Final score (RMSE): {}\".format(score_4l_do))\n",
    "print('R2 score: %.2f' % r2_score(y_test, pred_hl4_do))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### ReLU, sgd, 3 layers, early stopping and Model checkpoint  - score_relu_3l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# set up checkpointer\n",
    "checkpointer_relu = ModelCheckpoint(filepath=\"./best_weights_relu_3l_sgd.hdf5\", verbose=1, save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Train on 3074 samples, validate on 1318 samples\n",
      "Epoch 1/100\n",
      " - 16s - loss: 0.0681 - val_loss: 0.0641\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.06412, saving model to ./best_weights_relu_3l_sgd.hdf5\n",
      "Epoch 2/100\n",
      " - 2s - loss: 0.0641 - val_loss: 0.0645\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.06412\n",
      "Epoch 3/100\n",
      " - 2s - loss: 0.0641 - val_loss: 0.0645\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.06412\n",
      "Epoch 4/100\n",
      " - 2s - loss: 0.0641 - val_loss: 0.0641\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.06412\n",
      "Epoch 5/100\n",
      " - 2s - loss: 0.0641 - val_loss: 0.0641\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.06412 to 0.06412, saving model to ./best_weights_relu_3l_sgd.hdf5\n",
      "Epoch 6/100\n",
      " - 2s - loss: 0.0641 - val_loss: 0.0641\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.06412\n",
      "Epoch 00006: early stopping\n",
      "1\n",
      "Train on 3074 samples, validate on 1318 samples\n",
      "Epoch 1/100\n",
      " - 18s - loss: 0.0721 - val_loss: 0.0641\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.06412\n",
      "Epoch 2/100\n",
      " - 2s - loss: 0.0641 - val_loss: 0.0642\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.06412\n",
      "Epoch 3/100\n",
      " - 2s - loss: 0.0641 - val_loss: 0.0643\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.06412\n",
      "Epoch 4/100\n",
      " - 2s - loss: 0.0641 - val_loss: 0.0642\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.06412\n",
      "Epoch 5/100\n",
      " - 2s - loss: 0.0641 - val_loss: 0.0643\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.06412\n",
      "Epoch 6/100\n",
      " - 2s - loss: 0.0641 - val_loss: 0.0646\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.06412\n",
      "Epoch 00006: early stopping\n",
      "2\n",
      "Train on 3074 samples, validate on 1318 samples\n",
      "Epoch 1/100\n",
      " - 17s - loss: 0.0690 - val_loss: 0.0645\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.06412\n",
      "Epoch 2/100\n",
      " - 2s - loss: 0.0641 - val_loss: 0.0641\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.06412\n",
      "Epoch 3/100\n",
      " - 2s - loss: 0.0641 - val_loss: 0.0642\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.06412\n",
      "Epoch 4/100\n",
      " - 2s - loss: 0.0641 - val_loss: 0.0650\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.06412\n",
      "Epoch 5/100\n",
      " - 2s - loss: 0.0642 - val_loss: 0.0642\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.06412\n",
      "Epoch 6/100\n",
      " - 2s - loss: 0.0641 - val_loss: 0.0650\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.06412\n",
      "Epoch 00006: early stopping\n",
      "3\n",
      "Train on 3074 samples, validate on 1318 samples\n",
      "Epoch 1/100\n",
      " - 20s - loss: 0.0685 - val_loss: 0.0642\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.06412\n",
      "Epoch 2/100\n",
      " - 2s - loss: 0.0642 - val_loss: 0.0653\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.06412\n",
      "Epoch 3/100\n",
      " - 2s - loss: 0.0642 - val_loss: 0.0643\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.06412\n",
      "Epoch 4/100\n",
      " - 2s - loss: 0.0641 - val_loss: 0.0642\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.06412\n",
      "Epoch 5/100\n",
      " - 2s - loss: 0.0641 - val_loss: 0.0642\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.06412\n",
      "Epoch 6/100\n",
      " - 2s - loss: 0.0641 - val_loss: 0.0645\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.06412\n",
      "Epoch 00006: early stopping\n",
      "4\n",
      "Train on 3074 samples, validate on 1318 samples\n",
      "Epoch 1/100\n",
      " - 19s - loss: 0.0702 - val_loss: 0.0642\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.06412\n",
      "Epoch 2/100\n",
      " - 2s - loss: 0.0641 - val_loss: 0.0642\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.06412\n",
      "Epoch 3/100\n",
      " - 2s - loss: 0.0641 - val_loss: 0.0641\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.06412\n",
      "Epoch 4/100\n",
      " - 2s - loss: 0.0641 - val_loss: 0.0649\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.06412\n",
      "Epoch 5/100\n",
      " - 2s - loss: 0.0641 - val_loss: 0.0644\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.06412\n",
      "Epoch 6/100\n",
      " - 2s - loss: 0.0641 - val_loss: 0.0642\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.06412\n",
      "Epoch 00006: early stopping\n",
      "5\n",
      "Train on 3074 samples, validate on 1318 samples\n",
      "Epoch 1/100\n",
      " - 19s - loss: 0.0686 - val_loss: 0.0641\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.06412\n",
      "Epoch 2/100\n",
      " - 2s - loss: 0.0642 - val_loss: 0.0642\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.06412\n",
      "Epoch 3/100\n",
      " - 2s - loss: 0.0642 - val_loss: 0.0643\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.06412\n",
      "Epoch 4/100\n",
      " - 2s - loss: 0.0642 - val_loss: 0.0642\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.06412\n",
      "Epoch 5/100\n",
      " - 2s - loss: 0.0642 - val_loss: 0.0643\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.06412\n",
      "Epoch 6/100\n",
      " - 2s - loss: 0.0641 - val_loss: 0.0661\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.06412\n",
      "Epoch 00006: early stopping\n",
      "6\n",
      "Train on 3074 samples, validate on 1318 samples\n",
      "Epoch 1/100\n",
      " - 22s - loss: 0.0738 - val_loss: 0.0643\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.06412\n",
      "Epoch 2/100\n",
      " - 2s - loss: 0.0641 - val_loss: 0.0643\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.06412\n",
      "Epoch 3/100\n",
      " - 2s - loss: 0.0641 - val_loss: 0.0645\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.06412\n",
      "Epoch 4/100\n",
      " - 2s - loss: 0.0641 - val_loss: 0.0642\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.06412\n",
      "Epoch 5/100\n",
      " - 2s - loss: 0.0641 - val_loss: 0.0642\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.06412\n",
      "Epoch 6/100\n",
      " - 2s - loss: 0.0641 - val_loss: 0.0642\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.06412\n",
      "Epoch 00006: early stopping\n",
      "7\n",
      "Train on 3074 samples, validate on 1318 samples\n",
      "Epoch 1/100\n",
      " - 22s - loss: 0.0732 - val_loss: 0.0642\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.06412\n",
      "Epoch 2/100\n",
      " - 2s - loss: 0.0641 - val_loss: 0.0642\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.06412\n",
      "Epoch 3/100\n",
      " - 2s - loss: 0.0641 - val_loss: 0.0643\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.06412\n",
      "Epoch 4/100\n",
      " - 2s - loss: 0.0641 - val_loss: 0.0642\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.06412\n",
      "Epoch 5/100\n",
      " - 2s - loss: 0.0641 - val_loss: 0.0645\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.06412\n",
      "Epoch 6/100\n",
      " - 2s - loss: 0.0641 - val_loss: 0.0642\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.06412\n",
      "Epoch 00006: early stopping\n",
      "8\n",
      "Train on 3074 samples, validate on 1318 samples\n",
      "Epoch 1/100\n",
      " - 23s - loss: 0.0689 - val_loss: 0.0641\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.06412\n",
      "Epoch 2/100\n",
      " - 2s - loss: 0.0641 - val_loss: 0.0642\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.06412\n",
      "Epoch 3/100\n",
      " - 2s - loss: 0.0641 - val_loss: 0.0642\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.06412\n",
      "Epoch 4/100\n",
      " - 2s - loss: 0.0641 - val_loss: 0.0644\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.06412\n",
      "Epoch 5/100\n",
      " - 3s - loss: 0.0641 - val_loss: 0.0645\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.06412\n",
      "Epoch 6/100\n",
      " - 2s - loss: 0.0641 - val_loss: 0.0642\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.06412\n",
      "Epoch 00006: early stopping\n",
      "9\n",
      "Train on 3074 samples, validate on 1318 samples\n",
      "Epoch 1/100\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(i)\n",
    "    \n",
    "    # Build network\n",
    "    model_reg_relu = Sequential()\n",
    "\n",
    "    model_relu.add(Dense(60, input_dim=x_train.shape[1], activation='relu'))  \n",
    "    model_relu.add(Dense(30, activation='relu')) # Hidden 2\n",
    "    model_relu.add(Dense(10, activation='relu')) # Hidden 3\n",
    "    model_relu.add(Dense(1)) # Output\n",
    "    model_relu.compile(loss='mean_squared_error', optimizer='sgd')\n",
    "    monitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=5, verbose=1, mode='auto')\n",
    "    model_relu.fit(x_train,y_train,validation_data=(x_test,y_test),callbacks=[monitor,checkpointer_relu],verbose=2,epochs=100) \n",
    "    \n",
    "print('Training finished...Loading the best model') \n",
    "print()\n",
    "model_reg_relu.load_weights('./best_weights_relu_3l_sgd.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Predict stars\n",
    "pred_relu_3l_sgd = model_relu.predict(x_test)\n",
    "print(\"Shape: {}\".format(pred_relu_3l_sgd.shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Measure RMSE error.  RMSE is common for regression.\n",
    "score_relu_3l_sgd = np.sqrt(mean_squared_error(y_test,pred_relu_3l_sgd))\n",
    "print(\"Final score (RMSE): {}\".format(score_relu_3l_sgd))\n",
    "print('R2 score: %.2f' % r2_score(y_test, pred_relu_3l_sgd))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### ReLU, sgd, 3 layer and dropout - score_relu_3l_do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "    # Build network\n",
    "    model_relu = Sequential()\n",
    "\n",
    "    model_relu.add(Dense(50, input_dim=x_train.shape[1]))  \n",
    "    model_relu.add(Dropout(0.1))\n",
    "    model_relu.add(Dense(25, activation='relu')) # Hidden 2\n",
    "    model_relu.add(Dense(10, activation='relu')) # Hidden 3\n",
    "    model_relu.add(Dense(1)) # Output\n",
    "    model_relu.compile(loss='mean_squared_error', optimizer='sgd')\n",
    "    monitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=5, verbose=1, mode='auto')\n",
    "    model_relu.fit(x_train,y_train,validation_data=(x_test,y_test),callbacks=[monitor,checkpointer_relu],verbose=2,epochs=100) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Predict stars\n",
    "pred_hl3_do = model_relu.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Measure RMSE error.  RMSE is common for regression.\n",
    "score_3l_do = np.sqrt(mean_squared_error(y_test,pred_hl3_do))\n",
    "print(\"Final score (RMSE): {}\".format(score_3l_do))\n",
    "print('R2 score: %.2f' % r2_score(y_test, pred_hl3_do))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### ReLU, rmsprop, 3 layers, early stopping and Model checkpoint  - score_relu_3l_rms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# set up checkpointer\n",
    "checkpointer_relu = ModelCheckpoint(filepath=\"./best_weights_relu_3l_rms.hdf5\", verbose=1, save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    print(i)\n",
    "    \n",
    "    # Build network\n",
    "    model_reg_relu = Sequential()\n",
    "\n",
    "    model_relu.add(Dense(60, input_dim=x_train.shape[1], activation='relu'))  \n",
    "    model_relu.add(Dense(30, activation='relu')) # Hidden 2\n",
    "    model_relu.add(Dense(10, activation='relu')) # Hidden 3\n",
    "    model_relu.add(Dense(1)) # Output\n",
    "    model_relu.compile(loss='mean_squared_error', optimizer='rmsprop')\n",
    "    monitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=5, verbose=1, mode='auto')\n",
    "    model_relu.fit(x_train,y_train,validation_data=(x_test,y_test),callbacks=[monitor,checkpointer_relu],verbose=2,epochs=100) \n",
    "    \n",
    "print('Training finished...Loading the best model') \n",
    "print()\n",
    "model_reg_relu.load_weights('./best_weights_relu_3l_rms.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Predict stars\n",
    "pred_relu_3l_rms = model_relu.predict(x_test)\n",
    "print(\"Shape: {}\".format(pred_relu_3l_rms.shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Measure RMSE error.  RMSE is common for regression.\n",
    "score_relu_3l_rms = np.sqrt(mean_squared_error(y_test,pred_relu_3l_rms))\n",
    "print(\"Final score (RMSE): {}\".format(score_relu_3l_rms))\n",
    "print('R2 score: %.2f' % r2_score(y_test, pred_relu_3l_rms))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### ReLU, rmsprop, 3 layer and dropout - score_relu_3l_do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "    # Build network\n",
    "    model_relu = Sequential()\n",
    "\n",
    "    model_relu.add(Dense(50, input_dim=x_train.shape[1]))  \n",
    "    model_relu.add(Dropout(0.1))\n",
    "    model_relu.add(Dense(25, activation='relu')) # Hidden 2\n",
    "    model_relu.add(Dense(10, activation='relu')) # Hidden 3\n",
    "    model_relu.add(Dense(1)) # Output\n",
    "    model_relu.compile(loss='mean_squared_error', optimizer='rmsprop')\n",
    "    monitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=5, verbose=1, mode='auto')\n",
    "    model_relu.fit(x_train,y_train,validation_data=(x_test,y_test),callbacks=[monitor,checkpointer_relu],verbose=2,epochs=100) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Predict stars\n",
    "pred_hl3_rms_do = model_relu.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Measure RMSE error.  RMSE is common for regression.\n",
    "score_3l_rms_do = np.sqrt(mean_squared_error(y_test,pred_hl3_rms_do))\n",
    "print(\"Final score (RMSE): {}\".format(score_3l_rms_do))\n",
    "print('R2 score: %.2f' % r2_score(y_test, pred_hl3_rms_do))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
