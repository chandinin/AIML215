{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sci\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras import regularizers\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.layers import Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import io\n",
    "import requests\n",
    "import tensorflow as tf\n",
    "from scipy import sparse\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sklearn.feature_extraction.text as sk_text\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.metrics import accuracy_score, precision_score, f1_score\n",
    "import collections\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "import shutil\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data PreProcessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read network_intrusion_data.csv file and load data into network_df dataframe \n",
    "network_df= pd.read_csv('network_intrusion_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop any row with missing values\n",
    "network_df = network_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add column headers to the data in the dataframe\n",
    "network_df.columns = [\n",
    "'duration',\n",
    "'protocol_type',\n",
    "'service',\n",
    "'flag',\n",
    "'src_bytes',\n",
    "'dst_bytes',\n",
    "'land',\n",
    "'wrong_fragment',\n",
    "'urgent',\n",
    "'hot',\n",
    "'num_failed_logins',\n",
    "'logged_in',\n",
    "'num_compromised',\n",
    "'root_shell',\n",
    "'su_attempted',\n",
    "'num_root',\n",
    "'num_file_creations',\n",
    "'num_shells',\n",
    "'num_access_files',\n",
    "'num_outbound_cmds',\n",
    "'is_host_login',\n",
    "'is_guest_login',\n",
    "'count',\n",
    "'srv_count',\n",
    "'serror_rate',\n",
    "'srv_serror_rate',\n",
    "'rerror_rate',\n",
    "'srv_rerror_rate',\n",
    "'same_srv_rate',\n",
    "'diff_srv_rate',\n",
    "'srv_diff_host_rate',\n",
    "'dst_host_count',\n",
    "'dst_host_srv_count',\n",
    "'dst_host_same_srv_rate',\n",
    "'dst_host_diff_srv_rate',\n",
    "'dst_host_same_src_port_rate',\n",
    "'dst_host_srv_diff_host_rate',\n",
    "'dst_host_serror_rate',\n",
    "'dst_host_srv_serror_rate',\n",
    "'dst_host_rerror_rate',\n",
    "'dst_host_srv_rerror_rate',\n",
    "'outcome'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select only relevant columns for processing\n",
    "features_df = network_df[['duration',\n",
    "'protocol_type',\n",
    "'service',\n",
    "'flag',\n",
    "'src_bytes',\n",
    "'dst_bytes',\n",
    "'land',\n",
    "'wrong_fragment',\n",
    "'urgent',\n",
    "                          \n",
    "'hot',\n",
    "'num_failed_logins',\n",
    "'logged_in',\n",
    "'num_compromised',\n",
    "'root_shell',\n",
    "'su_attempted',\n",
    "'num_root',\n",
    "'num_file_creations',\n",
    "'num_shells',\n",
    "'num_access_files',\n",
    "'num_outbound_cmds',\n",
    "'is_host_login',\n",
    "'is_guest_login',\n",
    "                          \n",
    "'count',\n",
    "'srv_count',\n",
    "'serror_rate',\n",
    "'srv_serror_rate',\n",
    "'rerror_rate',\n",
    "'srv_rerror_rate',\n",
    "'same_srv_rate',\n",
    "'diff_srv_rate',\n",
    "'srv_diff_host_rate',\n",
    "'dst_host_count',\n",
    "'dst_host_srv_count',\n",
    "'dst_host_same_srv_rate',\n",
    "'dst_host_diff_srv_rate',\n",
    "'dst_host_same_src_port_rate',\n",
    "'dst_host_srv_diff_host_rate',\n",
    "'dst_host_serror_rate',\n",
    "'dst_host_srv_serror_rate',\n",
    "'dst_host_rerror_rate',\n",
    "'dst_host_srv_rerror_rate']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_df = network_df[['outcome']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ChandiniNagendra\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"\n",
      "C:\\Users\\ChandiniNagendra\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "#Normalize numeric features\n",
    "\n",
    "def normalize_numeric_minmax(df, name):\n",
    "    if(df[name].max() > 0):\n",
    "        df[name] = ((df[name] - df[name].min()) / (df[name].max() - df[name].min())).astype(np.float32)\n",
    "    else:\n",
    "        df[name] = df[name].astype(np.float32)\n",
    "    \n",
    "normalize_numeric_minmax(features_df,\"duration\") \n",
    "normalize_numeric_minmax(features_df,\"src_bytes\") \n",
    "normalize_numeric_minmax(features_df,\"dst_bytes\") \n",
    "normalize_numeric_minmax(features_df,\"wrong_fragment\") \n",
    "normalize_numeric_minmax(features_df,\"urgent\") \n",
    "\n",
    "normalize_numeric_minmax(features_df,\"hot\") \n",
    "normalize_numeric_minmax(features_df,\"num_failed_logins\") \n",
    "normalize_numeric_minmax(features_df,\"num_compromised\") \n",
    "normalize_numeric_minmax(features_df,\"num_root\") \n",
    "normalize_numeric_minmax(features_df,\"num_file_creations\") \n",
    "normalize_numeric_minmax(features_df,\"num_shells\") \n",
    "normalize_numeric_minmax(features_df,\"num_access_files\") \n",
    "normalize_numeric_minmax(features_df,\"num_outbound_cmds\") \n",
    "\n",
    "normalize_numeric_minmax(features_df,\"count\") \n",
    "normalize_numeric_minmax(features_df,\"srv_count\") \n",
    "normalize_numeric_minmax(features_df,\"serror_rate\") \n",
    "normalize_numeric_minmax(features_df,\"srv_serror_rate\") \n",
    "normalize_numeric_minmax(features_df,\"rerror_rate\") \n",
    "normalize_numeric_minmax(features_df,\"srv_rerror_rate\")  \n",
    "normalize_numeric_minmax(features_df,\"same_srv_rate\") \n",
    "normalize_numeric_minmax(features_df,\"diff_srv_rate\") \n",
    "normalize_numeric_minmax(features_df,\"srv_diff_host_rate\") \n",
    "normalize_numeric_minmax(features_df,\"dst_host_count\") \n",
    "normalize_numeric_minmax(features_df,\"dst_host_srv_count\") \n",
    "normalize_numeric_minmax(features_df,\"dst_host_same_srv_rate\") \n",
    "normalize_numeric_minmax(features_df,\"dst_host_diff_srv_rate\") \n",
    "normalize_numeric_minmax(features_df,\"dst_host_same_src_port_rate\") \n",
    "normalize_numeric_minmax(features_df,\"dst_host_srv_diff_host_rate\") \n",
    "normalize_numeric_minmax(features_df,\"dst_host_serror_rate\") \n",
    "normalize_numeric_minmax(features_df,\"dst_host_srv_serror_rate\") \n",
    "normalize_numeric_minmax(features_df,\"dst_host_rerror_rate\") \n",
    "normalize_numeric_minmax(features_df,\"dst_host_srv_rerror_rate\") \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-hot cooding of categorical columns\n",
    "\n",
    "def encode_text_dummy(df, name):\n",
    "    dummies = pd.get_dummies(df[name]).astype(np.float32)\n",
    "    for x in dummies.columns:\n",
    "        dummy_name = \"{}-{}\".format(name, x)\n",
    "        df[dummy_name] = dummies[x]\n",
    "    df.drop(name, axis=1, inplace=True)\n",
    "\n",
    "encode_text_dummy(features_df,\"protocol_type\") \n",
    "encode_text_dummy(features_df,\"service\") \n",
    "encode_text_dummy(features_df,\"flag\") \n",
    "encode_text_dummy(features_df,\"land\") \n",
    "\n",
    "encode_text_dummy(features_df,\"logged_in\") \n",
    "encode_text_dummy(features_df,\"root_shell\") \n",
    "encode_text_dummy(features_df,\"su_attempted\") \n",
    "encode_text_dummy(features_df,\"is_host_login\") \n",
    "encode_text_dummy(features_df,\"is_guest_login\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df = features_df[0:10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_df = label_df[0:10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function called encodeLabelBinarizer\n",
    "\n",
    "encodeLabelBinary = lambda x: 0 if x == 'normal.' else 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ChandiniNagendra\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "label_df['outcome'] = label_df['outcome'].apply(encodeLabelBinary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(features_df, label_df['outcome'] , test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and Prediction using Regression and Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Logistic Regression **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "Log_reg_model = LogisticRegression()\n",
    "\n",
    "Log_reg_model.fit(x_train, y_train)\n",
    "\n",
    "y_pred_logistic = Log_reg_model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error: 0.07\n",
      "R2 score: 0.97\n"
     ]
    }
   ],
   "source": [
    "# RMS value\n",
    "\n",
    "score_logistic = np.sqrt(mean_squared_error(y_test, y_pred_logistic))\n",
    "print(\"Root Mean Squared Error: %.2f\" % score_logistic)\n",
    "print('R2 score: %.2f' % r2_score(y_test, y_pred_logistic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** KNN **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#implementing Nearest Neighbor\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=2)\n",
    "\n",
    "knn.fit(x_train, y_train) \n",
    "\n",
    "y_pred_knn = knn.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score: 0.998\n",
      "Precision score: 0.9980051085568328\n",
      "Recall score: 0.998\n",
      "F1 score: 0.9979966916164151\n"
     ]
    }
   ],
   "source": [
    "# Metrics\n",
    "from sklearn import metrics\n",
    "\n",
    "score_knn_acc = metrics.accuracy_score(y_test, y_pred_knn)\n",
    "print(\"Accuracy score: {}\".format(score_knn_acc))\n",
    "\n",
    "score_knn_precision = metrics.precision_score(y_test, y_pred_knn, average= \"weighted\")\n",
    "print(\"Precision score: {}\".format(score_knn_precision))\n",
    "\n",
    "score_knn_recall = metrics.recall_score(y_test, y_pred_knn, average= \"weighted\")\n",
    "print(\"Recall score: {}\".format(score_knn_recall))\n",
    "\n",
    "score_knn_f1 = metrics.f1_score(y_test, y_pred_knn, average= \"weighted\")\n",
    "print(\"F1 score: {}\".format(score_knn_f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** SVM **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "svm_model = SVC(kernel=\"linear\")\n",
    "\n",
    "svm_model.fit(x_train, y_train)\n",
    "\n",
    "y_pred_svm = svm_model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score: 0.9985\n",
      "Precision score: 0.998502875399361\n",
      "Recall score: 0.9985\n",
      "F1 score: 0.9984981422199071\n"
     ]
    }
   ],
   "source": [
    "# Metrics\n",
    "\n",
    "score_svm_acc = metrics.accuracy_score(y_test, y_pred_svm)\n",
    "print(\"Accuracy score: {}\".format(score_svm_acc))\n",
    "\n",
    "score_svm_precision = metrics.precision_score(y_test, y_pred_svm, average= \"weighted\")\n",
    "print(\"Precision score: {}\".format(score_svm_precision))\n",
    "\n",
    "score_svm_recall = metrics.recall_score(y_test, y_pred_svm, average= \"weighted\")\n",
    "print(\"Recall score: {}\".format(score_svm_recall))\n",
    "\n",
    "score_svm_f1 = metrics.f1_score(y_test, y_pred_svm, average= \"weighted\")\n",
    "print(\"F1 score: {}\".format(score_svm_f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Gaussian Naive Bayes **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GNB\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "mnb_model = GaussianNB()\n",
    "\n",
    "mnb_model.fit(x_train, y_train)\n",
    "\n",
    "y_pred_gnb = mnb_model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score: 0.996\n",
      "Precision score: 0.9960094405594406\n",
      "Recall score: 0.996\n",
      "F1 score: 0.9960032745786752\n"
     ]
    }
   ],
   "source": [
    "# Metrics\n",
    "\n",
    "score_gnb_acc = metrics.accuracy_score(y_test, y_pred_gnb)\n",
    "print(\"Accuracy score: {}\".format(score_gnb_acc))\n",
    "\n",
    "score_gnb_precision = metrics.precision_score(y_test, y_pred_gnb, average= \"weighted\")\n",
    "print(\"Precision score: {}\".format(score_gnb_precision))\n",
    "\n",
    "score_gnb_recall = metrics.recall_score(y_test, y_pred_gnb, average= \"weighted\")\n",
    "print(\"Recall score: {}\".format(score_gnb_recall))\n",
    "\n",
    "score_gnb_f1 = metrics.f1_score(y_test, y_pred_gnb, average= \"weighted\")\n",
    "print(\"F1 score: {}\".format(score_gnb_f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and Prediction using Fully Connected Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Convert a Pandas dataframe to the x,y inputs that TensorFlow needs\n",
    "import collections\n",
    "def to_xy(df, target):\n",
    "    result = []\n",
    "    for x in df.columns:\n",
    "        if x != target:\n",
    "            result.append(x)\n",
    "    # find out the type of the target column. \n",
    "    target_type = df[target].dtypes\n",
    "    target_type = target_type[0] if isinstance(target_type, collections.Sequence) else target_type\n",
    "    # Encode to int for classification, float otherwise. TensorFlow likes 32 bits.\n",
    "    if target_type in (np.int64, np.int32):\n",
    "        # Classification\n",
    "        dummies = pd.get_dummies(df[target])\n",
    "        return df[result].values.astype(np.float32), dummies.values.astype(np.float32)\n",
    "    else:\n",
    "        # Regression\n",
    "        return df[result].values.astype(np.float32), df[target].values.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y=to_xy(features_df,'outcome')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_tf, x_test_tf, y_train_tf, y_test_tf = train_test_split(x,y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8000, 124)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** ReLU, adam, 4 hidden layers **\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up checkpointer and complete this\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.models import Sequential\n",
    "\n",
    "checkpointer_relu = ModelCheckpoint(filepath=\"./best_weights_relu_4l.hdf5\", verbose=1, save_best_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and Prediction using CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 992000 into shape (8000,1,121,1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-44-f6190a92e3f2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# we now reshape the x_train and x_test to image form used in CNN 2D\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mx_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx_train_tf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train_tf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m121\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mx_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx_test_tf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test_tf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m121\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: cannot reshape array of size 992000 into shape (8000,1,121,1)"
     ]
    }
   ],
   "source": [
    "# we now reshape the x_train and x_test to image form used in CNN 2D\n",
    "x_train = x_train_tf.reshape(x_train_tf.shape[0], 1, 121, 1)\n",
    "x_test = x_test_tf.reshape(x_test_tf.shape[0], 1, 121, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CNN 2D\n",
    "\n",
    "cnn = Sequential()\n",
    "\n",
    "# Conv2D layer 1\n",
    "cnn.add(Conv2D(41, kernel_size=(1, 3), strides=(1, 1), padding='valid',\n",
    "                 activation='relu',\n",
    "                 input_shape=(1,121,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(Conv2D(64, (1, 3), activation='relu'))\n",
    "\n",
    "cnn.add(MaxPooling2D(pool_size=(1, 2), strides=None))\n",
    "\n",
    "cnn.add(Dropout(0.25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(Dense(128, activation='relu'))\n",
    "\n",
    "cnn.add(Dropout(0.5))\n",
    "\n",
    "cnn.add(Dense(2, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.fit(x_train, y_train,     \n",
    "          batch_size=128,\n",
    "          epochs=10,\n",
    "          verbose=2,\n",
    "          validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Accuracy in Keras\n",
    "score = cnn.evaluate(x_test, y_test, verbose=0)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Test loss: {}'.format(score[0]))\n",
    "print('Test accuracy: {}'.format(score[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additional Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remove Redundant Records **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network_df.drop_duplicates(keep='first', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select only relevant columns for processing\n",
    "features_ad_df = network_df[['duration',\n",
    "'protocol_type',\n",
    "'service',\n",
    "'flag',\n",
    "'src_bytes',\n",
    "'dst_bytes',\n",
    "'land',\n",
    "'wrong_fragment',\n",
    "'urgent',\n",
    "                          \n",
    "'hot',\n",
    "'num_failed_logins',\n",
    "'logged_in',\n",
    "'num_compromised',\n",
    "'root_shell',\n",
    "'su_attempted',\n",
    "'num_root',\n",
    "'num_file_creations',\n",
    "'num_shells',\n",
    "'num_access_files',\n",
    "'num_outbound_cmds',\n",
    "'is_host_login',\n",
    "'is_guest_login',\n",
    "                          \n",
    "'count',\n",
    "'srv_count',\n",
    "'serror_rate',\n",
    "'srv_serror_rate',\n",
    "'rerror_rate',\n",
    "'srv_rerror_rate',\n",
    "'same_srv_rate',\n",
    "'diff_srv_rate',\n",
    "'srv_diff_host_rate',\n",
    "'dst_host_count',\n",
    "'dst_host_srv_count',\n",
    "'dst_host_same_srv_rate',\n",
    "'dst_host_diff_srv_rate',\n",
    "'dst_host_same_src_port_rate',\n",
    "'dst_host_srv_diff_host_rate',\n",
    "'dst_host_serror_rate',\n",
    "'dst_host_srv_serror_rate',\n",
    "'dst_host_rerror_rate',\n",
    "'dst_host_srv_rerror_rate']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_ad_df = network_df[['outcome']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalize numeric features\n",
    "\n",
    "\n",
    "def normalize_numeric_minmax(df, name):\n",
    "    if(df[name].max() > 0):\n",
    "        df[name] = ((df[name] - df[name].min()) / (df[name].max() - df[name].min())).astype(np.float32)\n",
    "    else:\n",
    "        df[name] = df[name].astype(np.float32)\n",
    "    \n",
    "normalize_numeric_minmax(features_ad_df,\"duration\") \n",
    "normalize_numeric_minmax(features_ad_df,\"src_bytes\") \n",
    "normalize_numeric_minmax(features_ad_df,\"dst_bytes\") \n",
    "normalize_numeric_minmax(features_ad_df,\"wrong_fragment\") \n",
    "normalize_numeric_minmax(features_ad_df,\"urgent\") \n",
    "\n",
    "normalize_numeric_minmax(features_ad_df,\"hot\") \n",
    "normalize_numeric_minmax(features_ad_df,\"num_failed_logins\") \n",
    "normalize_numeric_minmax(features_ad_df,\"num_compromised\") \n",
    "normalize_numeric_minmax(features_ad_df,\"num_root\") \n",
    "normalize_numeric_minmax(features_ad_df,\"num_file_creations\") \n",
    "normalize_numeric_minmax(features_ad_df,\"num_shells\") \n",
    "normalize_numeric_minmax(features_ad_df,\"num_access_files\") \n",
    "normalize_numeric_minmax(features_ad_df,\"num_outbound_cmds\") \n",
    "\n",
    "normalize_numeric_minmax(features_ad_df,\"count\") \n",
    "normalize_numeric_minmax(features_ad_df,\"srv_count\") \n",
    "normalize_numeric_minmax(features_ad_df,\"serror_rate\") \n",
    "normalize_numeric_minmax(features_ad_df,\"srv_serror_rate\") \n",
    "normalize_numeric_minmax(features_ad_df,\"rerror_rate\") \n",
    "normalize_numeric_minmax(features_ad_df,\"srv_rerror_rate\") \n",
    "normalize_numeric_minmax(features_ad_df,\"same_srv_rate\") \n",
    "normalize_numeric_minmax(features_ad_df,\"diff_srv_rate\") \n",
    "normalize_numeric_minmax(features_ad_df,\"srv_diff_host_rate\") \n",
    "normalize_numeric_minmax(features_ad_df,\"dst_host_count\") \n",
    "normalize_numeric_minmax(features_ad_df,\"dst_host_srv_count\") \n",
    "normalize_numeric_minmax(features_ad_df,\"dst_host_same_srv_rate\") \n",
    "normalize_numeric_minmax(features_ad_df,\"dst_host_diff_srv_rate\") \n",
    "normalize_numeric_minmax(features_ad_df,\"dst_host_same_src_port_rate\") \n",
    "normalize_numeric_minmax(features_ad_df,\"dst_host_srv_diff_host_rate\") \n",
    "normalize_numeric_minmax(features_ad_df,\"dst_host_serror_rate\") \n",
    "normalize_numeric_minmax(features_ad_df,\"dst_host_srv_serror_rate\") \n",
    "normalize_numeric_minmax(features_ad_df,\"dst_host_rerror_rate\") \n",
    "normalize_numeric_minmax(features_ad_df,\"dst_host_srv_rerror_rate\") \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-hot cooding of categorical columns\n",
    "\n",
    "def encode_text_dummy(df, name):\n",
    "    dummies = pd.get_dummies(df[name]).astype(np.float32)\n",
    "    for x in dummies.columns:\n",
    "        dummy_name = \"{}-{}\".format(name, x)\n",
    "        df[dummy_name] = dummies[x]\n",
    "    df.drop(name, axis=1, inplace=True)\n",
    "\n",
    "encode_text_dummy(features_ad_df,\"protocol_type\") \n",
    "encode_text_dummy(features_ad_df,\"service\") \n",
    "encode_text_dummy(features_ad_df,\"flag\") \n",
    "encode_text_dummy(features_ad_df,\"land\") \n",
    "\n",
    "encode_text_dummy(features_ad_df,\"logged_in\") \n",
    "encode_text_dummy(features_ad_df,\"root_shell\") \n",
    "encode_text_dummy(features_ad_df,\"su_attempted\") \n",
    "encode_text_dummy(features_ad_df,\"is_host_login\") \n",
    "encode_text_dummy(features_ad_df,\"is_guest_login\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reduce the dataset size\n",
    "features_ad_df = features_ad_df[0:40000]\n",
    "label_ad_df = label_ad_df[0:40000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function called encodeLabelBinarizer\n",
    "\n",
    "encodeLabelBinary = lambda x: 0 if x == 'normal.' else 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_ad_df['outcome'] = label_ad_df['outcome'].apply(encodeLabelBinary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_ad_train, x_ad_test, y_ad_train, y_ad_test = train_test_split(features_ad_df, label_ad_df['outcome'] , test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_ad_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Logistic Regression **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "Log_reg_model = LogisticRegression()\n",
    "\n",
    "Log_reg_model.fit(x_ad_train, y_ad_train)\n",
    "\n",
    "y_ad_pred_logistic = Log_reg_model.predict(x_ad_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RMS value\n",
    "\n",
    "score_logistic = np.sqrt(mean_squared_error(y_ad_test, y_ad_pred_logistic))\n",
    "print(\"Root Mean Squared Error: %.2f\" % score_logistic)\n",
    "print('R2 score: %.2f' % r2_score(y_ad_test, y_ad_pred_logistic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** KNN **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#implementing Nearest Neighbor\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=2)\n",
    "\n",
    "knn.fit(x_ad_train, y_ad_train) \n",
    "\n",
    "y_ad_pred_knn = knn.predict(x_ad_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics\n",
    "from sklearn import metrics\n",
    "\n",
    "score_ad_knn_acc = metrics.accuracy_score(y_ad_test, y_ad_pred_knn)\n",
    "print(\"Accuracy score: {}\".format(score_ad_knn_acc))\n",
    "\n",
    "score_ad_knn_precision = metrics.precision_score(y_ad_test, y_ad_pred_knn, average= \"weighted\")\n",
    "print(\"Precision score: {}\".format(score_ad_knn_precision))\n",
    "\n",
    "score_ad_knn_recall = metrics.recall_score(y_ad_test, y_ad_pred_knn, average= \"weighted\")\n",
    "print(\"Recall score: {}\".format(score_ad_knn_recall))\n",
    "\n",
    "score_ad_knn_f1 = metrics.f1_score(y_ad_test, y_ad_pred_knn, average= \"weighted\")\n",
    "print(\"F1 score: {}\".format(score_ad_knn_f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** SVM **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "svm_model = SVC(kernel=\"linear\")\n",
    "\n",
    "svm_model.fit(x_ad_train, y_ad_train)\n",
    "\n",
    "y_ad_pred_svm = svm_model.predict(x_ad_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics\n",
    "\n",
    "score_ad_svm_acc = metrics.accuracy_score(y_ad_test, y_ad_pred_svm)\n",
    "print(\"Accuracy score: {}\".format(score_ad_svm_acc))\n",
    "\n",
    "score_ad_svm_precision = metrics.precision_score(y_ad_test, y_ad_pred_svm, average= \"weighted\")\n",
    "print(\"Precision score: {}\".format(score_ad_svm_precision))\n",
    "\n",
    "score_ad_svm_recall = metrics.recall_score(y_ad_test, y_ad_pred_svm, average= \"weighted\")\n",
    "print(\"Recall score: {}\".format(score_ad_svm_recall))\n",
    "\n",
    "score_ad_svm_f1 = metrics.f1_score(y_ad_test, y_ad_pred_svm, average= \"weighted\")\n",
    "print(\"F1 score: {}\".format(score_ad_svm_f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Gaussian Naive Bayes **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GNB\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "mnb_model = GaussianNB()\n",
    "\n",
    "mnb_model.fit(x_ad_train, y_ad_train)\n",
    "\n",
    "y_ad_pred_gnb = mnb_model.predict(x_ad_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics\n",
    "\n",
    "score_ad_gnb_acc = metrics.accuracy_score(y_ad_test, y_ad_pred_gnb)\n",
    "print(\"Accuracy score: {}\".format(score_ad_gnb_acc))\n",
    "\n",
    "score_ad_gnb_precision = metrics.precision_score(y_ad_test, y_ad_pred_gnb, average= \"weighted\")\n",
    "print(\"Precision score: {}\".format(score_ad_gnb_precision))\n",
    "\n",
    "score_ad_gnb_recall = metrics.recall_score(y_ad_test, y_ad_pred_gnb, average= \"weighted\")\n",
    "print(\"Recall score: {}\".format(score_ad_gnb_recall))\n",
    "\n",
    "score_ad_gnb_f1 = metrics.f1_score(y_ad_test, y_ad_pred_gnb, average= \"weighted\")\n",
    "print(\"F1 score: {}\".format(score_ad_gnb_f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Feature Importance Analysis **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read network_intrusion_data.csv file and load data into network_df dataframe \n",
    "network_ad_df= pd.read_csv('network_intrusion_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop any row with missing values\n",
    "network_ad_df = network_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add column headers to the data in the dataframe\n",
    "network_ad_df.columns = [\n",
    "'duration',\n",
    "'protocol_type',\n",
    "'service',\n",
    "'flag',\n",
    "'src_bytes',\n",
    "'dst_bytes',\n",
    "'land',\n",
    "'wrong_fragment',\n",
    "'urgent',\n",
    "'hot',\n",
    "'num_failed_logins',\n",
    "'logged_in',\n",
    "'num_compromised',\n",
    "'root_shell',\n",
    "'su_attempted',\n",
    "'num_root',\n",
    "'num_file_creations',\n",
    "'num_shells',\n",
    "'num_access_files',\n",
    "'num_outbound_cmds',\n",
    "'is_host_login',\n",
    "'is_guest_login',\n",
    "'count',\n",
    "'srv_count',\n",
    "'serror_rate',\n",
    "'srv_serror_rate',\n",
    "'rerror_rate',\n",
    "'srv_rerror_rate',\n",
    "'same_srv_rate',\n",
    "'diff_srv_rate',\n",
    "'srv_diff_host_rate',\n",
    "'dst_host_count',\n",
    "'dst_host_srv_count',\n",
    "'dst_host_same_srv_rate',\n",
    "'dst_host_diff_srv_rate',\n",
    "'dst_host_same_src_port_rate',\n",
    "'dst_host_srv_diff_host_rate',\n",
    "'dst_host_serror_rate',\n",
    "'dst_host_srv_serror_rate',\n",
    "'dst_host_rerror_rate',\n",
    "'dst_host_srv_rerror_rate',\n",
    "'outcome'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select input columns\n",
    "features_ad_df = network_ad_df[['duration',\n",
    "'protocol_type',\n",
    "'service',\n",
    "'flag',\n",
    "'src_bytes',\n",
    "'dst_bytes',\n",
    "'land',\n",
    "'wrong_fragment',\n",
    "'urgent',\n",
    "                          \n",
    "'hot',\n",
    "'num_failed_logins',\n",
    "'logged_in',\n",
    "'num_compromised',\n",
    "'root_shell',\n",
    "'su_attempted',\n",
    "'num_root',\n",
    "'num_file_creations',\n",
    "'num_shells',\n",
    "'num_access_files',\n",
    "'num_outbound_cmds',\n",
    "'is_host_login',\n",
    "'is_guest_login',\n",
    "                          \n",
    "'count',\n",
    "'srv_count',\n",
    "'serror_rate',\n",
    "'srv_serror_rate',\n",
    "'rerror_rate',\n",
    "'srv_rerror_rate',\n",
    "'same_srv_rate',\n",
    "'diff_srv_rate',\n",
    "'srv_diff_host_rate',\n",
    "'dst_host_count',\n",
    "'dst_host_srv_count',\n",
    "'dst_host_same_srv_rate',\n",
    "'dst_host_diff_srv_rate',\n",
    "'dst_host_same_src_port_rate',\n",
    "'dst_host_srv_diff_host_rate',\n",
    "'dst_host_serror_rate',\n",
    "'dst_host_srv_serror_rate',\n",
    "'dst_host_rerror_rate',\n",
    "'dst_host_srv_rerror_rate']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_ad_df = network_ad_df[['outcome']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalize numeric features\n",
    "\n",
    "def normalize_numeric_minmax(df, name):\n",
    "    if(df[name].max() > 0):\n",
    "        df[name] = ((df[name] - df[name].min()) / (df[name].max() - df[name].min())).astype(np.float32)\n",
    "    else:\n",
    "        df[name] = df[name].astype(np.float32)\n",
    "    \n",
    "normalize_numeric_minmax(features_ad_df,\"duration\") \n",
    "normalize_numeric_minmax(features_ad_df,\"src_bytes\") \n",
    "normalize_numeric_minmax(features_ad_df,\"dst_bytes\") \n",
    "normalize_numeric_minmax(features_ad_df,\"wrong_fragment\") \n",
    "normalize_numeric_minmax(features_ad_df,\"urgent\") \n",
    "\n",
    "normalize_numeric_minmax(features_ad_df,\"hot\") \n",
    "normalize_numeric_minmax(features_ad_df,\"num_failed_logins\") \n",
    "normalize_numeric_minmax(features_ad_df,\"num_compromised\") \n",
    "normalize_numeric_minmax(features_ad_df,\"num_root\") \n",
    "normalize_numeric_minmax(features_ad_df,\"num_file_creations\") \n",
    "normalize_numeric_minmax(features_ad_df,\"num_shells\") \n",
    "normalize_numeric_minmax(features_ad_df,\"num_access_files\") \n",
    "normalize_numeric_minmax(features_ad_df,\"num_outbound_cmds\") \n",
    "\n",
    "normalize_numeric_minmax(features_ad_df,\"count\") \n",
    "normalize_numeric_minmax(features_ad_df,\"srv_count\") \n",
    "normalize_numeric_minmax(features_ad_df,\"serror_rate\") \n",
    "normalize_numeric_minmax(features_ad_df,\"srv_serror_rate\") \n",
    "normalize_numeric_minmax(features_ad_df,\"rerror_rate\") \n",
    "normalize_numeric_minmax(features_ad_df,\"srv_rerror_rate\") \n",
    "normalize_numeric_minmax(features_ad_df,\"same_srv_rate\") \n",
    "normalize_numeric_minmax(features_ad_df,\"diff_srv_rate\") \n",
    "normalize_numeric_minmax(features_ad_df,\"srv_diff_host_rate\") \n",
    "normalize_numeric_minmax(features_ad_df,\"dst_host_count\") \n",
    "normalize_numeric_minmax(features_ad_df,\"dst_host_srv_count\") \n",
    "normalize_numeric_minmax(features_ad_df,\"dst_host_same_srv_rate\") \n",
    "normalize_numeric_minmax(features_ad_df,\"dst_host_diff_srv_rate\") \n",
    "normalize_numeric_minmax(features_ad_df,\"dst_host_same_src_port_rate\") \n",
    "normalize_numeric_minmax(features_ad_df,\"dst_host_srv_diff_host_rate\") \n",
    "normalize_numeric_minmax(features_ad_df,\"dst_host_serror_rate\") \n",
    "normalize_numeric_minmax(features_ad_df,\"dst_host_srv_serror_rate\") \n",
    "normalize_numeric_minmax(features_ad_df,\"dst_host_rerror_rate\") \n",
    "normalize_numeric_minmax(features_ad_df,\"dst_host_srv_rerror_rate\") \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-hot cooding of categorical columns\n",
    "\n",
    "def encode_text_dummy(df, name):\n",
    "    dummies = pd.get_dummies(df[name]).astype(np.float32)\n",
    "    for x in dummies.columns:\n",
    "        dummy_name = \"{}-{}\".format(name, x)\n",
    "        df[dummy_name] = dummies[x]\n",
    "    df.drop(name, axis=1, inplace=True)\n",
    "\n",
    "encode_text_dummy(features_ad_df,\"protocol_type\") \n",
    "encode_text_dummy(features_ad_df,\"service\") \n",
    "encode_text_dummy(features_ad_df,\"flag\") \n",
    "encode_text_dummy(features_ad_df,\"land\") \n",
    "\n",
    "encode_text_dummy(features_ad_df,\"logged_in\") \n",
    "encode_text_dummy(features_ad_df,\"root_shell\") \n",
    "encode_text_dummy(features_ad_df,\"su_attempted\") \n",
    "encode_text_dummy(features_ad_df,\"is_host_login\") \n",
    "encode_text_dummy(features_ad_df,\"is_guest_login\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reduce the dataset size\n",
    "features_ad_df = features_ad_df[0:40000]\n",
    "label_ad_df = label_ad_df[0:40000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function called encodeLabelBinarizer\n",
    "\n",
    "encodeLabelBinary = lambda x: 0 if x == 'normal.' else 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_ad_df['outcome'] = label_ad_df['outcome'].apply(encodeLabelBinary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Importance\n",
    "from sklearn import datasets\n",
    "from sklearn import metrics\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "extraTreeClassifier = ExtraTreesClassifier()\n",
    "extraTreeClassifier.fit(features_ad_df, label_ad_df['outcome'])\n",
    "# display the relative importance of each attribute\n",
    "print(extraTreeClassifier.feature_importances_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance = extraTreeClassifier.feature_importances_\n",
    "plt.figure(figsize=(16, 6))\n",
    "plt.yscale('log', nonposy='clip')\n",
    "plt.bar(range(len(feature_importance)), feature_importance, align='center')\n",
    "plt.xticks(range(len(feature_importance)), features_ad_df, rotation='vertical')\n",
    "plt.title('Feature importance')\n",
    "plt.ylabel('Importance')\n",
    "plt.xlabel('Features')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Choose only relevant features\n",
    "\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "model = SelectFromModel(extraTreeClassifier, prefit=True)\n",
    "X_new = model.transform(features_ad_df)\n",
    "X_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_adf_train, x_adf_test, y_adf_train, y_adf_test = train_test_split(X_new, label_ad_df['outcome'] , test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Logistic Regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "Log_reg_model = LogisticRegression()\n",
    "\n",
    "Log_reg_model.fit(x_adf_train, y_adf_train)\n",
    "\n",
    "y_adf_pred_logistic = Log_reg_model.predict(x_adf_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RMS value\n",
    "\n",
    "score_adf_logistic = np.sqrt(mean_squared_error(y_adf_test, y_adf_pred_logistic))\n",
    "print(\"Root Mean Squared Error: %.2f\" % score_adf_logistic)\n",
    "print('R2 score: %.2f' % r2_score(y_adf_test, y_adf_pred_logistic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** KNN ** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#implementing Nearest Neighbor\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=2)\n",
    "\n",
    "knn.fit(x_adf_train, y_adf_train) \n",
    "\n",
    "y_adf_pred_knn = knn.predict(x_adf_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics\n",
    "from sklearn import metrics\n",
    "\n",
    "score_adf_knn_acc = metrics.accuracy_score(y_adf_test, y_adf_pred_knn)\n",
    "print(\"Accuracy score: {}\".format(score_adf_knn_acc))\n",
    "\n",
    "score_adf_knn_precision = metrics.precision_score(y_adf_test, y_adf_pred_knn, average= \"weighted\")\n",
    "print(\"Precision score: {}\".format(score_ad_knn_precision))\n",
    "\n",
    "score_adf_knn_recall = metrics.recall_score(y_adf_test, y_adf_pred_knn, average= \"weighted\")\n",
    "print(\"Recall score: {}\".format(score_ad_knn_recall))\n",
    "\n",
    "score_adf_knn_f1 = metrics.f1_score(y_adf_test, y_adf_pred_knn, average= \"weighted\")\n",
    "print(\"F1 score: {}\".format(score_adf_knn_f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** SVM ** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "svm_model = SVC(kernel=\"linear\")\n",
    "\n",
    "svm_model.fit(x_adf_train, y_adf_train)\n",
    "\n",
    "y_adf_pred_svm = svm_model.predict(x_adf_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics\n",
    "\n",
    "score_adf_svm_acc = metrics.accuracy_score(y_adf_test, y_adf_pred_svm)\n",
    "print(\"Accuracy score: {}\".format(score_adf_svm_acc))\n",
    "\n",
    "score_adf_svm_precision = metrics.precision_score(y_adf_test, y_adf_pred_svm, average= \"weighted\")\n",
    "print(\"Precision score: {}\".format(score_adf_svm_precision))\n",
    "\n",
    "score_adf_svm_recall = metrics.recall_score(y_adf_test, y_adf_pred_svm, average= \"weighted\")\n",
    "print(\"Recall score: {}\".format(score_ad_svm_recall))\n",
    "\n",
    "score_adf_svm_f1 = metrics.f1_score(y_adf_test, y_adf_pred_svm, average= \"weighted\")\n",
    "print(\"F1 score: {}\".format(score_ad_svm_f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Gaussian Naive Bayes **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GNB\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "mnb_model = GaussianNB()\n",
    "\n",
    "mnb_model.fit(x_adf_train, y_adf_train)\n",
    "\n",
    "y_adf_pred_gnb = mnb_model.predict(x_adf_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics\n",
    "\n",
    "score_adf_gnb_acc = metrics.accuracy_score(y_adf_test, y_adf_pred_gnb)\n",
    "print(\"Accuracy score: {}\".format(score_ad_gnb_acc))\n",
    "\n",
    "score_adf_gnb_precision = metrics.precision_score(y_adf_test, y_adf_pred_gnb, average= \"weighted\")\n",
    "print(\"Precision score: {}\".format(score_ad_gnb_precision))\n",
    "\n",
    "score_adf_gnb_recall = metrics.recall_score(y_adf_test, y_adf_pred_gnb, average= \"weighted\")\n",
    "print(\"Recall score: {}\".format(score_ad_gnb_recall))\n",
    "\n",
    "score_adf_gnb_f1 = metrics.f1_score(y_adf_test, y_adf_pred_gnb, average= \"weighted\")\n",
    "print(\"F1 score: {}\".format(score_ad_gnb_f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
